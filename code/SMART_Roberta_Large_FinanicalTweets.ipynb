{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-15T15:33:49.448919700Z",
     "start_time": "2024-03-15T15:33:38.142287400Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, RobertaConfig\n",
    "import sys\n",
    "\n",
    "from smart_pytorch import SMARTLoss, kl_loss, sym_kl_loss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import (\n",
    "    Dataset, \n",
    "    DataLoader, \n",
    "    RandomSampler, \n",
    "    SequentialSampler\n",
    ")\n",
    "import math \n",
    "from transformers.optimization import (\n",
    "    AdamW, \n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    matthews_corrcoef,\n",
    "    accuracy_score,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    average_precision_score,\n",
    "    f1_score,\n",
    ")\n",
    "from scipy.special import softmax\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-15T15:33:49.537264600Z",
     "start_time": "2024-03-15T15:33:49.451912700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.1\n",
      "2.2.1\n",
      "True\n",
      "True\n",
      "3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "print(torch.version.cuda)\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.backends.cudnn.enabled)\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-15T15:33:57.468410900Z",
     "start_time": "2024-03-15T15:33:49.532235600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/lenovo/.cache/huggingface/datasets/zeroshot___csv/zeroshot--twitter-financial-news-sentiment-a86f08210b79b812/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Found cached dataset csv (C:/Users/lenovo/.cache/huggingface/datasets/zeroshot___csv/zeroshot--twitter-financial-news-sentiment-a86f08210b79b812/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    }
   ],
   "source": [
    "train_data = datasets.load_dataset('zeroshot/twitter-financial-news-sentiment', split='train')\n",
    "test_data = datasets.load_dataset('zeroshot/twitter-financial-news-sentiment', split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-15T15:33:57.525519600Z",
     "start_time": "2024-03-15T15:33:57.470405400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['$BYND - JPMorgan reels in expectations on Beyond Meat https://t.co/bd0xbFGjkT',\n '$CCL $RCL - Nomura points to bookings weakness at Carnival and Royal Caribbean https://t.co/yGjpT2ReD3',\n '$CX - Cemex cut at Credit Suisse, J.P. Morgan on weak building outlook https://t.co/KN1g4AWFIb',\n '$ESS: BTIG Research cuts to Neutral https://t.co/MCyfTsXc2N',\n '$FNKO - Funko slides after Piper Jaffray PT cut https://t.co/z37IJmCQzB',\n '$FTI - TechnipFMC downgraded at Berenberg but called Top Pick at Deutsche Bank https://t.co/XKcPDilIuU',\n '$GM - GM loses a bull https://t.co/tdUfG5HbXy',\n '$GM: Deutsche Bank cuts to Hold https://t.co/7Fv1ZiFZBS',\n '$GTT: Cowen cuts to Market Perform',\n '$HNHAF $HNHPD $AAPL - Trendforce cuts iPhone estimate after Foxconn delay https://t.co/rlnEwzlzzS',\n \"$HOG - Moody's warns on Harley-Davidson https://t.co/LurHBEadeU\",\n '$HXL - Citing aero ties, Wells slashes PT on Hexcel https://t.co/wU5P2i8WBU',\n '$I - Intelsat cut to Market Perform at Raymond James https://t.co/YsvsMSQRIb',\n '$KRG: Compass Point cuts to Sell https://t.co/MCyfTsXc2N',\n '$LK - Muddy Waters goes short Luckin Coffee https://t.co/8yrbwAjLKG',\n '$MANT - ManTech downgraded ahead of difficult comps https://t.co/mJ1eSrsFXJ',\n '$MDCO: Oppenheimer cuts to Perform',\n '$MPLX $MPC - MPLX cut at Credit Suisse on potential dilution from Marathon strategic review https://t.co/0BFQy4ZU6W',\n '$MSGN - Imperial downgrades MSG Networks amid sports-free airwaves https://t.co/Ul2S6XNXw8',\n '$MTLS - Piper hits the Materialise sidelines https://t.co/qlFxZuhkrt',\n '$NCBS: Hovde Group cuts to Market Perform',\n '$NFLX - New Netflix bear steps out https://t.co/PdPkgLk0FQ',\n '$SHOP - Shopify loses a bull https://t.co/eoq7Lchtk4',\n '$STAY - Nomura Instinet loses confidence in Extended Stay America https://t.co/WrQXTC38nB',\n '$TWLO - Twilio gets Street-low target on virus risk https://t.co/Ky3d8DxnjX',\n \"$VCLT $SPLB $IGLB - Guggenheim's Minerd sees more gloom ahead in stocks https://t.co/NKYQgy2o3J\",\n '$VIAC $VIACA $FOX - MoffettNathanson expects NFL TV costs to skyrocket https://t.co/APfxEqSSRF',\n '$VIAC: Barrington Research cuts to Mkt Perform https://t.co/7Fv1ZiFZBS',\n '$XLNX - Mizuho cuts XLNX target on near-term headwinds https://t.co/hsWCvJb1Ct',\n '2U downgraded to sector weight from overweight at KeyBanc Capital',\n 'Alphabet and Facebook price targets cut at Barclays on weaker ad outlook https://t.co/PlbSaE1Nc2',\n 'AMC Networks price target cut to $35 from $39 at Imperial Capital',\n 'American Software downgraded to neutral from buy at B. Riley FBR',\n \"Analysts question Silicon Motion's margin outlook\",\n 'Anchiano Therapeutics downgraded to peer perform from outperform at Oppenheimer',\n 'Arch Coal stock price target cut to $97 from $100 at B. Riley FBR',\n \"AT&T downgraded by MoffettNathanson to sell on concern wireless can't carry the whole company\",\n 'AT&T shares sink after MoffettNathanson downgrade',\n 'AT&T stock falls after MoffettNathanson downgrades, saying dividend looks less compelling',\n 'Aurora Cannabis stock price target cut to C$1.00 from C$1.75 at Stifel Nicolaus',\n 'Autodesk downgraded to underweight from neutral at JPMorgan',\n 'BAML warns of profit pressure for Hormel',\n 'Barclays cools on Molson Coors',\n 'Becton Dickinson downgraded to outperform from strong buy at Raymond James',\n 'Becton Dickinson downgraded to sector weight from overweight at KeyBanc Capital',\n 'Berry Petroleum downgraded to market perform from outperform at BMO Capital',\n 'Berry Petroleum downgraded to underweight from sector weight at KeyBanc Capital',\n 'Bluebird bio downgraded to in line from outperform at Evercore ISI',\n 'Bluebird bio stock price target cut to $86 from $172 at Evercore ISI',\n 'Canada Goose downgraded to buy from strong buy at CFRA',\n 'Canada Goose stock price target cut to $50 from $65 at CFRA',\n \"Children's Place downgraded as e-commerce puts margins at risk\",\n \"Children's Place price target cut to $80 from $105 at Monness Crespi Hardt\",\n \"Cinedigm Corp. Analysts Are Cutting Their Estimates: Here's What You Need To Know\",\n 'Clovis Oncology stock price target cut to $8 from $12 at Evercore ISI',\n 'Cumberland Pharmaceuticals stock price target cut to $9.00 from $12.25 at B. Riley FBR',\n 'Dell, HPE targets trimmed on compute headwinds',\n 'Dish may not be key winner amid Intelsat woes - analyst',\n \"Disney downgraded as analyst says parks attendance could take 2 years to 'normalize' https://t.co/InJKourtW3\",\n 'Downgrades 11/25: $AEO $COLM $DGII $EQT $ESS $EVRG $GTT $KRG $MANT $MDCO $NFLX $OMP $PEG $PRU $SBT $SITC $TAP $VIPS',\n 'Downgrades 4/7: $MLND $MSGN $MTLS $O $OC $OVV $PEB $PK $PLYA $QEP $RHP $RPAI $SAIA $SDC $SHOP $SKY $SLG $SM $TWLO… https://t.co/NGkYTADOSt',\n 'Dropbox on watch after stinging Spruce Point report',\n 'Earnings Update: electroCore, Inc. Just Reported And Analysts Are Trimming Their Forecasts',\n 'Evercore downgrades bluebird bio and Clovis in premarket analyst action',\n \"Fannie Mae, Freddie Mac downgraded to sell, as analyst cites 'unintended consequences' of CARES Act https://t.co/NhiYwphBQd\",\n 'Forescout downgraded to neutral from buy at Monness Crespi Hardt',\n \"FormFactor cut on 'unsustainable' revenue outlook\",\n 'Funko downgraded to hold from buy at Stifel Nicolaus',\n 'Funko stock price target cut to $10 from $25 at Stifel Nicolaus',\n 'GM stock price target cut to $30 from $32 at Deutsche Bank',\n 'Grubhub gains a bear on margin view',\n 'H&P downgraded at Argus as drilling industry weakness seen persisting',\n 'Hanesbrands stock price target cut to $13 vs. $16 at BofA Merrill Lynch',\n 'Helmerich & Payne started at sell with $34 stock price target at Deutsche Bank',\n 'Hexo stock price target cut to C$5 from C$6 at MKM Partners',\n 'Home Depot stock price target cut to $237 from $240 at Instinet',\n 'Intelsat downgraded to underweight from neutral at J.P. Morgan',\n 'Intelsat lower after Morgan Stanley target cut',\n 'Jefferies warns on L Brands',\n \"Kohl's stock price target cut to $48 from $52 at Deutsche Bank\",\n \"Kohl's stock price target cut to $50 from $53 at Wedbush\",\n 'L Brands stock price target cut to $16 from $20 at Wedbush',\n 'Lovesac stock price target cut to $28 vs. $33 at Canaccord',\n 'Lyft stock price target cut to $58 from $70 at Deutsche Bank',\n 'Macerich downgraded to underweight from sector weight at KeyBanc Capital',\n 'Marinus Pharma stock price target cut to $6 from $7 at Leerink',\n 'Methanex downgraded at TD Securities on valuation',\n 'Metro Inc. Just Missed Earnings And Its EPS Looked Sad - But Analysts Have Updated Their Models',\n 'Mid-America Apartment Communities downgraded to sector weight from overweight at KeyBanc Capital',\n 'Mitek Systems stock price target cut to $11 from $14 at Benchmark',\n 'Napco Security stock price target cut to $30 from $35 at Imperial Capital',\n \"Needham's Martin Defends Her Prediction That Netflix Subscriptions Will Fall\",\n 'Netflix downgraded to ‘underperform’ at Needham',\n 'Netflix stock falls after Wells Fargo turns bearish',\n 'New Netflix bear steps out',\n 'New York Opioid Case A Significant Risk For Pharma Stocks, Says Bearish BofA',\n 'Norwegian Cruise Line downgraded to strong sell from hold at CFRA',\n 'Novo Nordisk downgraded by UBS on concerns over drug launch cost',\n 'Oppenheimer cuts estimates on Yum China',\n 'Orbcomm price target lowered to $9 at Canaccord',\n \"O'Reilly Automotive stock price target cut to $405 from $415 at J.P. Morgan\",\n 'Patterson-UTI Energy started at sell with $7 stock price target at Deutsche Bank',\n 'PS Business Parks downgraded to underweight from sector weight at KeyBanc Capital',\n 'Rio Tinto downgraded to neutral from overweight at J.P. Morgan',\n 'Rosenblatt Projects 47% Downside In Apple Shares, Warns Of Drop In iPhone Production',\n 'Royal Caribbean downgraded to sell from buy at CFRA',\n 'Royal Caribbean stock price target cut to $100 from $135 at CFRA',\n \"Royal Caribbean's stock falls after CFRA swings to bearish on 'more dire' coronavirus assessment\",\n \"Scandinavian Tobacco Group A/S Earnings Missed Analyst Estimates: Here's What Analysts Are Forecasting Now\",\n 'Sherwin-Williams cut from Conviction Buy at Goldman on valuation',\n 'Sirius downgraded on valuation',\n 'Spotify Double Downgrade Reflects Prospect of Slower Revenue Growth https://t.co/jMJ3N4A06T',\n 'SunTrust latest to trim Nielsen target',\n 'Taco Bell parent Yum Brands downgraded to neutral from buy at BTIG',\n 'Take-Two downgraded on murky game slate',\n 'Take-Two Interactive stock price target cut to $124 from $133 at MKM Partners',\n 'Target Hospitality downgraded to perform from outperform at Oppenheimer',\n 'Taubman Centers downgraded to sector weight from overweight at KeyBanc Capital',\n 'The Manitowoc Company, Inc. Just Reported Earnings, And Analysts Cut Their Target Price',\n 'Transocean started at sell with $3 stock price target at Deutsche Bank',\n 'Ultra Clean downgraded to hold from buy at Stifel Nicolaus',\n 'Wells Fargo Downgrades Netflix $NFLX to Underperform but sees as a takeover target. NFLX could get acquired: NFLX i… https://t.co/vuNpYwRDSg',\n 'Wells Fargo Downgrades Netflix On Spending Concerns',\n \"Wells Fargo downgrades Netflix: 'If content is king, then cash is queen' https://t.co/niUwYR5YUD\",\n 'Welltower stock price target cut to $90 from $95 at J.P. Morgan',\n 'WWE stock price target cut to $57 from $79 at Benchmark',\n '$ALTG: Dougherty & Company starts at Buy',\n \"$AMD - AMD's Navi shows strong adoption - BofA https://t.co/WnCksfl1gX\",\n '$ANCUF: BMO Capital Markets ups to Outperform',\n '$BHE: Lake Street starts at Buy',\n '$CNC *S&P Raises Centene Corp. Rtg To BBB- From BB+; Outlk Stable*',\n '$DE - Citi gives big boost to Deere PT https://t.co/MfhnyZe593',\n '$EBAY - Guggenheim sees favorable catalysts for eBay https://t.co/1adfDkHeaw',\n '$FCX - Freeport upgraded as Credit Suisse sees big buying opportunity https://t.co/d4TGqrNi7Y',\n '$FLXN: BTIG Research starts at Buy https://t.co/MCyfTsXc2N',\n '$FNV - Franco-Nevada will not need more funding despite suspensions, analyst says https://t.co/pyARTEahWH',\n '$GBT: Cantor Fitzgerald resumes at Overweight',\n '$GM - Morgan Stanley sees GM doubling in bull case scenario https://t.co/uNK2xzGQuq',\n '$GOOS: Barclays starts at Overweight',\n '$HAS - Hasbro gains after UBS upgrade https://t.co/taTMaip1uM',\n '$JD - New https://t.co/BDnqOJSFq5 bull praises acceleration, margin profile https://t.co/Swtzl0mjnF',\n '$JD: Barclays ups to Overweight',\n '$KHC - JPMorgan constructive on Kraft Heinz https://t.co/Sp4I2ehAVw',\n '$KIM: Compass Point ups to Neutral',\n \"$LOW - Lowe's racks up another positive rating despite recession risk https://t.co/ekx4f6aqIK\",\n \"$LRCX - Lam target boosted on 'compelling' setup https://t.co/TBgKmZH6pM\",\n '$LULU: Barclays starts at Overweight https://t.co/MCyfTsXc2N',\n '$MU $WDC - Bull sees 2020 upsides for MU, WDC https://t.co/65zGfs82xI',\n '$NBY Rated buy https://t.co/xWbLXU1yOY',\n '$NFLX - SunTrust expecting Netflix to post strong user growth https://t.co/6z62FXsUom',\n '$NVDA - Nvidia set for gaming tailwinds - BofA https://t.co/l3m78pJzrW',\n \"$OCN - BTIG sees 'material improvement' to Ocwen book value https://t.co/8MMo05uiM6\",\n '$OYST: Cowen starts at Outperform',\n '$RAPT: BMO Capital Markets starts at Outperform',\n '$RNG - Evercore starts RingCentral at Outperform https://t.co/rfNcO3sw2k',\n '$SBAC $SBAC $AMT - MoffettNathanson boosts SBAC to Buy https://t.co/8k4l0c5x89',\n '$SNAP - Loop turns bullish on Snap ad growth https://t.co/TekdZS48et',\n '$STXS: Lake Street starts at Buy',\n \"$TENB - Cowen sees upside in TENB's Indegy purchase https://t.co/Ve7bTreR7Y\",\n '$UBER $LYFT - Uber, Lyft can raise prices to profit - Barclays https://t.co/1xx084l36h',\n '$WABC: Maxim Group ups to Buy',\n '$WRI: Compass Point ups to Buy',\n \"$ZS - New Zscaler bull sees 'sustainable' growth; shares +4% https://t.co/4fx811IVG5\",\n \"Activision, Take-Two charts look 'constructive' as Wells Fargo gets bullish on video game plays: Trader $ATVI $TTWO… https://t.co/1s0WRleA2n\",\n 'Adobe stock price target raised to $315 vs. $290 at Wedbush',\n 'Adobe stock price target raised to $318 vs. $310 at Instinet',\n 'Aegis Capital positive on Netflix usage trend',\n 'After a 60% drop in 6 months, Bank of America says buy pot stock Canopy',\n 'Akero Therapeutics started at buy with $36 stock price target at Canaccord Genuity',\n 'Align Technology stock price target raised to $305 from $290 at Stifel Nicolaus',\n 'Allergan upgraded to buy from neutral at UBS',\n 'Altria stock price target raised to $54 from $50 at Stifel Nicolaus',\n 'Amneal Pharma price target raised to $6 vs. $4 at SunTrust Robinson Humphrey',\n 'An analyst at Loop Capital upgrades $LYFT  to buy from hold, citing higher revenue growth potential.  https://t.co/2blCalNe9P',\n 'Analyst: AT&T Is The Top Telecom Pick For 2020',\n 'Analysts Mostly Bullish On Newly Public Insurance Firm BRP Group',\n 'Analysts See 18% Gains Ahead For The Holdings Of The First Trust Dow Jones Internet Index Fund ETF',\n 'Ansys stock price target raised to $265 from $235 at Benchmark',\n 'Anthem stock price target raised to $330 from $300 at SunTrust RH',\n 'Applied Industrial Technologies stock price target raised to $70 from $65 at KeyBanc Capital',\n 'arGen-X stock price target raised to $255 vs. $203 at Instinet',\n 'Armata Pharmaceuticals started at buy with $8 stock price target at Maxim Group',\n 'Arrow Electronics stock price target raised to $87 vs. $84 at SunTrust Robinson Humphrey',\n 'Asset Management One Co. has a bullish call on Treasuries. One for the long, long run https://t.co/8go8ZhMvdc',\n 'Audentes Therapeutics stock price target raised to $40 from $34 at Evercore ISI',\n 'Audentes Therapeutics upgraded to outperform from in line at Evercore ISI',\n 'Axcelis Technologies stock price target raised to $32 from $24 at Benchmark',\n 'Baird bullish on beat-up Canada Goose',\n 'Baird likes drug wholesalers in premarket analyst action',\n 'Baird returns to Wingstop bull camp',\n 'Baker Hughes started at buy with $32 stock price target at Deutsche Bank',\n 'Bank of America stock price target raised to $49 from $48 at Oppenheimer',\n 'Barclays analysts says Uber and Lyft are well-positioned to achieve profitability https://t.co/emLga0CBm1',\n 'Bausch Health upgraded to overweight from neutral at JPMorgan, price target raised to $38 from $32',\n 'Bed Bath & Beyond reiterated as buy at BofA Merrill Lynch',\n 'Belden stock price target raised to $56 vs. $54 at SunTrust Robinson Humphrey',\n 'Benchmark says this roof supplier is a Buy',\n \"Bernstein turns bullish on VMware's valuation\",\n 'Berry Petroleum upgraded to sector weight from underweight at KeyBanc Capital',\n 'Best Buy stock price target raised to $105 from $86 at Oppenheimer',\n 'Biogen stock price target raised to $392 from $320 at Instinet',\n 'Biogen stock price target raised to $410 from $350 at SVB Leerink',\n 'Boeing stock price target raised to $427 from $406 at CFRA',\n 'BofA Upgrades WestRock After Containerboard Conference, London Pulp Week',\n 'Bouygues SA Just Beat Analyst Forecasts, And Analysts Have Been Updating Their Predictions',\n \"BTIG expects Domino's to boost buybacks\",\n \"BTIG sees 'material improvement' to Ocwen book value\",\n 'Bull sees 2020 upsides for MU, WDC',\n 'Cabot upgraded to buy from neutral at UBS',\n 'Canada Goose upgraded to outperform from neutral at Baird, price target C$53',\n 'Capital Southwest initiated at outperform at Raymond James with $23.50 price target',\n 'Celanese stock price target raised to $120 vs. $111 at BofA Merrill Lynch',\n 'CF Industries stock price target raised to $56 vs. $52 at BofA Merrill Lynch',\n 'CF Industries upgraded to buy vs. neutral at BofA Merrill Lynch',\n 'Ciena stock price target raised to $47 vs. $43 at Instinet',\n 'Citi gives big boost to Deere PT',\n 'Citi reels in PT on Nu Skin',\n \"CITRON SAYS IT'S LONG ON LUCKIN COFFEE\\n\\nMW vs CItron\",\n 'CNH Industrial upped to Buy at Deutsche Bank on valuation',\n 'Copa Holdings stock price target raised to $130 from $103 at Deutsche Bank',\n 'Costco price target raised to $330 vs. $320 at BofA Merrill Lynch',\n 'CSX stock price target raised to $90 from $72 at Deutsche Bank',\n 'CSX upgraded to buy from hold at Deutsche Bank',\n 'CVS Health stock price target raised to $90 from $85 at SunTrust RH',\n 'CyrusOne upgraded to overweight from sector weight at KeyBanc Capital',\n 'Diamondrock Hospitality upgraded to sector weight from underweight at KeyBanc Capital',\n 'DLH Holdings maintained as buy with $7 price target at Canaccord',\n 'Domtar upgraded to market perform at BMO Capital',\n 'Dow stock price target raised to $52 vs. $46 at BofA Merrill Lynch',\n \"Earnings Update: Here's Why Analysts Just Lifted Their Aemetis, Inc. Price Target To US$2.25\",\n \"Earnings Update: Here's Why Analysts Just Lifted Their Milestone Scientific Inc. Price Target To US$2.00\",\n \"Earnings Update: Here's Why Analysts Just Lifted Their Skyworks Solutions, Inc. Price Target To US$104\",\n 'Eastman Chemical stock price target raised to $85 vs. $82 at BofA Merrill Lynch',\n 'Edgewell stock price target raised to $41 from $33 at CFRA',\n 'Elastic upgraded to buy from hold at Canaccord Genuity',\n 'Enbridge upgraded to buy from neutral at UBS, price target lifted to C$54 from C$50',\n 'Energizer Holdings upgraded to neutral from underweight at JP Morgan, price target lifted to $51 from $41',\n \"Energizer shakes off JPMorgan's bear call\",\n 'Equity Lifestyle Properties upgraded to outperform at BMO Capital',\n 'Estee Lauder stock price target raised to $238 from $202 at Raymond James',\n 'Etsy initiated as buy with $55 price target at SunTrust Robinson Humphrey',\n 'FedEx stock price target raised to $168 from $163 at BofA Securities',\n 'FleetCor stock price target raised to $325 from $310 at CFRA',\n \"Ford's electric SUV impresses Credit Suisse\",\n 'Fortinet stock price target raised to $115 from $104 at BMO Capital',\n 'Fortinet stock price target raised to $120 from $110 at Monness Crespi Hardt',\n 'Glenmark Pharmaceuticals Limited Just Beat Analyst Forecasts, And Analysts Have Been Updating Their Predictions',\n 'Goldman adds Workday to conviction list; shares +1%',\n 'Goldman Says 36,000% Rally in Brazilian Retailer Has Room to Run',\n 'GrubHub stock price target raised to $40 from $34 at Oppenheimer',\n 'GrubHub stock price target raised to $60 from $30 at Susquehanna',\n 'Hasbro upgraded on earnings growth potential from Entertainment One acquisition',\n 'Hasbro upgraded to buy from neutral at UBS, price target $117',\n 'Heat Biologics started at buy with $1 stock price target at Maxim Group',\n 'Home Depot upgraded to outperform from neutral at Credit Suisse, price target raised to $235 from $225',\n 'Humana stock price target raised to $363 from $319 at Deutsche Bank',\n 'Huntsman named Conviction Buy at Goldman in chemical sector shuffle',\n 'Intel stock price target raised to $62 vs. $60 at SunTrust Robinson Humphrey',\n 'Intercept Pharmaceuticals maintained as buy with $165 price target at Canaccord',\n 'Invitae Corp. maintained as buy at Benchmark',\n 'J.C. Penney stock price target raised to $1.00 from 80 cents at B. Riley FBR',\n 'Jabil stock price target raised to $47 from $36 at CFRA',\n 'Jefferies analysts lifted Tesla $TSLA shares to buy while paring the price target 19%. https://t.co/BJhXfQuRTd',\n 'JPMorgan likes Bausch Health in premarket analyst action',\n \"JPMorgan out bullish on Uber's upside potential\",\n 'Kilroy Realty upgraded to overweight from sector weight at KeyBanc Capital',\n 'L3Harris started at buy with $280 stock price target at Benchmark',\n 'Lagging MLP upgraded at Goldman as new projects seen lifting cash flows',\n 'Ligand Pharmaceuticals started at buy with $135 stock price target at Benchmark',\n 'Lovesac maintained as buy at Canaccord',\n 'Lululemon stock price target raised to $260 vs. $222 at Susquehanna',\n 'Lyft stock gets an upgrade due to competitive positioning',\n 'LyondellBasell sock price target raised to $107 vs. $99 at BofA Merrill Lynch',\n 'Magnolia Oil & Gas started at overweight with $14 stock price target at KeyBanc Capital',\n 'Mesa Air reiterated as outperform at Cowen',\n 'Microchip Technology stock price target raised to $113 vs. $106 at SunTrust Robinson Humphrey',\n 'Mizuho raises Intel on aggressive price cuts',\n 'Molina Healthcare stock price target raised to $165 from $155 at SunTrust RH',\n 'Morgan Stanley analyst Joseph Moore upgraded NVIDIA (NASDAQ: $NVDA) from Equalweight to Overweight with a price tar… https://t.co/NVjiO6Brwb',\n 'Morgan Stanley bullish on Eldorado Resorts ahead of Caesars combination',\n 'Morgan Stanley joins mortgage sector bulls',\n \"Morgan Stanley Raises Fortinet's Price Target After Bullish Analyst Day\",\n 'Morgan Stanley upgrades NVIDIA $NVDA from Equal-Weight to Overweight and announces $259 price target.',\n \"Morgan Stanley upgrades Nvidia to buy, predicting 2020 will be 'a return to solid growth' https://t.co/9gTGxKbSGj\",\n 'Munster Doubles Down, Says Apple Has 40% Upside This Year',\n 'New bull sees 22% upside for Splunk',\n 'New for subscribers:  Morgan Stanley adds Amazon to its list of favorite stocks. Here are the rest... https://t.co/QbgS6IQXRt',\n 'Newmont Goldcorp stock price target raised to $41.50 from $39.50 at B. Riley FBR',\n 'NLight target raised on Nutronics deal',\n 'Nokia stock price target raised to $6.00 from $5.50 at MKM Partners',\n 'Nuvasive upgraded to outperform vs market perform at Leerink',\n 'Nvidia Has One Of The Best Opportunities To Maintain High Multiple, Morgan Stanley Says In Upgrade',\n 'Nvidia stock climbs after Morgan Stanley turns bullish',\n 'NXP Semiconductors stock price target raised to $139 vs. $128 at SunTrust Robinson Humphrey',\n 'Olin Corp. stock price target raised to $23 vs. $22 at BofA Merrill Lynch',\n 'One analyst says TD could unlock \"significant value\" by selling its minority stake in Ameritrade after its purchase… https://t.co/swy7IVNnQE',\n 'Onto Innovation stock price target raised to $50 from $40 at Benchmark',\n 'Owens Corning upgraded at Wells after raising residential insulation prices',\n 'Owens Corning Upgraded On Strong Insulation, Roofing Potential',\n 'Oyster Point Pharma initiated at overweight with $26 price target at JP Morgan',\n 'Paul Tudor Jones Sees the Potential for a Stock Market Explosion, to the Upside',\n 'PennMac Financial rises to record as Piper gets more bullish',\n 'Piper likes DexCom in premarket analyst action',\n 'Pixelworks, Inc. Just Reported And Analysts Have Been Lifting Their Price Targets',\n 'PPG Industries stock price target raised to $113 vs. $105 at BofA Merrill Lynch',\n 'Prothena upgraded to outperform from in line at Evercore ISI',\n 'Purple Innovation initiated at overweight with $12 price target at KeyBanc Capital',\n 'Quanterix maintained as outperform with $35 price target at Leerink',\n 'Results: KLA Corporation Beat Earnings Expectations And Analysts Now Have New Forecasts',\n 'RPM International stock price target raised to $79 vs. $75 at BofA Merrill Lynch',\n 'S&P Global stock price target raised to $322 from $274 at Stifel Nicolaus',\n 'Sanderson Farms stock price target raised to $159 from $150 at J.P. Morgan',\n 'Schlumberger started at buy with $42 stock price target at Deutsche Bank',\n \"Selvaag Bolig ASA Just Beat Earnings Expectations: Here's What Analysts Think Will Happen Next\",\n 'SoftBank will have the \"last laugh\" with WeWork deal, one Bernstein analyst says https://t.co/qmfAPKICdp',\n 'Sonoma Pharma maintained as buy with $9 price target at Benchmark',\n 'Splunk upgraded to overweight at Morgan Stanley',\n 'Starbucks Stock Is a Buy, According to This Investment Bank',\n 'Starbucks upgraded to overweight at J.P. Morgan after meeting with executives',\n 'Stifel Upgrades Parker-Hannifin On Potential Synergies, Industry Recovery Prospects',\n \"Stifel Upgrades Wendy's, Bullish On Breakfast And Digital Prospects\",\n 'Syanptics raised on forecast, margin strength',\n 'Synaptics stock price target raised to $40 from $32 at J.P. Morgan',\n 'Synopsys upgraded to overweight from neutral at JPMorgan, price target raised to $160 from $157',\n 'TE Connectivity stock price target raised to $95 vs. $94 at SunTrust Robinson Humphrey',\n 'Teekay Tankers upped two notches to Buy at BAML',\n 'Texas Instruments stock price target raised to $125 vs. $122 at SunTrust Robinson Humphrey',\n 'Twitter stock price target raised to $36.50 from $34.50 at Wedbush',\n 'Twitter stock price target raised to $38 from $36 at SunTrust RH',\n 'Twitter stock price target raised to $47 from $34 at Susquehanna',\n 'Twitter, Inc. Just Reported And Analysts Have Been Lifting Their Price Targets',\n 'U.S. Well Services initiated at buy with $3.50 price target at Stifel',\n 'Uber stock price target raised to $45 from $34 at MKM Partners',\n 'Uber stock price target raised to $49 from $47 at BofA Securities',\n 'Uber stock price target raised to $50 from $45 at Oppenheimer',\n 'Uber stock price target raised to $52 from $50 at Wedbush',\n 'Uber stock price target raised to $54 from $50 at Deutsche Bank',\n 'UBS raises Peloton’s price target to $40',\n 'UBS upgrades Hasbro on strong sales of Frozen II dolls into Black Friday https://t.co/23HWM60K7q',\n 'UDR upgraded to overweight from sector weight at KeyBanc Capital',\n 'Unisys stock price target raised to $21 from $18 at Sidoti & Co.',\n 'UnitedHealth stock price target raised to $338 from $303 at CFRA',\n 'Upgrades 11/25: $ARWR $CACI $CF $DECK $DKS $ENR $EQM $ETRN $FE $HAS $HLX $JD $KIM $LYFT $NTR $NVDA $ORAN $PROS $SNAP $WEN $WING $WRI $ZS',\n 'Upgrades 4/7: $AMT $BHP $BK $CHH $ENI $EQNR $ETR $EXR $FCX $HOLX $HSIC $LOW $LSI $MAR $MAS $MGP $MHK $MXL $NGG $NWE… https://t.co/jXZNb22Cwb',\n 'Vertex Pharmaceuticals stock price target raised to $248 from $240 at BMO Capital',\n 'Vertical Research expects Allison Transmission to break out',\n 'Voyager Therapeutics started at outperform with $26 stock price target at Oppenheimer',\n 'Walt Disney stock price target raised to $143 from $141 at Imperial Capital',\n \"Wells Fargo analyst Dori Kesten lifted her rating on Marriott $MAR to overweight, noting that the hotel chain's sha… https://t.co/9TLXzYZjUs\",\n 'Wells still bullish on T-Mobile after unsurprising CEO change',\n 'Westlake Chemical stock price target raised to $65 vs. $64 at BofA Merrill Lynch',\n 'WestRock stock price target raised to $48 from $42 at BofA Merrill Lynch',\n 'WestRock upgraded to buy from neutral at BofA Merrill Lynch',\n 'Why Credit Suisse Sees the S&P 500 Gaining 10% in 2020',\n 'Williams Cos. initiated as buy with $27 price target at SunTrust Robinson Humphrey',\n \"Workday added to Goldman's conviction list, stock gains\",\n 'Zendesk stock price target raised to $85 from $75 at Stifel Nicolaus',\n \"Zendesk targets raised after 'solid' results\",\n \"$LB - MKM Partners puts a number on Victoria's Secret https://t.co/VSzHLqLBgE\",\n '$WING - Baird returns to Wingstop bull camp https://t.co/KfPaweOVgo',\n 'Analysts React To FCC Decision On Intelsat C-Band Spectrum Auction',\n 'Arex Capital ramps up pressure on Zagg',\n 'Aviation Capital Group Assigned ‘Baa2’ Rating by Moody’s https://t.co/kEKxNfJEgZ https://t.co/5hasUcG1ON',\n \"Benzinga's Top Upgrades, Downgrades For December 17, 2019\",\n \"Benzinga's Top Upgrades, Downgrades For November 25, 2019\",\n 'BRP Group started at hold with $18 stock price target at Jefferies',\n 'BTIG resumes coverage on range of healthcare stocks, neutral on AbbVie',\n 'Bull camp empty on Sally Beauty',\n \"CA$10.60 - That's What Analysts Think Vecima Networks Inc. Is Worth After These Results\",\n \"Chicken Soup for the Soul Entertainment, Inc. Third-Quarter Results: Here's What Analysts Are Forecasting For Next Year\",\n 'Citron Research, owned by the prominent short seller Andrew Left, placed a $5 price target on $PTON  https://t.co/a5WT4knuVf',\n 'Coin Toss: Morgan Stanley Raises Tesla Bull Case To $500, Keeps Bear Case At $10 https://t.co/hjXizuPs7Z',\n \"GDS Holdings Limited Just Released Its Third-Quarter Earnings: Here's What Analysts Think\",\n 'Global Automotive Tensioner Market 2019-2023 | Growing Adoption of Start-Stop System to Boost Growth | Technavio… https://t.co/xST0VP6IgS',\n 'Here are the biggest analyst calls of the day: Johnson & Johnson, Canopy Growth, Intelsat & more',\n 'Here are the biggest analyst calls of the day: Netflix, Nvidia, Lyft, Molson Coors, Snap & more',\n 'Here are the biggest analyst calls of the day: Netflix, Nvidia, Lyft, Molson Coors, Snap & more https://t.co/jzxbhHJwOu',\n 'II-Vi Inc. initiated as neutral at Susquehanna',\n 'LivePerson started at neutral with $40 stock price target at J.P. Morgan',\n 'Miller Value Partners Likes These Two Stocks; Bearish on One',\n \"MKM Partners puts a number on Victoria's Secret\",\n 'Morgan Stanley has a new favorite pick in the chip space',\n 'MPLX initiated as hold with $28 price target at SunTrust Robinson Humphrey',\n 'Oppenheimer calls out transformation of big-box retailer',\n 'Pipestone Energy Corp. Just Reported, And Analysts Assigned A CA$2.55 Price Target',\n \"Royal Caribbean Cruises Ltd. Full-Year Results Just Came Out: Here's What Analysts Are Forecasting For Next Year\",\n 'Spirit Airlines, Inc. Just Released Its Full-Year Results And Analysts Are Updating Their Estimates',\n 'Stock Market Update: NVIDIA shares upgraded, Netflix downgraded',\n 'TJX Q3 FactSet consensus 66 cents',\n 'Top Analyst Upgrades and Downgrades: AbbVie, Biogen, Boeing, Etsy, Gilead, GoPro, Micron, Newmont, Peloton, Pfizer, Valero, Zynga and More',\n \"Twilio Inc. Full-Year Results Just Came Out: Here's What Analysts Are Forecasting For Next Year\",\n 'Upgrades 2/4: $AMAL $CM $CMCSA $ELF $GDOT $LFUS $MLI $RRR $TD $TXRH $VLY $WW Downgrades 2/4: $ABG $AWK $CHKP $DEO…\\xa0https://t.co/xscOdKUhfo',\n \"US$50.08: That's What Analysts Think Weibo Corporation Is Worth After Its Latest Results\",\n 'Valaris started at hold with $4 stock price target at Deutsche Bank',\n 'William Blair sees rivals pressing Viasat',\n 'Aggressive Fed boost fails to stop sell-off #Samp;P500 #economy #MarketScreener https://t.co/p8lniQQTZt https://t.co/VsHuUI8mgj',\n 'Bank of Ireland cuts key profit target as low rates take toll https://t.co/pWzgfDXDm3',\n \"Bank of Nova Scotia : Scotiabank CEO expects 'somewhat elevated' loan losses for 3-4 quart... #BankofNovaScotia… https://t.co/OAXfDtrtAk\",\n \"Breakingviews - Fed's ultra-loose efforts will cast long shadow https://t.co/nktomiS4jW https://t.co/A5r5dgKIhX\",\n 'European banks slash $280bn from main US businesses https://t.co/Gwahc0Sc0F',\n \"Europe's banks were only halfway through cleaning up more than $1 trillion of loans that turned sour in the last cr… https://t.co/a5XeRxCEVl\",\n \"Eurozone banks' returns were already hurting before COVID-19, ECB says https://t.co/hDvBAwFDeG\",\n 'Fed Liquidity Drain Spoils Virus-Surge-Inspired Stock Buying-Panic | Zero Hedge https://t.co/wnLxXP4sHu',\n 'Fed policymakers working to limit damage as pandemic puts U.S. economy on pause https://t.co/QwCihVGWly https://t.co/ZRbeFgfAdo',\n 'Fed Report : Small-Business Sector Highly Vulnerable to Coronavirus Crisis--Update #FedReport #economy… https://t.co/AjGZqiRl5b',\n 'Fed Warns Virus Poses ‘New Risk’ to Global Growth, Markets',\n \"Fed's Bullard: China to 'slow noticeably' in first quarter due to virus https://t.co/i1jKKq9kNi https://t.co/4Ug71D7IN6\",\n \"Fed's corporate credit measures to limit buybacks, dividend https://t.co/4FrdSvMV8T\",\n \"Fed's Williams: Risks to U.S. economy still to the downside\",\n 'Former Fed chief Bernanke sees bad year, no quick recovery #economy #MarketScreener https://t.co/YaN7qVv5aq https://t.co/zn3oEbjuL7',\n 'Janet Yellen says a V-shaped economic recovery \"is possible,\\'\\x9d but she is \"worried that the outcome will be worse\" https://t.co/11FIumVTXC',\n 'Lagarde Says ECB Running Out of Room to Fight Global Threats',\n \"Lagarde Says ECB's Options Limited by Low Rates, Low Inflation\",\n 'New #IMFBlog shows that long-lasting credit booms that featured rapid construction growth never ended well.… https://t.co/gMBiUS6z7J',\n 'Overall Fed Temporary Liquidity Continues to Shrink #economy #MarketScreener https://t.co/UjqUoGk3L9 https://t.co/OVGpDhlZOb',\n \"Schweizerische Nationalbank : Switzerland Loses Grip on World's Lowest Borrowing Costs -- ...… https://t.co/fuN3T0obd5\",\n \"Some officials worried about the bank's decision to drop self-imposed limits on the ECB's bond purchases https://t.co/elfM7nRACS\",\n 'Thailand has a limited space to deliver a “strong dose” of monetary policy, central bank governor says as he cautio… https://t.co/WR3CYfD3hy',\n \"The Federal Reserve's rescue of the overnight lending market appears to be having an unintended side effect: it's j… https://t.co/8Ko2fxHj7W\",\n \"UAE stimulus to help bank liquidity but may increase problem loans - Moody's #economy #MarketScreener… https://t.co/8rGhax4y8C\",\n 'UPDATE 2-China cbank warns high financial risks amid rising economic headwinds',\n 'Yellen Blames \"Enormous Debt And Buybacks\" For Coming Default Wave; Morgan Stanley Says It\\'s All The Fed\\'s Fault  https://t.co/Ed0jVBpmhZ',\n \"Fed Chair Powell: 'no reason' rising wages, job gains can't continue https://t.co/cCUifiy1YG https://t.co/pjXZLMl9Jf\",\n \"Federal Reserve Chairman Jerome Powell said the U.S. economy is 'resilient,' even as he cited the potential threat… https://t.co/jdypDu2LhY\",\n \"Fed's 'bazooka' soothes dollar funding squeeze https://t.co/ZHKV5YcU5L https://t.co/gB2rZSnrcF\",\n \"Fed's Daly says U.S. economy well placed to weather storms https://t.co/46F3sPCsXW https://t.co/Hn3cXkJQhp\",\n \"Fed's Daly says US job market still has room to run https://t.co/nQOWWsXkh7\",\n \"Fed's Kaplan: Inflation Will Be Muted For Some Time (Radio)\",\n \"Fed's Mary Daly says current interest rates put economy 'in a good place to weather the storms'\",\n \"Fed's Mester, upbeat on outlook, say central bank can watch and wait on interest rates\",\n \"Fed's Rosengren says he is 'optimistic' on economy\",\n \"Fed's Rosengren says U.S. unlikely to have economic downturn in 2020\",\n \"World Bank's Malpass upbeat on prospects for progress on debt relief #economy #MarketScreener… https://t.co/tILUoEQkYC\",\n 'Commercial and industrial loans at all commercial banks fall to $2.345 trillion, down $7 billion from a week earlie… https://t.co/IO6JFnTMWX',\n 'Czech interest rates could stay where they are for another year as global risks cloud the outlook for the export-or… https://t.co/bDhYKopjCM',\n 'ECB bought record debt volumes last week in crisis fight #economy #MarketScreener https://t.co/08TD4PqNna https://t.co/5ct4SutY3Q',\n \"Fed Chairman Jerome Powell says central bank 'watching carefully' as firms brace for wave of missed mortgage paymen… https://t.co/mBzYyHmCrW\",\n \"Italy's top bank unveils new lending measures as firms battle cash crunch https://t.co/ioNcxm5GzF https://t.co/7PFLksQXJP\",\n 'The Fed may lower the rate on its support facility for money market funds, strategists say https://t.co/FrLrivydji',\n '$ECONX: Federal Reserve will establish facility to facilitate lending to small businesses via the Small Business... https://t.co/OCCUO9DX7c',\n '$XLF $FAS $FAZ - Fed revives TALF to bolster ABS market https://t.co/0rRXf0aqLC',\n '$XLF $FAS $FAZ - The Fed to establish term financing for small business loans https://t.co/4nTXbSxrHN',\n '.@WorldBank Group asks #G20 leaders to suspend debt repayments from poorest countries. This will help focus critica… https://t.co/VMcZWh4sop',\n 'Asset Allocation Committee Outlook 2Q2020 | Contagion. https://t.co/3SLpGWc3KL #economy #markets #investing',\n 'Australia central bank pumps liquidity, proposes bond buys to ease financial conditions https://t.co/JkbAPjX6Yv https://t.co/GFYiDHmP2U',\n 'Australia plans an additional A$66 billion ($38.2 billion) in stimulus for the coronavirus-stricken economy, includ… https://t.co/7O2gkIusAO',\n 'Bank of Israel expands its crisis QE with $13.6 billion in bond buying https://t.co/xEMWn1vjAO',\n \"Brazil's central bank stepped in to prop up the currency https://t.co/yQBoa6mTqi\",\n 'Central Bank actions will contribute to easing the economic impact of the pandemic - Governor Gabriel… https://t.co/UhzEj1icQ3',\n 'ECB makes it easier for banks to tap credit amid coronavirus crisis https://t.co/aUORNC7SON https://t.co/rrzy3Yv56J',\n 'ECB to ease collateral requirements for banks looking to borrow #economy #MarketScreener https://t.co/rlYmLpiGKA https://t.co/9v3IP9D10o',\n 'eib: #MustRead➡️EIB Bank Lending #Survey for #CESEE shows optimistic opinion of the region, positive profitability… https://t.co/jtZoHHCmeB',\n 'Fed, saying aggressive action is needed, starts unlimited QE https://t.co/pbo30sSQ9Z',\n \"Fed's Quarles says 2-3 weeks away from getting loans to mid-sized U.S. businesses through its newly created Main St… https://t.co/3R4DyktUJ0\",\n 'Fed\\'s Quarles: \"Main Street\" loans like two to three weeks off as details finalized https://t.co/pUobuzSFBj https://t.co/ZTGR5dNCBo',\n 'Governments and central banks globally have pledged a dizzying $3 trillion-\\x9dand counting. But companies are struggli… https://t.co/YrbmCS9ZX9',\n 'IMF encouraged by recovery in China, but pandemic could resurge https://t.co/UCjDmDNwoP https://t.co/Rua86ZJmMO',\n 'Instant View: Fed launches unprecedented range of credit support https://t.co/3WQX0rnURD https://t.co/YAnONEz11p',\n 'Italy announces guarantees for bank loans worth over 400 billion euros https://t.co/xmvuhapL8v https://t.co/lOpNKjwNeJ',\n 'Mexico cuts its key rate for a 5th time amid economic stagnation https://t.co/HqpKAGRDt7 https://t.co/Jx7BHGv88y',\n \"Mexico's central bank cut its key interest rate for a fifth straight decision https://t.co/xnfKe4SFXt\",\n \"Mexico's central bank cuts interest rates to three-year low https://t.co/0N0vaHBnQj\",\n 'Powell Admits \"Low Rates Are Not A Choice Any More\", Says QE Will Be Used In Next Downturn | #FreeZeroHedge https://t.co/RFVxk9uMth',\n \"Powell: Fed will use 'QE' aggressively to fight next recession https://t.co/me3x7SJac6\",\n \"SNB's sight deposit holdings rise by second-highest level in 12 months https://t.co/aUsTEz08h3 https://t.co/rMaW0F4suE\",\n 'The Bank of Korea is taking unprecedented action in the face of coronavirus-\\x9dnot about the rate decision itself, but… https://t.co/CmxUuSBiab',\n 'The Fed moved to bolster a new small-business lending program by allowing banks to turn those loans over to the U.S… https://t.co/Y2piO9HcrP',\n \"The Fed's foray into corporate bonds and certain credit ETFs helping restore order between the two markets https://t.co/lkJuQCQEKV\",\n \"The U.K. is tapping fintechs and the Queen's Bank for its emergency lending program https://t.co/sJJTyjCVMa\",\n 'FOMC dot plot indicates a long pause https://t.co/F6wezJ7rlP',\n '\"In my time at the Fed, I\\'ve seen how changes in business and consumer confidence impact the economy as a whole,\" P… https://t.co/rZgYHFXsKk',\n '\"It\\'s not really time yet to stimulate spending,\" former Fed Chair Ben Bernanke said today, adding that it will \"ha… https://t.co/iYSEFhCzGY',\n '\"The Committee also directs the Desk to continue conducting term and overnight repurchase agreement operations at l… https://t.co/1W8onnWjru',\n '\"the Committee directs the Desk to continue purchasing Treasury bills at least into the second quarter of 2020 to m… https://t.co/b1GHWeY0dy',\n '\"The Fed Can Lend To Anybody\": Watch Live As Bernanke Holds Virtual Discussion On US Economy https://t.co/oCbErkJi7h',\n '\"What we clearly see again is unelected central banks, having failed at their mandates, to pivot to politics to try… https://t.co/U4Ft0573Ro',\n \"$RINF $IVOL - Fed's Daly discusses shifting focus to higher inflation https://t.co/4xmIHceCvh\",\n '.@TimLampkin of @higherpurposeco says that being a part of the St. Louis Fed’s Community Development Advisory Counc… https://t.co/spGCRWBKVo',\n '@TimLampkin of @higherpurposeco says that being a part of the St. Louis Fed’s Community Development Advisory Counci… https://t.co/Ex5SCguwe4',\n 'A Rate Cut From Russia?',\n \"A WSJ primer on the central bank's novel $600 billion direct-loan program https://t.co/43O0vIETta\",\n 'An emerging priority for Powell Fed: The plight of the poor https://t.co/jP1T0yjdQ4 https://t.co/2DQRic3UvN',\n \"Applications for participation in the Fed's Commercial Paper Funding Facility must be submitted by Thursday, the Ne… https://t.co/DvwRfmIJXr\",\n 'Argia Sbordone shared findings from \"Anchoring of Inflation Expectations\" at today’s joint New York Fed-European Ce… https://t.co/yXvQvWPXZm',\n 'Bank of Canada can buy corporate, municipal bonds if necessary #taux #MarketScreener https://t.co/38OhHeNmWA https://t.co/cowCcOVS13',\n 'Bank of Canada gets authority to buy corporate, municipal debt https://t.co/j9DJUMtl3D',\n 'Bank of Canada’s Poloz Sees Era of Low Global Interest Rates',\n 'Bank of England activates emergency liquidity measure https://t.co/FvgTR5d5eg https://t.co/FZWcMjZddZ',\n \"Bank of England Governor Mark Carney offers his backing to a key plank of UK Prime Minister Boris Johnson's economi… https://t.co/r5qQSOrsPG\",\n \"Bank of England Mark Carney is hanging up his hat. So what's next for the central bank leader?\\n\\n@flacqua sits down… https://t.co/xxqMzXupmE\",\n 'Bank of Ireland : Gov Co - 7 KB #BankofIreland #Stock #MarketScreener https://t.co/lPLIZQTwme https://t.co/iFkjaIFL5u',\n 'Bank of Ireland : Notification of Significant Shareholding - 99 KB #BankofIreland #Stock #MarketScreener… https://t.co/f4rdA0nB11',\n 'Bank of Ireland : Pillar 3 Disclosures #BankofIreland #Stock #MarketScreener https://t.co/V8GNASXXiY https://t.co/An3gEqsXmw',\n 'Bank of Ireland : to make branch network changes in response to COVID-19 #BankofIreland #Stock #MarketScreener… https://t.co/2rKs52jf1Y',\n 'Bank of Jamaica : 14-Day Repo Auction Results #BankofJamaica #economy #MarketScreener https://t.co/UE7vS1tv6o https://t.co/efXjAe32Fg',\n 'Bank of Jamaica 14-Day Repo Auction Results -10 February 2020 #Stock #MarketScreener https://t.co/wBnwfayNhV https://t.co/bQ3CSymjTi',\n 'Bank of Jamaica 30-day CD Auction Press Release - 24 February 2020 #Stock #MarketScreener https://t.co/iRuPoA3LTD https://t.co/MjLqnkV6mj',\n 'Bank of Japan : Accounts (March 20) #BankofJapan #Stock #MarketScreener https://t.co/Di0sVY7MoH https://t.co/GoCJarxay0',\n 'Bank of Japan : Statistics on Securities Financing Transactions in Japan (Jan. 2020) #BankofJapan #economy… https://t.co/sD0oDaY7rX',\n 'Bank of Japan : The Results of BIS International Locational Banking Statistics and Interna... #BankofJapan #Stock… https://t.co/z2dSe5N09a',\n 'Bank of Mexico Makes Fifth Consecutive Rate Cut -- Update #economy #MarketScreener https://t.co/NfkbjZjf6D https://t.co/otogWpgEnu',\n \"Bank of Nova Scotia : Acting Administrator Pilkerton's Statement on USMCA Agreement #BankofNovaScotia #Stock… https://t.co/jGxhlOVCTF\",\n 'Bank of Queensland : Trading Halt #BankofQueensland #Stock #MarketScreener https://t.co/P7JUKMwkZ1 https://t.co/AYwhOiYQbV',\n 'Banks To Make Risk-Free Killing On Small Business Bailout: Fed Will Buy Payroll Loans Issued By Banks https://t.co/ujsbBPAq9K',\n \"BoE's Haldane: We have not yet hit peak globalisation #economy #MarketScreener https://t.co/BAMp44H0w2 https://t.co/yUYhes88V2\",\n 'BOJ Seen Staying on Hold This Week After Abe Stimulus: Survey',\n 'Brazil Caps Record-Breaking Easing Cycle With Final Rate Cut',\n 'Canadian Banks Have Succulent Yields. https://t.co/QaD9D53X7f #economy #business #trading',\n 'Central Bank of Philippines : Bangko Sentral ng Pilipinas Convenes the Financial Education...… https://t.co/mFmcU9A7EI',\n 'Central banks must evolve to help governments fight coronavirus https://t.co/mfSJuTKUDm',\n 'Chair Powell presents the Monetary Policy Report to the Senate Committee on Banking, Housing, and Urban Affairs:… https://t.co/TAfePghHQO',\n 'Chairman Jerome Powell speaks after Fed leaves interest rates unchanged https://t.co/lA3GN9o7em',\n 'China still has room for conventional monetary expansion, but its ability to deal successfully with a crisis depend… https://t.co/jR2R3VLkOo',\n 'China to Price Existing Loans With New Benchmark Lending Rates #taux #MarketScreener https://t.co/PELmoyfWz4 https://t.co/Nq8pxwGr8U',\n 'China’s Rate Cut Also An Insurance Policy As Hong Kong Crisis Worsens',\n 'Chinese Media Stunner: China Will Be The Next Country To Cut Rates To Zero https://t.co/MQl6OREX0H',\n 'Christine Lagarde Can Face Down the ECB Skeptics',\n 'Cleveland Fed President Says Coronavirus Requires Careful Monitoring #economy #MarketScreener… https://t.co/YbgcgoxZPk',\n 'Commerzbank told to speed up restructuring by ECB https://t.co/DfFV4bgnfi',\n 'Coronavirus fears raise market expectations for Fed rate cut in March https://t.co/Z33zqdljVu by @bcheungz https://t.co/XwHo1ZlMR2',\n 'Czechs’ Unexpected Rate Hike Lifts Koruna to Seven-Year High',\n \"Dealers gobble up funding in Bank of Canada's first BAPF operation #economy #MarketScreener https://t.co/YmwzTi5reu https://t.co/uQPDBGe66r\",\n \"Derby's Take: Even Post-Coronavirus, Swollen Fed Balance Sheet Will Remain https://t.co/5uPrJkM8xc\",\n \"Did The Fed's QEternity Just Kill A Fiscal Stimulus Deal?  https://t.co/a8f6L67zgl\",\n 'ECB keeps rates unchanged',\n \"ECB's house price headache too big to solve\",\n \"ECB's new chief Christine Lagarde has a deal for bank policymakers - cut out the open dissent and she'll spend more... https://t.co/q8kkm1J7Cz\",\n \"Egypt's central bank governor appointed for second term -state media #economy #MarketScreener… https://t.co/D6r5mixdx5\",\n 'Emerging-economy central banks from India to Brazil still enjoy the firepower to shore up the global economy https://t.co/cTVUGNf4vM',\n 'EU bank regulators lay out tougher doomsday stress test https://t.co/4DMWnZSihp',\n 'European Central Bank keeps key deposit rate at -0.5%',\n 'Expect next round of virus stimulus by May, and somewhere in the $1.5 trillion neighborhood - FBN, citing Wall Stre… https://t.co/TIHDm8kJnl',\n \"Explainer: How the Fed's latest move will get money to Main Street #economy #MarketScreener https://t.co/x3pgUHAqsa https://t.co/NtM1SzX2fR\",\n 'Exploration deepens into central bank digital currencies',\n 'Fed Adds $72.8 Billion to Markets, Balance Sheet Moves to $4.07 Trillion--Update #economy #MarketScreener… https://t.co/am5nGCnG5b',\n 'Fed Aims a Half-Trillion Dollar Liquidity Hose at Year-End Risks',\n 'Fed bombards credit markets with limitless pledge, but investors barely respond https://t.co/KfTfB94OD1',\n \"Fed Chair Jerome Powell says  “Low rates are not really a choice anymore, they are a fact of reality'\\x9d https://t.co/IblPVbnP0s\",\n 'Fed Chairman Jerome Powell puts lawmakers on notice that fiscal policy may need to play a bigger role countering do… https://t.co/gQ6Eee8hhs',\n \"Fed Chairman Powell thought it was 'terrific' that a senator defended him against Trump https://t.co/VyMW1XR0OM\",\n 'Fed Has Given Economy a Cushion and Should Cut Mid-Year, Economist Swonk Says',\n 'Fed Intervenes With $45.55 Billion Weekend Repo, But Overall Liquidity Ticks Down https://t.co/luXKbrLrXC',\n 'Fed is ‘super-glued’ to its seat until after the election, economists say after stellar jobs report https://t.co/WgE7fNtuQu',\n \"Fed is now effectively the 'lender of last resort' to Main Street not just Wall Street https://t.co/acyqwv78jZ\",\n \"Fed is watching virus effects, flags 'elevated' business debt in report\",\n 'Fed Monitoring Coronavirus Outbreak and Risks for Broader Disruptions -- 4th Update #economy #MarketScreener… https://t.co/KnkPKhBHHM',\n 'Fed nominee Judy Shelton faces bipartisan concerns in Senate https://t.co/av7l6k1mk7',\n 'Fed nominee Shelton gets bipartisan grilling in Senate https://t.co/lJXCmNEKuU',\n 'Fed policymaker says more evidence needed for rate cut https://t.co/bmwrgql7zj',\n 'Fed Preparing to Purchase New Small Business Payroll Loans https://t.co/iz5xKoWFwC',\n 'Fed Provides More Liquidity; Phase 1 Trade Deal, But No Corroboration On Jobs Report',\n 'Fed Pumps $70.2 Billion in Short-Term Liquidity Into Markets #currency #MarketScreener https://t.co/H1YtUn9FoK https://t.co/Nrvkw9LLnm',\n 'Fed Repo Action Oversubscribed in Clamor for Year-End Funds',\n 'Fed report: #OpportunityOccupations, jobs that don’t require a 4-yr #collegedegree & pay above the national annual… https://t.co/Elr1SXOcDD',\n \"Fed says it will provide financing against new 'payroll protection' loans #economy #MarketScreener… https://t.co/A4ierJsls4\",\n 'Fed says Powell met with Trump, Mnuchin on Monday at White House',\n 'Fed sets up facility to create secondary market for new small business loans https://t.co/nu8LLQC4rJ',\n 'Fed to beat a faster retreat from repo market https://t.co/jEGsYl2DvM',\n \"Fed to establish facility to facilitate lending to small businesses via SBA's PPP by providing term financing backe… https://t.co/41GvG98U17\",\n 'Fed Unveils Major Expansion of Market -2- #economy #MarketScreener https://t.co/Flf1QDVuCH https://t.co/yzwBwPp89h',\n 'Fed Unveils Major Expansion of Market Intervention -- 4th Update #economy #MarketScreener https://t.co/2bnBJUcxTD https://t.co/0RgMApfhO8',\n 'Fed warranty: will keep rates unchanged until 2021, or until there is a 5% drop in stocks, whichever comes first',\n 'Fed: Powell stressed policy dependent on incoming information',\n 'Fed’s Jerome Powell steered divided committee toward policy shift in effort to bolster economy  https://t.co/3PucAE0bbf',\n 'Federal Reserve Bank of St. Louis President James Bullard said that the U.S. should declare a three-month break for… https://t.co/cAhdYJWnk9',\n 'Federal Reserve Board announces termination of enforcement actions with Discover Financial... #Stock… https://t.co/sMoeOMo1N8',\n 'Federal Reserve Chairman Jerome Powell came close to acknowledging that the central bank may not have the firepower… https://t.co/jl7rxYgzcW',\n 'Federal Reserve Continues To Step Up. https://t.co/aCj2ASpOba #stocks #markets #economy',\n 'Federal Reserve just saved the stock market from the worst of the coronavirus pandemic -\\x9dit may be time to invest ag… https://t.co/nRWrlgdlEZ',\n 'Federal Reserve lowers community bank leverage ratio https://t.co/SXUmdUzINW https://t.co/dyT4UYvJT2',\n 'Federal Reserve Shows Specifically How Much Uncertainty Has Hurt the Stock Market',\n 'Federal Reserve System publishes annual financial statements: https://t.co/TjqI5AXBCq',\n 'Federal Reserve to backstop Payment Protection Program loans https://t.co/YTnMrTby81 by @bcheungz https://t.co/HiWgPrK4Su',\n \"Fed's Bullard: Don't take the economy's temperature until July 1 https://t.co/qqv1J5MgAd\",\n \"Fed's Kaplan says revisiting some stress testing rules could help ease repo  pressures\",\n \"Fed's Kaplan Sees Risks to Outlook as 'Fairly Balanced'\",\n \"Fed's Kaplan Talks Economic Outlook, Trade and Credit Spreads\",\n \"Fed's Mester says policymakers want to prevent 'lasting damage' to the economy https://t.co/VYMW1cZvaW https://t.co/Mut4kwmdby\",\n \"Fed's Mester says she opposed recent series of rate cuts but called it a 'close call'\",\n \"Fed's Mester sees 'downside risk' from coronavirus, but does not want to cut rates https://t.co/P6XtdRcjNI\",\n \"Fed's Mester sees U.S. economy performing well, coronavirus a 'big risk' #economy #MarketScreener… https://t.co/fHOfgB9n6R\",\n \"Fed's Mester would have preferred no rate cuts in July, September\",\n \"Fed's Powell disputes notion that federal welfare programs are dissuading Americans from seeking work https://t.co/IpK9Fa1fOO\",\n \"Fed's Powell to update on economy Thursday in webcast statement #economy #MarketScreener https://t.co/w8Mog9t66T https://t.co/liSE7rRvh4\",\n \"Fed's Powell: U.S. economy 'resilient' but coronavirus a risk #economy #MarketScreener https://t.co/du44jHD8eC https://t.co/Y8Cv1f4Ksm\",\n \"Fed's Rosengren urges patience in considering policy changes\",\n \"Fed's Williams says interest rates policy is in a good place but 'not locked-in'\",\n 'Finance ministers and central bank governors from the Group of 20 nations joined an emergency call on Monday https://t.co/61CFPU3q2M',\n 'Financial conditions are in the driver seat for investors and the Fed rate decisions, says Jeffrey Rosenberg of Bla… https://t.co/hDyPx6PZca',\n \"Former Federal Reserve chairwoman Janet Yellen thinks America's banks should slam the brakes on dividends to make s… https://t.co/66zg0Sd0of\",\n 'Futures traders are no longer fully pricing in a quarter-point cut from the Federal Reserve next year after the U.S… https://t.co/iiGPzuKrF6',\n 'Helicopter money is a valid tool for central bankers https://t.co/M0nfifXDoS',\n 'Helicopter Money Might Be the ECB’s Best and Worst Options',\n 'Hellenic Exchanges : Transactions by Eurobank Equities (regulated information - Law 3556/2... #HellenicExchanges... https://t.co/xpZ379riaL',\n 'Hellenic Exchanges : Transactions by National Bank of Greece (regulated information - Law ... #HellenicExchanges… https://t.co/BpeiSxTRq5',\n 'Here is what the People’s Bank of China is really up to\\nhttps://t.co/FUmGGvzCam',\n 'Honduras - DRM Development Policy Credit with a Catastrophe Deferred Drawdown Option (Cat DDO) #economy… https://t.co/ZsxtwAt8FS',\n 'IMF African Department Director Abebe Aemro Selassie will be on @CNBCAfrica today at 5pm Lagos Time (12pm DC Time)… https://t.co/xMWhTEke42',\n 'IMF and World Bank consider #Somalia eligible for debt relief on initial assessment. This is an important milestone… https://t.co/mnGkqZxsWb',\n 'IMF to continue new-loan talks with Ukraine in coming weeks #economy #MarketScreener https://t.co/z7E36jR1i8 https://t.co/MQQ8AxnaGI',\n 'In \"Unprecedented\" Move To Ease Conditions, ECB Cuts Collateral Haircuts By 20%, Will Accept Greek Debt As Collater… https://t.co/SU2jKATE3I',\n \"In first #MENA emergency financing, the IMF approved US$745 million to support #Tunisia's efforts to address the hu… https://t.co/j5dfvZCBza\",\n 'India Unveils Steps for Bad Loan-Laden Banks to Boost Credit',\n 'Indonesia’s central bank sounded a more cautious tone on interest rates https://t.co/XvYExp61ZH',\n 'Inflation remains low and Fed Chairman Jerome Powell has as good as ruled out a rate increase unless price growth p… https://t.co/EMQ6vAOo55',\n 'Interest rate conditions of the Magyar Nemzeti Bank effective from 8 April 2020 #economy #MarketScreener… https://t.co/LtgoxFiEBO',\n \"It may seem odd for the Federal Reserve to buy ETFs, but it's not unprecedented for a central bank -œ Japan did it f… https://t.co/Tm7u3av8zC\",\n 'It was 7 years ago that Jerome Powell said \"we look like we are blowing a fixed-income duration bubble right across… https://t.co/eJHWtlyGo4',\n 'Italian banks ready new measures as economy stops, but debt goes on https://t.co/ZMrEl3ZDOL https://t.co/AB8Hpn3BLN',\n 'James Bullard, President of the Federal Reserve Bank of St. Louis, discusses the Federal Reserve\\'s \"planned tempora… https://t.co/NEVONwysvh',\n 'Japan panel warns against fiscal complacency amid low rates #taux #MarketScreener https://t.co/bgHBLjLZaW https://t.co/6F4t7RytXF',\n 'Jerome Powell told lawmakers Tuesday that the Fed will never declare victory on one of its mandates: full employment https://t.co/lnEGDrhUln',\n 'Kenya Cuts Rates After Scrapping Loan-Price Cap to Boost Growth',\n 'Kenya’s central bank lowers interest rates for the first time in 16 months https://t.co/7SQ003stA9',\n 'Kenya’s central bank lowers interest rates for the first time in 16 months https://t.co/yQpS4kMTOD https://t.co/ExAA5O8fT1',\n 'Komileva: Central Banks Far from Normalizing',\n 'Lagarde Says All the Blame Can’t Be Dumped on ECB Negative Rates',\n 'Latin America Price Surprise Fails to Move the Dial on Rate Bets',\n 'Lawmakers interviewed gave Fed Chairman Jerome Powell high marks for his unwillingness to be drawn into a spat with... https://t.co/hHopJ74bbw',\n 'Liquidity Warning: Fed Shrinks Overnight Repos By $20BN, Term Repos By $10BN | Zero Hedge https://t.co/NP2ocBHkvr',\n 'LIVE NOW: Press conference with #FOMC Chair Powell: https://t.co/sh1FXgYlwr and https://t.co/FJa6TbkDMt',\n \"LIVE: Fed Chair Jerome Powell delivers remarks at the decade's last FOMC meeting https://t.co/4wAYSuWpHF\",\n \"LIVE: Fed Chair Jerome Powell speaks on today's decision to hold rates ▶️\\n\\nhttps://t.co/Wfg74SaImY\",\n 'Market participants submitted bids for nearly twice what was on offer in the Fed’s 42-day term repo operation, whic… https://t.co/qIqTKdQtpk',\n \"Mexico's central bank defends autonomy as AMLO concern lingers https://t.co/9CKZZc3uxq\",\n \"Monetary policy is in the 'right place', Fed's Williams says\",\n 'Monetary Policy Statement - March 24, 2020 (24.03.2020) #Stock #MarketScreener https://t.co/BkGvDXrRZL https://t.co/DaEOwqXxuI',\n 'Monetary Policy: Das’ Draghi Encore Comes With Risks',\n 'Monetary Policy: RBI Returns To A More Active Credit Policy To Boost Retail,  MSME Credit',\n 'MPC Decision: MPC Keeps Rates On Hold Amid ‘Highly Uncertain’ Inflation Outlook',\n \"N.Y. FED'S 42-DAY REPO OP. OVERSUBSCRIBED; $49.05B OF BIDS\",\n 'New BoE governor makes plea to Brussels on City access https://t.co/5X7rXAAsHG',\n \"New developments added to @FedFRASER's COVID-19 timeline in the latest week: second historic rise in unemployment i… https://t.co/o4yYfNRbhA\",\n 'New staff study by Kenneth Garbade examines the efforts of the Federal Open Market Committee to first control, and… https://t.co/seVxzGnVOu',\n 'New York Fed opens registration for commercial paper funding facility https://t.co/K5ZI55Mei5 https://t.co/eUnLPyar0E',\n 'New York Fed to further reduce repo support https://t.co/weKgRz0LMd https://t.co/UR8cWv9xZN',\n 'New York Fed to start publishing SOFR averages in March https://t.co/O0sIXhkddG',\n 'New Zealand’s central bank doesn’t expect its new bank capital rules to present a headwind for the economy, which l… https://t.co/YXwseGIYHw',\n 'Next BoE governor makes City access plea to Brussels https://t.co/3HVBMfL9Ze',\n \"Norway's central bank governor warns politicians against meddling too much with the country's massive sovereign wea… https://t.co/H7bqZcQjoi\",\n 'Now available: Weekly data on the Assets and Liabilities of Commercial Banks in the United States: https://t.co/Q478LdiYLB #FedData',\n 'NY Fed Offers First Year-End Repo, Adds $93.5 Bln in Cash Banks Seek Liquidity\\nhttps://t.co/twaYHd2PDZ\\n@mdbaccardax',\n \"Off the 'QT': Does Federal Reserve nominee Judy Shelton have a balance sheet problem? https://t.co/masWFSb9oB https://t.co/RyPS7DYCC3\",\n 'Once we have more clarity about the new coronavirus outbreak, expect interest rates to rise, says Quicken Loans CEO… https://t.co/OOv4bsPKpX',\n \"Orr May Signal Readiness to Cut Rates as Virus Hits New Zealand's Economy\",\n 'Parkmead : Bank of Jamaica 14-Day Repo Auction Announcement -25 November 2019 #Parkmead #Stock #MarketScreener… https://t.co/cMHuPB7ZFM',\n 'Patrick Harker, President of the Philly Fed, is due to speak now.',\n 'PBOC Tells Property Speculators That ‘Homes Are For Living In’',\n 'PBOC Will Resolve Risks Gradually as Downward Pressure on Chinese Economy Has Increased #economy #MarketScreener… https://t.co/wMzcC38OfV',\n 'Peter Conti-Brown On The CARES Act And The Expanding Fed-Treasury Relationship In Response To COVID-19.… https://t.co/5rEILyYuNC',\n \"Powell's 'half-full' U.S. glass sturdy but still at risk for spills as Fed meets\\nhttps://t.co/HPxoBCxi3B https://t.co/NQqEENkR6e\",\n 'President Trump meets with Fed Chairman Powell to discuss economy',\n 'Press release on the Monetary Council meeting of 7 April 2020 #economy #MarketScreener https://t.co/vSMuCNmMuE https://t.co/13ghOSot3X',\n 'Property Lenders Surge After India Central Bank Eases Cash Rules',\n 'QE Infinity: So It Begins. https://t.co/HppY8KK3fV #business #stocks #stockmarket',\n 'Rate futures surge as coronavirus seen pushing Fed to ease https://t.co/Fi8x6k42Ap https://t.co/MPjtg6wicr',\n 'Read the latest FOMC statement https://t.co/VCPbCo6m61',\n 'Regional Fed leaders favored holding rates steady for now, but that any worsening of novel coronavirus could affect… https://t.co/tcZzqWkz52',\n \"Rise of the intangibles is pushing down on central bank rates - BoE's Haskel https://t.co/3gYMZ51yzI https://t.co/yFeH07V30c\",\n 'Robert Kaplan Says Fed Should Hold Barring ‘Material’\\xa0Change in Outlook',\n 'Russia Delivers Sixth Straight Rate Cut, Signals Further Easing',\n 'Russian and Romanian Rate Decisions, Commodity Chaos: Eco Day',\n 'Saudi Arabian Monetary Agency : SAMA Licenses HalalaH and BayanPay Companies #SaudiArabianMonetaryAgency #economy… https://t.co/mCmqEEmaP0',\n 'SBI Cuts MCLR-Based Lending Rates By 5 Basis Points Across Tenors',\n \"Sizing Up the Fed's Historic Coronavirus Crisis Intervention #Samp;P500 #economy #MarketScreener… https://t.co/o4qzaZPUeG\",\n 'South African Reserve Bank : Regulatory relief measures and guidance to the banking sector...… https://t.co/kXm8dScZTn',\n \"St. Louis Fed President Jim Bullard recommends declaring a  “National Pandemic Adjustment Period'\\x9d and discusses thre… https://t.co/o942U7BKPI\",\n 'Stock market live updates: Former Federal Reserve Chair Janet Yellen said the economy is in the throes of an \"absol… https://t.co/2xqOHCLmEe',\n 'Stock Market Update: President Trump meets with Fed Chair Powell',\n \"Sweden's Riksbank underscores its commitment to zero interest rates, despite cutting its inflation forecast https://t.co/cqDzr8JVZ9\",\n 'Swedish central bank set to keep rates steady on Feb. 12, and through 2021: Reuters Poll',\n \"Thailand's central bank faces a fresh battle to contain the baht's strength\",\n \"The Bank of England is launching an extra weekly liquidity facility after banks' demand for cash soared at its regu… https://t.co/N8q2VXLg5u\",\n 'The Bank of Israel extends its pause in interest rates, surprising most economists https://t.co/8lydT0Ph6e',\n 'The ECB dived into the fight to save the economy with 34 billion euros of bond purchases last week https://t.co/JNSEDvs9lt',\n 'The ECB ramped up its government bond buying program even before it boosted quantitative easing by an extra $805 bi… https://t.co/bYKZM2CTjP',\n 'The ECB will accept junk-rated Greek government bonds as collateral as part of wide-ranging measures to ease lender… https://t.co/8Kx1J07gTl',\n \"The ECB will probably decide to take better account of home costs in its inflation measure, but there's unlikely to… https://t.co/Kg9elTbD90\",\n \"The Fed and the Treasury will start buying debt issued by banks that lend through the government's small businesses… https://t.co/ZE7SV2CCmR\",\n 'The Fed and Treasury Department are planning to launch a program to buy loans that financial firms make through the… https://t.co/DUUAXs3yzq',\n 'The Fed announced a massive second wave of initiatives to support the U.S. economy, many specifically targeting com… https://t.co/6wgz15jypX',\n 'The Fed announced new programs to help markets, the NYSE begins trading remotely today, and Trump activated the Nat… https://t.co/6QWlnqOWDy',\n 'The Fed Faces a Housing Conundrum',\n 'The Fed is going all in to try and ramp up the economy. Will it be enough? Insights via @CMEGroup https://t.co/BpEqm1utJ9',\n 'The Fed is going all in to try and ramp up the economy. Will it be enough? Insights via @CMEGroup https://t.co/RxmrkDKQOa',\n 'The Fed is going to buy ETFs. What does it mean? https://t.co/t0Gh4NVwfQ',\n 'The Fed Is Now Buying Investment Grade Bond ETFs Like LQD  https://t.co/ZQC24DFN3L',\n 'The Fed is on hold for now, but it might not take much to change that',\n 'The Fed may be about to get a lot more involved in the bond market (via @bopinion) https://t.co/WxC8AWRRaS',\n 'The Fed plans to lend directly to companies outside the banking system. Former Fed Governor Narayana Kocherlakota s… https://t.co/WIJjtKBYJD',\n \"The Fed Says 'Helicopter Money' Is Here. https://t.co/JRqmtSRtbf #stocks #business #stockmarket\",\n 'The Fed sidesteps a bickering Congress with sweeping rescue plan https://t.co/SBxYPhDR2q',\n \"The Fed took the lid off QE. Here's how the central bank has been responding to the coronavirus crisis… https://t.co/z51VeD10Tb\",\n \"The Fed will create a new facility to help speed support to small businesses through the government's coronavirus s… https://t.co/M22EUfsoP9\",\n 'The Fed will launch a barrage of programs aimed at helping markets function more efficiently in the wake of the cor… https://t.co/kSh6hJKgyo',\n 'The Fed’s interventions are aimed at ensuring that the financial system has enough liquidity and that short-term bo… https://t.co/cwaCeyj97h',\n 'The Federal Reserve assured the nation that the Central Bank has an essentially limitless ability to provide liquid… https://t.co/43Z7zVmKtL',\n 'The Federal Reserve Bank of New York added liquidity to financial markets Thursday in what is effectively a short-t… https://t.co/XWjrSLEXbj',\n 'The Federal Reserve is signaling it will do whatever it takes to save the coronavirus-ravaged American economy from… https://t.co/EB9feds0vx',\n 'The Federal Reserve is wading into the $3.9 trillion municipal-bond market. \\n\\nBut the central bank still has more l… https://t.co/CVR76uG6LQ',\n 'The Federal Reserve on Monday announced a massive second wave of initiatives to support a shuttered U.S. economy https://t.co/24MqL6jTmM',\n 'The Federal Reserve on Monday once again stepped in to shore up the lending market as it rolled out an array of pro… https://t.co/9U9ndPky3a',\n 'The Federal Reserve on Monday rolled out an extraordinary array of programs to backstop an economy reeling from res… https://t.co/uCAEJ41Rzs',\n \"The Federal Reserve took more extraordinary measures to keep the economy from seizing up, but it can't soften the b… https://t.co/SsvXAyhVr3\",\n \"The Fed's Emergence as a Power Player Poses New Risks to Its Independence #economy #MarketScreener… https://t.co/hwe3xwwGma\",\n 'The IMF has three main roles: economic policy advice, lending, and capacity development. How did we serve our 189 m… https://t.co/XreOyj15Bu',\n 'The IMF is providing emergency financial assistance to member countries facing the economic impact of the #COVID19… https://t.co/Vu7rLIfpN7',\n \"The IMF may launch a new program that could back up the Fed's campaign to keep dollars flowing in the global economy https://t.co/wOOa8e7wX6\",\n 'The IMF says that Uganda will spend as much as a fifth of government revenue on interest payments https://t.co/upwkLxCclO',\n \"The man about to become Britain's top central banker says the U.K. should be free to change its own rules, ahead of… https://t.co/K7CYXMDfaf\",\n \"The New York Fed said it will shrink its repurchase agreement operations further, starting with Friday's overnight… https://t.co/eAhxhyAocY\",\n 'The NY Fed said it accepted $25 billion in bids, from a total of $49 billion placed at the central bank  https://t.co/qmdK6LGP1t',\n 'The quarterly handoff in Treasury shorts is riskier than normal https://t.co/BlFNMTJ4Pw',\n \"The Russian government tapped its sovereign wealth fund to purchase the central bank's 50% holding in the country's… https://t.co/XNbtGWFcog\",\n 'The St. Louis Fed is proud to assist @USTreasury in presenting Your Guide to America’s Finances, an annual snapshot… https://t.co/1VqagVX5ok',\n 'The stricken Bank of Jinzhou will unload $21 billion of assets to the central bank for less than a third of theirÃ‚Â\\xa0r… https://t.co/xGuCP9lBlH',\n 'The target for the overnight interbank funding rate is decreased by 25 basis points #Stock #MarketScreener… https://t.co/pUpRxfyNco',\n 'The U.S. should declare a 3-month break for nonessential businesses, says St. Louis Fed president https://t.co/vsE3wjGyHk',\n 'The worst scramble for cash is happening in an opaque corner of the market, where the Fed has little control, write… https://t.co/1ZddVnhX3j',\n 'Today, the IMF Executive Board approved a disbursement of €176.5 million to #NorthMacedonia. This financial suppor… https://t.co/hloz3ZRGNH',\n \"Traders are now trying to get ahead of the Fed's purchases https://t.co/UTF5phTB1V\",\n 'Traders Maintain Amount of Fed Easing Seen in 2020 After Jobs',\n 'Trump doubles down on lower rates at Powell meeting',\n 'Trump says he talked about interest rates, dollar strength with Powell',\n 'Tunisia eyes IMF talks in March on sixth review of loan deal -minister #economy #MarketScreener… https://t.co/9eio1gEMyk',\n 'Turkey is trying to stop emergency loans intended to boost its economy during the coronavirus slowdown from fueling… https://t.co/EcSwShpQGl',\n 'Two Regional Fed Chiefs Lay Out Aggressive Coronavirus Response -- Update #economy #MarketScreener… https://t.co/9OUm67AaSL',\n \"Two top BOE officials suggested that the UK financial system's rules may have to diverge from the EU's after Brexit https://t.co/85NEJl6fLm\",\n 'U.S. banks cram for Fed risk test, with ripple effects in repo #economy #MarketScreener https://t.co/Hvz8lXjRl0 https://t.co/Dg0f2nRHHP',\n 'U.S. Fed buys $427 million of mortgage bonds, sells none https://t.co/ezA4AiwOlC https://t.co/z4h3yRWGTj',\n \"U.S. to Block Iran's Request to IMF for $5 Billion Loan to Fight Coronavirus -- Update #economy #MarketScreener… https://t.co/n3Crpty48r\",\n \"U.S. Treasury, Fed working on 'Main Street,' municipal loan facilities - Mnuchin #economy #MarketScreener… https://t.co/xlHUAdkJG9\",\n \"UK banks' dollar demand rises to $3.555 billion: BoE repo operation https://t.co/D5YDDpaj13 https://t.co/licT21042C\",\n 'UK watchdog gauging how virus affects plan to retire Libor https://t.co/B1wWDKFasL https://t.co/3ZDXFQvGtE',\n 'US Fed admits Libra has potential to ‘rapidly’ achieve mass adoption',\n 'US Treasury To Ask For $200 Billion More In Small Business Loans https://t.co/MpQKXdG9Xt',\n 'Vital conversations happen all around the Eighth Federal Reserve District. One of the ways we listen is through our… https://t.co/tsLMen6MSn',\n \"Watch Fed Chairman Jerome Powell's news conference live https://t.co/AbOHHqlbeA\",\n 'Watch Live: Fed Chair Powell Avoid Admitting QE4 Has Begun & Repocalypse Is Imminent https://t.co/BlKwurqscz',\n \"WATCH: Fed Chair Powell holds a press conference following the FOMC's unanimous decision to leave interest rates un… https://t.co/od2QG12LGo\",\n \"'We know that there will be very likely some effects on the United States', said the Fed chairman Jay Powell about… https://t.co/eAeVR5GUm2\",\n 'Well laid plans at the Federal Reserve keep getting blown off course by the trade war (via @RichMiller28 &… https://t.co/ESn0p1N19h',\n \"What the Fed meeting minutes could say about interest rates that Powell didn't say\",\n 'What to look for when Fed Chair Powell meets with Congress this week',\n 'Will the Fed provide any stimulus to the U.S. economy? Look to the equities market for clues. Insights via @CMEGroup https://t.co/zk1s08F1zW',\n \"With coronavirus ravaging the US economy, the Federal Reserve may have to get creative. CNN Business'… https://t.co/wqKtHN3mVC\",\n 'With latest moves, Fed becomes creditor in chief for U.S. business #economy #MarketScreener https://t.co/LjIQ3iMwmv https://t.co/CY4JHRFSxb',\n 'World Bank could deploy $150 billion over 15 months in coronavirus battle - Malpass #economy #MarketScreener… https://t.co/lU5FmuUBUa',\n 'World Bank Suggests Creating ‘Brand Northeast’',\n \"Yellen says the Fed doesn't need to buy equities now, but Congress should reconsider allowing it https://t.co/ms6Y9qvmUK\",\n 'Zimbabwe Central Bank Reverses Policy and Halves Key Rate to 35%',\n '\"I’ve owned the Apple Card for 3 months…this is why it sucks.\" https://t.co/ngzSpdvFPK',\n '\"That\\'s Not Safe! Somebody\\'s Going To Die!\": Senator Markey Slams Tesla Autopilot At Commerce Committee Hearing https://t.co/PHvFLxKnQu',\n '\"We Totally Failed As A Business\": Unicorn Scooter Impales Investors After Company Goes Hooves-Up https://t.co/otp1fQAFgj',\n '$300,000 Pilot Jobs Drying Up in China After Boeing Grounding',\n '$AAL - American halts flights to Mainland China https://t.co/zQIKTbqqUk',\n '$AMGN - Partial response in Amgen brain cancer study https://t.co/6sAXBkTCST',\n '$AXAS - Abraxas Petroleum: Poor Liquidity Points To A Chapter 11 May Be On The Horizon. https://t.co/bdKkXTKDOL… https://t.co/PG849ZPziP',\n '$AXAS - Abraxas Petroleum: Poor Liquidity Points To A Chapter 11 May Be On The Horizon. https://t.co/bdKkXTKDOLâ€¦ https://t.co/PG849ZPziP',\n \"$BA - 737 MAX crisis claims job of Boeing's top communications official https://t.co/FwntvthN9F\",\n '$CAT - Caterpillar sees more weak demand ahead as firms defer capital decisions https://t.co/CzAxE8TDL1',\n '$CBAY: CymaBay Therapeutics halts clinical development of Seladelpar based on initial histological findings observe… https://t.co/v8K1pQPrsK',\n \"$CORV - FDA advisory committee thumbs down on Correvio's Brinavess for AF https://t.co/vLfvDUDw9v\",\n '$CTSO: CytoSorbents says temporarily pausing enrollment of REFRESH 2-AKI study at the recommendation of its Data... https://t.co/6ibg4NhPh1',\n '$LJPC - La Jolla Pharma to reassess development of LJPC-401 https://t.co/sJITlHYGuz',\n '$LK - Citron and Muddy Waters at odds over Luckin Coffeee https://t.co/gkLLQ0bBQA',\n '$UBER - Uber CEO comments on London license loss https://t.co/ppLtG1r4lP',\n '$UN $UL - Unilever U.S. recalls some chicken products https://t.co/jLN7s2LAd7',\n '“Vaping is taking us backward,” CVS Health CEO Larry Merlo says. “Something has to be done.” https://t.co/CwxbCtXvWW https://t.co/NBv10IejXx',\n '2020 Will Be Another Year of Pain for Tanger',\n 'After Growing 16% In 2 Years, Why Did Lowe’s Number Of Stores Decrease?',\n 'Airbus : Boeing deliveries halved in first eleven months of 2019 #Airbus #Stock #MarketScreener… https://t.co/5Ux2IqpCAP',\n 'Apple closing all China stores and offices',\n 'Apple saw China iPhone units fall in November, Credit Suisse says',\n 'As a reminder, JPMorgan remains the riskiest bank with the highest GSIB score of any bank and a 4.0% surcharge. https://t.co/8jTxnywzqM',\n 'As Liquidation Threat Looms, Jet Airways Bidders Still Not Ready',\n 'Axonics launches $110M equity offering; shares down 3% after hours',\n 'BlackRock admonishes Siemens for environmental actions',\n 'Boeing deliveries halved in first eleven months of 2019 https://t.co/aNHnf3f89F https://t.co/LHeC37Q814',\n 'Boeing Forced to Halt 737 MAX Production',\n 'Boeing to suspend 737 Max production in January',\n 'Boeing, suppliers slip as Wall Street counts MAX halt costs',\n \"Boeing's halt to 737 MAX production could ding U.S. economy, jobs\",\n 'Breaking - United $UAL suspends flights to China as coronavirus outbreak continues',\n \"Bristol-Myers' Opdivo + Yervoy disappoints in melanoma study\",\n 'Bumble Bee files for bankruptcy after $25 million fine for tuna price fixing https://t.co/nLaDnUUmKh',\n 'Bumble Bee Foods filed for bankruptcy, blaming its \"recent and significant legal challenges.\" https://t.co/kFYH8KrLKA',\n 'Burberry says coronavirus hurting luxury demand',\n 'Burberry Says Viral Epidemic\\xa0Devastates China Sales',\n 'BW Group trims its stake in DHT Holdings',\n \"California department to reject Sezzle's application for lending license in state\",\n 'Canadian National Laying Off Workers',\n 'Carmakers close Chinese factories because of coronavirus',\n 'China’s Coronavirus Set To Dampen Economic Growth In The United States',\n 'Coronavirus reports hang over cruise line sector',\n 'CORRECT: Tapestry estimates 2020 negative impact of $200 mln-$250 mln from coronavirus outbreak in China',\n 'CORRECTED-Nissan recalls nearly 400,000 vehicles over braking system defect (Nov. 16)',\n \"Credit Suisse's Top Investor Wants Chairman to Follow CEO Out\",\n 'Delta and American airlines halt flights to China in response to coronavirus https://t.co/XtxOndVRIg',\n 'Delta has decided to temporarily suspend all flights from the US to China from February 6 through April 30, due to... https://t.co/vJLfdbPhvV',\n \"Disney+ faces backlash over 'The Simpsons' aspect ratio\",\n 'DNA testing company Veritas Genetics is suspending American operations and cutting all U.S. staff after a financing… https://t.co/LZZNsdU41T',\n 'Efforts to save South African Airways by placing it into bankruptcy protection hit a snag https://t.co/ptXgcTsac7',\n 'Electrolux to incur $70 million charge for U.S. overhaul',\n \"Expedia's Problems Run Deeper Than SEO Headwinds\",\n 'Facebook under fire as political ads vanish from archive: FT https://t.co/4gXYZZSiWR https://t.co/NIfMVvmsj3',\n 'Factbox: Automakers close factories in China due to virus outbreak',\n 'Finnair cancels 276 flights as unions join solidarity strike #FinnairOyj #Stock #MarketScreener… https://t.co/j4PMDAs2cI',\n 'Ford Recalls More Than 500,000 Pickups for Fire Risk',\n \"Ford risks Mustang fan backlash with SUV: They'll need to pry gas-powered Mustangs from 'my cold dead fingers'\",\n 'Frozen Wells Fargo Bonuses Show a Peril for Bankers After Crisis',\n \"FTC's Look Into Facebook A 'Negative Development,' Says RBC's Mahaney\",\n \"Gilead's Second CAR-T Treatment Suffers the Same Issues as Yescarta\",\n 'Goldman partner embroiled in 1MDB scandal exits bank - FT\\xa0https://t.co/uiXjEqyc6e\\xa0https://t.co/cV1UlIMIB4',\n 'Gulfport Energy to stop share buybacks, cut jobs; Houston to leave board',\n 'Harvest Health becomes latest cannabis company to scale back a previously-agreed deal',\n \"Hexo's Q1 Report Could Be Ugly, Says Cantor Fitzgerald\",\n 'Honda may keep Wuhan plants closed longer due to outbreak: Nikkei',\n 'Hong Kong Airlines to cut 400 jobs, ask employees to take unpaid leave: SCMP',\n 'Hong Kong Airlines to cut 400 jobs, operations as coronavirus hits travel',\n 'Hyundai Motor, Kia Motors to keep Chinese factories idled until February 16',\n 'IFA Irish Farmers Association : HAND IN LETTERS OF PROTEST TO MEAT INDUSTRY IRELAND AND MI... #IFA #economy… https://t.co/xxnZ8OgPnG',\n 'In a sharply worded letter, HP told Xerox it has “significant concerns about both the near-term health and long-ter… https://t.co/xYlMfujdGg',\n \"ING's Fourth-Quarter Results Were Quite Challenging, Says CFO\",\n 'J.C. Penney Again at Risk Of NYSE Delisting',\n 'Jaguar Land Rover to cut output at two UK factories',\n 'La Jolla Pharma to reassess development of LJPC-401',\n 'La Jolla Pharmaceutical to reassess continued development of LJPC-401 based on clinical results',\n 'Lifeblood of Amazon merchants threatened as coronavirus infects Chinese workers',\n 'Loonie dips as Bank of Canada official points out uncertainties',\n \"L'Oreal CEO says coronavirus will hamper sales in coming weeks\",\n \"L'Oreal Expects Short-Term Virus Hit\",\n \"Lowe's says it will shut 34 stores in Canada\",\n 'Luckin Coffee and Yum China hit again by coronavirus anxiety',\n \"Macy's says weather, soft tourism and weak performance at lower-tier malls weighed in Q3\",\n 'Mitsubishi postpones SpaceJet delivery again, books $4.5 billion special loss',\n 'Nike to record $425 mln charge as it transitions brand business in Brazil, Argentina, Chile, Uruguay to distributor partnerships',\n 'Nike Warns Coronavirus Is Presuring Sales in China',\n 'Nintendo Switch Shipments to Japan Delayed by Coronavirus',\n 'Nissan orders deep spending cuts - Reuters',\n 'Novartis to cut 150 jobs at Shanghai site in shift to commercial development',\n 'NTSB calls on Boeing to redesign thousands of 737s',\n \"NTSB chair criticizes Uber's safety culture\",\n 'Nutrien : to Shut Down Rocanville Mine for Two Weeks Due to CN Strike #Nutrien #Stock #MarketScreener… https://t.co/7RnmPSYg3X',\n 'Oracle’s Need for ‘Steep’ Sales Climb Leaves Street Cautious',\n 'Participation Of BSNL, MTNL In Spectrum Auction May Lead To Conflict Of Interest: Department Of Telecom\\xa0',\n 'Peter Thiel Divides Facebook Internally Over Ad Policy (Radio)',\n 'PG&E loses half of financial backing for bankruptcy plan',\n \"Pizza Hut's Struggling Turnaround Weighs on Yum Brands Results\",\n 'Protest forces AngloGold to suspend Guinea gold production',\n 'Qualcomm Projects Uneven Sales Growth on Pickup of 5G',\n 'Royal Mail Labor Trouble Throws Long-Term Targets Into Jeopardy',\n 'Royal Mail Labor Troubles Put Long-Term Targets in Jeopardy',\n \"Russia's Rusal faces tough market for aluminium even after sanctions dropped\",\n 'S. African Airways Cuts Foreign, Local Routes to Stay Afloat',\n 'Saputo Canadian Closures Show How Much Dairy Industry Is Hurting',\n \"Sell-siders on board with Bristol-Myers Squibb's Q4 & outlook\",\n 'SoftBank Group Corp.’s massive investment in WeWork triggered a multi-billion-dollar writedown and a rare apology f… https://t.co/PogaY0B09Z',\n \"Some gamers accessing Google's new cloud gaming platform, Stadia, through a Chromecast Ultra dongle are reporting trouble… https://t.co/t6dCbMr0C3\",\n 'Sparkle fades as coronavirus risks wiping out luxury goods growth',\n 'Steel Giant to Axe 3,000 Jobs as Crisis Rips Through Europe',\n 'Swiss investor adviser Ethos calls for Credit Suisse overhaul after spying',\n 'Tesla Cybertruck debut drew ridicule online after the newly-unveiled model’s armored glass windows cracked like spi… https://t.co/3h9ch5lK1r',\n 'Tesla Zealotry on Street Fades as Focus Shifts to Fundamentals',\n \"Tesla's Return to Profitability Is Not Sustainable\",\n 'Teva, Bausch Could Be Next to File for Bankruptcy',\n \"The reveal of the Tesla $TSLA #Cybertruck didn't go quite as expected... but moving on, there's a few specs you sho… https://t.co/wYnwjcjnEF\",\n 'The Supreme Cannabis Company, Inc. Consensus Forecasts Have Become A Little Darker Since Its Latest Report',\n 'Thiel driving FB division on political ads - WSJ',\n 'Three of the largest US airlines -- Delta, American and United -- have canceled more flights between the United Sta... https://t.co/fY9qG2bXpa',\n 'TikTok looks to diversify users as U.S. pressure mounts',\n 'Toyota keeps China plant output stopped through February 16 as virus hits supply, logistics',\n 'Toyota, Honda Extend China Shutdowns as Virus Gathers Pace',\n 'Trading on the News Adds a Risk Premium to Shares of American Micro Devices',\n 'Trans Mountain expansion costs soars to C$12.6B - report',\n 'TSB to axe at least 15% of branches in cost-cutting drive https://t.co/8c17g12zuC',\n 'TSB to close 82 branches, slash costs in strategy overhaul https://t.co/RWpUxn2xTH https://t.co/TUTUh0Au4G',\n 'TSB to cut 82 branches in strategy overhaul, up to 400 may go https://t.co/D9inDP0OOQ https://t.co/w2h4hTfFyz',\n 'Tullow to Cut 40% Jobs in Kenya, Focus on Investment Decision',\n \"Uber co-founder Travis Kalanick has sold $1.5 billion worth of the company's stock this month https://t.co/ZLtMcVtFcp\",\n \"Uber co-founder Travis Kalanick sold $1.5 billion of the company's stock this month https://t.co/L5f47o9NnD\",\n 'Uber Loses License In London',\n 'Uber Loses London License (Again)\\xa0',\n 'Uber may be about to lose the right to operate in London https://t.co/PWXX1xSAOj',\n 'UBER SAYS AWARE OF APP NOT LOADING, WORKING TO RESOLVE IT',\n \"Uber's license will not be renewed in London over safety concerns https://t.co/n6NCmvWaGt\",\n \"'Unfit' Uber loses London license over safety failures\",\n \"'Unfit' Uber stripped of London license #economy #MarketScreener https://t.co/wgqFijpV50 https://t.co/clmEwq2cJK\",\n \"Unilever : Recalls Some Lipton and Knorr Products Due to a Poultry Supplier's Recall #Unilever #Stock… https://t.co/FxpwjQYkXh\",\n \"United joins Delta and American in suspending flights between the U.S. and mainland China. United's flight suspensi... https://t.co/5ZtcJXzf7O\",\n 'UPDATE 1-Bristol-Myers misses main goal of late-stage skin cancer trial',\n 'UPDATE 1-CymaBay Therapeutics scraps liver disease studies; shares plunge',\n \"UPDATE 1-FDA declines to approve Adamis Pharma's Zimhi to treat opioid overdose\",\n 'Vans parent VF Corp. says 60% of China owned and partnered stores are closed over coronavirus',\n 'Vans parent VF Corp. says 60% of China stores are closed over coronavirus, to update guidance in May',\n \"Vietnam's budget carrier VietJet Aviation JSC will suspend all of its fights to China February 1 amid the novel cor... https://t.co/nWRl3RDuIy\",\n 'Walmart Inc.’s Jet subsidiary is ending its fresh-food delivery business just a year after introducing the service… https://t.co/MpzLxKX3cW',\n \"Walmart Pulls The Plug On Jet.com 'City Grocery Experience'\",\n \"Walmart's Jet is ending its fresh-food delivery business just a year after introducing the service in New York City https://t.co/Y8sI7u2mmB\",\n 'WATCH: Cosmetics maker Coty, faced with weak sales, is expected to start an auction process in December and consume… https://t.co/qSW2swob2C',\n 'WeWork Layoffs: 10 Things to Know About the Upcoming Job Cuts',\n 'WeWork set to lay off thousands - NYT',\n 'WeWork’s\\xa0Path to Survival Is Narrow and Perilous',\n \"What went wrong at Tesla's Cybertruck launch? Hear from Bloomberg's Ed Ludlow @EdLudlow who was there https://t.co/chEHrylNx7\",\n \"Why I'm Cutting Netflix -- and Millions of Americans Might, Too\",\n 'Why London Has Banned Uber —\\xa0Again',\n '$PBR - Petrobras expects lower April production; makes oil discovery https://t.co/kSkaXkxlze',\n '$PSX $VLO $HFC - Phillips 66 cuts rates at Bayway refinery on weak cracks - S&P Global https://t.co/0sOtRQG5t5',\n \"$XOM - Exxon's Baton Rouge refinery cuts production on low demand - Reuters https://t.co/xRIss6avLy\",\n 'Exxon is cutting billions of dollars in investments to keep its financial footing during the worst oil-price rout i… https://t.co/QoPqAc2J9a',\n 'WATCH: Exxon Mobil, the largest U.S. oil producer, said it will cut planned capital spending by 30% this year https://t.co/pewe0L13xG',\n 'Freddie Mac Prices $731 Million Multifamily K-Deal, K-F74  https://t.co/c31iovNuJy',\n '$AAPL - Apple forecasts 100M+ 5G iPhone sales - report https://t.co/QhkaVvX1fZ',\n \"$AQST FDA approves Aquestive's ALS treatment https://t.co/7g7ZbrkoqF via @YahooFinance\",\n '$AQST: Aquestive Therapeutics confirms FDA approval for Exservan Oral Film for the treatment of amyotrophic lateral… https://t.co/PIRPvqJ4s7',\n '$BA $EADSY $EADSF - Boeing reports 30 orders of 737 MAX in November https://t.co/zOOFn8b2ip',\n '$CNK - Cinemark Holdings (CNK) Presents At Southwest Ideas Investor Conference - Slideshow. Get more updates here:… https://t.co/vdvcgOsGLP',\n \"$DIS $SNE - Disney now sees $110M-$120M for 'Frozen II' https://t.co/I59tzaKreO\",\n '$DMPI DelMar Pharmaceuticals Presents Positive Interim Data of VAL-083 Demonstrating Favorable Outcomes...… https://t.co/5WbTTriTwV',\n '$HEPA Hepion Pharmaceuticals Successfully Advances to Next Dosing Level in Ongoing Multiple Ascending Dose Clinical Study of CRV431',\n '$ICPT: Intercept Pharma announces FDA acceptance of New Drug Application for obeticholic acid seeking accelerated... https://t.co/31L0KiSWEZ',\n '$IONS $AKCA $AKCA - Akcea and Ionis launches late-stage AKCEA-TTR-LRx study https://t.co/ybDZfFkp0Z',\n '$LDOS - Leidos bull positive on DoE contract win https://t.co/KCDOa63aHE',\n '$LMT - Lockheed Martin missile test goes off well https://t.co/gYVX1UzVqD',\n '$OYST: Oyster Point Pharma reports positive top-line results from the Phase 1 ZEN Study https://t.co/3fKa6WS85r',\n '$PAC - Grupo Aeroportuario Del Pacifico reports 10.1% rise in November traffic https://t.co/1tr3oiR6mE',\n '$PTCT: PTC Therapeutics announces that the FDA has granted priority review for the New Drug Application for... https://t.co/EIQ7MgrPSc',\n '$RADA - RADA Electronic announces $12.5M in new orders https://t.co/wEocrRMgtL',\n '$SGEN - Seattle Genetics earns drug approval in Canada https://t.co/Q1co2SGurS',\n '$TAK: Takeda Pharma presents long-term data for ALUNBRIG; demonstrates continued Superiority after two years https://t.co/RPW80FKq6S',\n '$TEVA - Teva inks early-stage drug research deals https://t.co/yqK6GHv0Ff',\n '$X $MT $TMST - U.S. Steel hikes HRC prices by $30/ton; steel stocks rise https://t.co/6iAnyB79Sa',\n \"A Closer Look At Asbury Automotive Group, Inc.'s (NYSE:ABG) Impressive ROE\",\n 'Ad Group Publicis Keeps Guidance in Sign That Worst May Be Over',\n 'AECOM recognized with national award for U.S. Federal small business program excellence',\n 'Airbus racks up strong January orders',\n 'Amani Gold : Secures Gold Dealers License in Tanzania #AmaniGold #Stock #MarketScreener https://t.co/fUr63A7aVB https://t.co/AHlEWGRAtq',\n 'Amazon Employees Surge Toward 1 Million',\n 'Amazon says it signed up new Prime service members in record numbers in the U.K. this week when it aired Premier Le… https://t.co/SM7E7PiVsL',\n 'Amazon to create 500 new jobs in Florida as it plans to open a distribution center',\n 'Amazon to create 500 new jobs in Florida as part of plans to open a fulfillment center in Auburndale',\n 'Amazon to create 500 new jobs in Mississippi as it builds a 2nd fulfillment center in the state',\n 'Amazon to create 500 new jobs in Mississippi as it builds new fulfillment center',\n 'AMD launches 7nm GPU for workstations; shares +4%',\n 'Analysis: Popeyes Chicken Sandwich Translates To Surge In Foot Traffic',\n \"Analysts positive on New Relic's investor event\",\n 'Apple Averted An iPhone Fiasco With Phase 1 Trade Deal',\n 'Apple forecasts 100M+ 5G iPhone sales - report',\n 'Apple Just Put the Swiss Watch Industry to Shame',\n \"Apple's iPhone 11 'massive' success - Wedbush\",\n 'ArcelorMittal More Optimistic on Steel Demand Outlook for 2020',\n 'ArcelorMittal sees 2020 steel pick-up as debt hits low',\n 'As Uber, GrubHub and Postmates battle over cities, Doordash is thriving in the suburbs https://t.co/U7tr1vKX5x',\n \"Azul's January traffic rose 29.1%\",\n 'Baidu Leads the Way in Innovation with 5,712 Artificial Intelligence Patent Applications  https://t.co/btciiZtLpZ',\n \"Blue Apron to offer 'diabetes-friendly' menus this month, stock rallies\",\n 'Boeing reports 30 new orders for the troubled 737 Max in November https://t.co/jGejqEi0oT',\n 'Brazilian food delivery app iFood looks to grow in retail in 2020',\n 'Canopy Growth gets Health Canada license for beverage facility',\n \"Canopy Growth Obtains Health Canada License For Beverage Facility, Now Has All 'Cannabis 2.0' Licenses\",\n 'China EV startup Byton receives California distributor license',\n 'Cipla’s India Arm Will Show Market-Beating Growth In Next Few Quarters: CEO Vohra',\n 'Coca-Cola Breaks Down a Record Quarter',\n \"Conoco Plans Share Buyback That's Almost 50% of Market Value\",\n 'Consensus is Being Reached Over Eskom Debt, Cosatu Says',\n 'CuriosityStream, a streaming service devoted to science and nature has quietly amassed 10 million subscribers, more… https://t.co/bh8ZZdSJWv',\n 'Dana extends buyback program',\n 'Deutsche Bank Plans Bigger Bonuses at Asset Management Division',\n \"Dunkin' Brands Accelerates Growth Into 2020\",\n 'Electronic Arts Breaks Down a Record Quarter',\n 'Eli Lilly to create 100 jobs in Indianapolis as it adds capacity to its technology center campus',\n 'Elon Musk says Tesla has 187,000 orders for its Cybertruck, less than 48 hours after the broken-glass reveal https://t.co/AtBQDgnjUP',\n 'Elon Musk says Tesla has amassed more than 200,000 orders for its Cybertruck, even after two windows unexpectedly s… https://t.co/KL1H0oIjfL',\n 'engage:BDR : 25 New Clients, Revenue Growth & Balance Sheet Improvements #engage:BDR #Stock #MarketScreener… https://t.co/bwNFFeO1w4',\n 'Extreme Networks board authorizes $100 mln share buyback',\n \"FDA accepts Bioepis' application for Avastin biosimilar\",\n \"FDA Approves Sarepta's Second DMD Drug, Analyst Projects $500M-Plus Opportunity\",\n \"FDA OKs Zogenix's Fintepla for Dravet syndrome\",\n 'Ford invests more in Michigan',\n 'Ford to Add 3,000 Jobs at Michigan Plants, Invest $1.45 Billion',\n 'Ford to invest $1.45 billion in two Detroit plants, add 3,000 jobs',\n 'Former ConsenSys executive says Bitcoin will increase ‘stability for the average person’',\n 'Grupo Aeroportuario reports 10% increase in January traffic',\n \"Hecla's Fourth Quarter Continues a Strong Second Half Of 2019\",\n 'Highlight 2019: Ruyi Group Records Strong Development and Growth',\n 'HollyFrontier OKs new $1 billion stock buyback program, to build new renewable diesel unit',\n 'HollyFrontier outlines new biodiesel project, to buy back $1B in shares',\n 'HollyFrontier sets new $1 bln share repurchase program',\n 'How Nordstrom Is Benefiting From Direct-to-Consumer Brands',\n 'How Target has morphed into a model for avoiding the retail apocalypse https://t.co/pfetquaLsu',\n 'How the Rise of At-Home Fitness Services Could Benefit Lululemon Athletica',\n 'Imagination TV announces new share buyback program',\n 'India Business Will Show Market-Beating Growth In Next Few Quarters, Says Cipla CEO',\n 'Invora™ Herbicide Receives EPA Federal Approval',\n 'JUST IN: For the first time since January, orders for Boeing’s troubled 737 Max airplanes incrementally rose last m… https://t.co/myUZbIwzbP',\n \"Karuna Therapeutics 'cautiously optimistic' about schizophrenia drug results in next trial: CEO https://t.co/p4vsl99X3m\",\n 'Kazia Announces Positive Interim Efficacy Data From GDC-0084 Phase Ii Studyin Glioblastoma Released at SNO Conferen… https://t.co/Bn7gXP51EJ',\n 'Kodak Is Having a Comeback Moment as Oscars Embrace Film',\n 'L’Oreal Expects to Outperform Rivals Amid Temporary Virus Hit',\n 'Lincoln Eyes Growing Market for $50,000 Rides: Auto Show Update',\n ...]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-15T15:33:57.528515300Z",
     "start_time": "2024-03-15T15:33:57.523499900Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"roberta-large\"\n",
    "\n",
    "num_labels = 3\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "tokenizer_name = model_name\n",
    "\n",
    "max_seq_length = 128\n",
    "train_batch_size = 8\n",
    "test_batch_size = 8\n",
    "warmup_ratio = 0.06\n",
    "weight_decay=0.0\n",
    "gradient_accumulation_steps = 1\n",
    "num_train_epochs = 15\n",
    "learning_rate = 1e-05\n",
    "adam_epsilon = 1e-08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-15T15:34:26.316826900Z",
     "start_time": "2024-03-15T15:33:57.533501800Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./roberta_pretrained_fin_0.5_e1 and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "class SMARTRobertaClassificationModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, model, weight = 0.02):\n",
    "        super().__init__()\n",
    "        self.model = model \n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "\n",
    "        # Get initial embeddings \n",
    "        embed = self.model.roberta.embeddings(input_ids) \n",
    "\n",
    "        # Define eval function \n",
    "        def eval(embed):\n",
    "            outputs = self.model.roberta(inputs_embeds=embed, attention_mask=attention_mask)\n",
    "            pooled = outputs[0] \n",
    "            logits = self.model.classifier(pooled) \n",
    "            return logits \n",
    "        \n",
    "        # Define SMART loss\n",
    "        smart_loss_fn = SMARTLoss(eval_fn = eval, loss_fn = kl_loss, loss_last_fn = sym_kl_loss)\n",
    "        # Compute initial (unperturbed) state \n",
    "        state = eval(embed)\n",
    "        # Apply classification loss \n",
    "        loss = F.cross_entropy(state.view(-1, 3), labels.view(-1))\n",
    "        # Apply smart loss \n",
    "        loss += self.weight * smart_loss_fn(embed, state)\n",
    "        \n",
    "        return state, loss\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# tokenizer = AutoTokenizer.from_pretrained('./roberta_pretrained_fin')\n",
    "\n",
    "config = RobertaConfig.from_pretrained(model_name, num_labels=num_labels)\n",
    "model = AutoModelForSequenceClassification.from_pretrained('./roberta_pretrained_fin_0.5_e1', config = config)\n",
    "\n",
    "model_smart = SMARTRobertaClassificationModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-15T15:34:26.330299800Z",
     "start_time": "2024-03-15T15:34:26.312270200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model=\n",
      " SMARTRobertaClassificationModel(\n",
      "  (model): RobertaForSequenceClassification(\n",
      "    (roberta): RobertaModel(\n",
      "      (embeddings): RobertaEmbeddings(\n",
      "        (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
      "        (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
      "        (token_type_embeddings): Embedding(1, 1024)\n",
      "        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): RobertaEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0-23): 24 x RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (classifier): RobertaClassificationHead(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (out_proj): Linear(in_features=1024, out_features=3, bias=True)\n",
      "    )\n",
      "  )\n",
      ") \n"
     ]
    }
   ],
   "source": [
    "print('Model=\\n',model_smart,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-15T15:34:27.160178200Z",
     "start_time": "2024-03-15T15:34:26.323318700Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyClassificationDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data, tokenizer):\n",
    "        text, labels = data\n",
    "        self.examples = tokenizer(text=text,text_pair=None,truncation=True,padding=\"max_length\",\n",
    "                                  max_length=max_seq_length,return_tensors=\"pt\")\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {key: self.examples[key][index] for key in self.examples}, self.labels[index]\n",
    "\n",
    "train_examples = (train_data['text'], train_data['label'])\n",
    "train_dataset = MyClassificationDataset(train_examples,tokenizer)\n",
    "\n",
    "test_examples = (test_data['text'], test_data['label'])\n",
    "test_dataset = MyClassificationDataset(test_examples,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-15T15:34:27.168709900Z",
     "start_time": "2024-03-15T15:34:27.162171900Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_inputs_dict(batch):\n",
    "    inputs = {key: value.squeeze(1).to(device) for key, value in batch[0].items()}\n",
    "    inputs[\"labels\"] = batch[1].to(device)\n",
    "    return inputs\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset,shuffle=True,batch_size=train_batch_size)\n",
    "\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset,sampler=test_sampler, batch_size=test_batch_size)\n",
    "\n",
    "#Extract a batch as sanity-check\n",
    "# batch = get_inputs_dict(next(iter(train_dataloader)))\n",
    "# input_ids = batch['input_ids'].to(device)\n",
    "# attention_mask = batch['attention_mask'].to(device)\n",
    "# labels = batch['labels'].to(device)\n",
    "\n",
    "# print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-15T15:34:28.736842800Z",
     "start_time": "2024-03-15T15:34:27.169690500Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "t_total = len(train_dataloader) // gradient_accumulation_steps * num_train_epochs\n",
    "optimizer_grouped_parameters = []\n",
    "custom_parameter_names = set()\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters.extend(\n",
    "    [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model_smart.named_parameters()\n",
    "                if n not in custom_parameter_names and not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model_smart.named_parameters()\n",
    "                if n not in custom_parameter_names and any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "warmup_steps = math.ceil(t_total * warmup_ratio)\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-15T15:34:28.747327500Z",
     "start_time": "2024-03-15T15:34:28.740834900Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics(preds, model_outputs, labels, eval_examples=None, multi_label=True):\n",
    "    assert len(preds) == len(labels)\n",
    "    mismatched = labels != preds\n",
    "    #wrong = [i for (i, v) in zip(eval_examples, mismatched) if v.any()]\n",
    "    mcc = matthews_corrcoef(labels, preds)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='macro')\n",
    "    con_m = confusion_matrix(labels, preds, labels=[0, 1, 2])\n",
    "#     scores = np.array([softmax(element)[1] for element in model_outputs])\n",
    "#     fpr, tpr, thresholds = roc_curve(labels, scores)\n",
    "#     auroc = auc(fpr, tpr)\n",
    "#     auprc = average_precision_score(labels, scores)\n",
    "    return (\n",
    "        {\n",
    "            **{\"mcc\": mcc, \"acc\":acc, \"f1\": f1},\n",
    "        },\n",
    "        con_m\n",
    "    )\n",
    "\n",
    "def print_confusion_matrix(result):\n",
    "    print('confusion matrix:')\n",
    "    print('            predicted    ')\n",
    "    print('          0     |     1')\n",
    "    print('    ----------------------')\n",
    "    print('   0 | ',format(result['tn'],'5d'),' | ',format(result['fp'],'5d'))\n",
    "    print('gt -----------------------')\n",
    "    print('   1 | ',format(result['fn'],'5d'),' | ',format(result['tp'],'5d'))\n",
    "    print('---------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "OrderedDict([('model.roberta.embeddings.position_ids',\n              tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n                        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n                        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n                        42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n                        56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n                        70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n                        84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n                        98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n                       112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n                       126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n                       140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n                       154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n                       168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n                       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n                       196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n                       210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n                       224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n                       238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n                       252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n                       266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n                       280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n                       294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n                       308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n                       322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n                       336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n                       350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n                       364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n                       378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n                       392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n                       406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n                       420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n                       434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n                       448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461,\n                       462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n                       476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n                       490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503,\n                       504, 505, 506, 507, 508, 509, 510, 511, 512, 513]], device='cuda:0')),\n             ('model.roberta.embeddings.word_embeddings.weight',\n              tensor([[-0.1410, -0.0103,  0.0400,  ...,  0.0494, -0.0039, -0.0369],\n                      [ 0.0078, -0.0156,  0.0156,  ..., -0.0156,  0.0231,  0.0156],\n                      [-0.0805,  0.0009, -0.1165,  ...,  0.1060,  0.0697, -0.0391],\n                      ...,\n                      [ 0.0381,  0.0027,  0.0473,  ..., -0.0238, -0.0499,  0.0337],\n                      [ 0.0503,  0.0272,  0.0421,  ..., -0.0370, -0.0102,  0.0072],\n                      [-0.0158, -0.0116, -0.0225,  ...,  0.0442,  0.0115, -0.0331]],\n                     device='cuda:0')),\n             ('model.roberta.embeddings.position_embeddings.weight',\n              tensor([[-0.0038,  0.0253, -0.0092,  ...,  0.0177,  0.0062, -0.0162],\n                      [ 0.0117, -0.0019, -0.0267,  ...,  0.0062, -0.0193,  0.0264],\n                      [ 0.0306,  0.0138, -0.0539,  ..., -0.0727, -0.0439,  0.0460],\n                      ...,\n                      [-0.0209, -0.0052,  0.0484,  ..., -0.0394,  0.0463,  0.0537],\n                      [-0.0274,  0.1172,  0.0470,  ...,  0.0170, -0.1204,  0.0525],\n                      [ 0.0969, -0.0729,  0.0558,  ..., -0.1204, -0.1075,  0.0489]],\n                     device='cuda:0')),\n             ('model.roberta.embeddings.token_type_embeddings.weight',\n              tensor([[-4.9027e-04, -9.3924e-04,  7.1708e-04,  ..., -1.3981e-04,\n                        3.4021e-04,  6.7425e-05]], device='cuda:0')),\n             ('model.roberta.embeddings.LayerNorm.weight',\n              tensor([0.9337, 0.9196, 0.9120,  ..., 0.9407, 0.9152, 0.9008], device='cuda:0')),\n             ('model.roberta.embeddings.LayerNorm.bias',\n              tensor([ 0.0312,  0.0413,  0.1933,  ..., -0.2255, -0.0890,  0.1254],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.0.attention.self.query.weight',\n              tensor([[-0.0078,  0.0344, -0.0003,  ...,  0.0027,  0.0638, -0.0416],\n                      [-0.0255,  0.0519, -0.0168,  ..., -0.0282, -0.0128,  0.0083],\n                      [ 0.0035,  0.0731, -0.0302,  ...,  0.0820,  0.0115, -0.0104],\n                      ...,\n                      [-0.0583,  0.0199, -0.0422,  ..., -0.0319,  0.0050,  0.0683],\n                      [ 0.0428,  0.0217, -0.0633,  ..., -0.0523, -0.0179,  0.0184],\n                      [-0.0201, -0.0419, -0.0092,  ...,  0.0452,  0.0259, -0.0192]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.0.attention.self.query.bias',\n              tensor([ 0.3110,  0.0530, -0.0734,  ..., -0.0689, -0.0523, -0.0670],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.0.attention.self.key.weight',\n              tensor([[-0.0068, -0.0172, -0.0132,  ..., -0.0043,  0.0096, -0.0152],\n                      [-0.0234, -0.0014,  0.0281,  ...,  0.0392,  0.0421, -0.0231],\n                      [-0.0279, -0.0523, -0.0143,  ..., -0.0343,  0.0054,  0.0184],\n                      ...,\n                      [-0.0703, -0.0221, -0.0179,  ..., -0.0187,  0.0154,  0.1035],\n                      [ 0.0152,  0.0052, -0.0205,  ..., -0.0007, -0.0105,  0.0421],\n                      [-0.0075, -0.0627,  0.0458,  ...,  0.0457,  0.0194, -0.0483]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.0.attention.self.key.bias',\n              tensor([-0.0041, -0.0033, -0.0012,  ...,  0.0013,  0.0017,  0.0018],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.0.attention.self.value.weight',\n              tensor([[ 0.0312,  0.0011, -0.0229,  ..., -0.0241,  0.0015,  0.0224],\n                      [ 0.0583,  0.0474,  0.0028,  ..., -0.0183,  0.0912, -0.0143],\n                      [-0.0173, -0.0434,  0.0127,  ..., -0.0509, -0.0027,  0.0640],\n                      ...,\n                      [-0.0083,  0.0084, -0.0129,  ...,  0.0364,  0.0264,  0.0147],\n                      [-0.0048, -0.0114, -0.0564,  ...,  0.0376, -0.0317,  0.0317],\n                      [ 0.0022, -0.0076, -0.0104,  ..., -0.0252,  0.0905, -0.0175]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.0.attention.self.value.bias',\n              tensor([-0.0005,  0.0023, -0.0078,  ..., -0.0239, -0.0208, -0.0347],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.0.attention.output.dense.weight',\n              tensor([[ 0.0033,  0.0423, -0.0188,  ..., -0.0160, -0.0335, -0.0131],\n                      [-0.0362,  0.0113, -0.0140,  ...,  0.0348, -0.0068,  0.0279],\n                      [ 0.0272, -0.0742,  0.0172,  ..., -0.0316, -0.0062,  0.1002],\n                      ...,\n                      [ 0.0261,  0.0034,  0.0227,  ...,  0.0131, -0.0115, -0.0385],\n                      [-0.0057,  0.0547, -0.0470,  ...,  0.0412,  0.0248, -0.0185],\n                      [ 0.0457,  0.0263,  0.0906,  ...,  0.0275,  0.0092,  0.0100]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.0.attention.output.dense.bias',\n              tensor([-0.0134,  0.0288,  0.0848,  ...,  0.0735, -0.0084,  0.0116],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.0.attention.output.LayerNorm.weight',\n              tensor([0.9792, 0.9865, 0.9747,  ..., 0.9832, 0.9915, 0.9960], device='cuda:0')),\n             ('model.roberta.encoder.layer.0.attention.output.LayerNorm.bias',\n              tensor([-0.4314,  0.2757, -0.0081,  ...,  0.0116,  0.3291, -0.2966],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.0.intermediate.dense.weight',\n              tensor([[ 0.0577, -0.0669, -0.0934,  ...,  0.0056,  0.0165, -0.0154],\n                      [ 0.0178, -0.0283,  0.0224,  ...,  0.0173, -0.0421,  0.1212],\n                      [ 0.0377, -0.0670, -0.0021,  ...,  0.0302, -0.0237, -0.0207],\n                      ...,\n                      [ 0.0167, -0.0915,  0.0031,  ...,  0.0332, -0.0485,  0.0017],\n                      [ 0.1343,  0.0516, -0.1341,  ..., -0.1476, -0.0335,  0.0248],\n                      [ 0.0724, -0.0306, -0.0594,  ...,  0.0090, -0.0556, -0.0313]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.0.intermediate.dense.bias',\n              tensor([-0.0954, -0.0776, -0.0809,  ..., -0.1075, -0.0713, -0.0931],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.0.output.dense.weight',\n              tensor([[ 0.0427,  0.1013,  0.0279,  ..., -0.0557,  0.0575,  0.0663],\n                      [ 0.0153,  0.0094, -0.0096,  ..., -0.0045,  0.0204, -0.0114],\n                      [ 0.0271, -0.0682,  0.0500,  ..., -0.0419, -0.0107,  0.0015],\n                      ...,\n                      [-0.0148, -0.0052,  0.0546,  ...,  0.0109, -0.0613,  0.0121],\n                      [-0.0489,  0.0341, -0.0729,  ...,  0.0562, -0.0357, -0.0020],\n                      [-0.0959, -0.0453, -0.0966,  ..., -0.0557,  0.0546, -0.0317]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.0.output.dense.bias',\n              tensor([ 0.0639, -0.0394,  0.0427,  ...,  0.0074, -0.0963,  0.0567],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.0.output.LayerNorm.weight',\n              tensor([0.9689, 0.9582, 0.9674,  ..., 0.9714, 0.9684, 0.9488], device='cuda:0')),\n             ('model.roberta.encoder.layer.0.output.LayerNorm.bias',\n              tensor([ 0.3965, -0.1880,  0.0424,  ..., -0.0486, -0.2754,  0.2011],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.1.attention.self.query.weight',\n              tensor([[ 2.0586e-02, -1.1372e-01,  2.0037e-02,  ..., -6.3993e-02,\n                       -2.8836e-02, -3.5014e-02],\n                      [ 3.3292e-02, -1.0130e-02,  9.9167e-02,  ..., -2.4900e-02,\n                        2.4497e-02,  1.6927e-01],\n                      [ 9.8554e-03, -4.3028e-02,  3.2516e-02,  ...,  9.3935e-03,\n                       -4.4441e-05, -3.2700e-03],\n                      ...,\n                      [ 1.5116e-02,  3.3033e-02,  6.4967e-02,  ...,  4.5620e-02,\n                       -4.7860e-02,  1.0199e-01],\n                      [-1.1664e-01,  6.9632e-02,  6.8583e-02,  ..., -1.3568e-02,\n                       -7.5707e-02,  6.1204e-02],\n                      [ 2.7984e-02, -6.9453e-02,  2.3233e-02,  ...,  6.3222e-02,\n                        5.6454e-04,  5.5750e-03]], device='cuda:0')),\n             ('model.roberta.encoder.layer.1.attention.self.query.bias',\n              tensor([ 0.0726,  0.0485, -0.0698,  ...,  0.0828,  0.0432, -0.0765],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.1.attention.self.key.weight',\n              tensor([[-0.0114,  0.0588, -0.0073,  ..., -0.0316,  0.0136, -0.0164],\n                      [-0.0200, -0.0895, -0.0561,  ...,  0.0529, -0.0397,  0.0147],\n                      [-0.0127,  0.0378,  0.0355,  ...,  0.0081, -0.0012, -0.0112],\n                      ...,\n                      [-0.0631, -0.0679,  0.0809,  ...,  0.0230,  0.0230,  0.0018],\n                      [ 0.0472, -0.0501,  0.0360,  ...,  0.0481,  0.0417, -0.0341],\n                      [ 0.0600,  0.0380,  0.0576,  ...,  0.0910,  0.0260,  0.0032]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.1.attention.self.key.bias',\n              tensor([ 1.3997e-03, -8.0402e-04, -8.0995e-04,  ...,  2.1106e-04,\n                      -8.1406e-05,  1.0122e-03], device='cuda:0')),\n             ('model.roberta.encoder.layer.1.attention.self.value.weight',\n              tensor([[-0.0643,  0.0029, -0.0251,  ..., -0.0097,  0.0536,  0.0281],\n                      [-0.0194,  0.0374, -0.0339,  ..., -0.0097,  0.0323, -0.0201],\n                      [-0.0010,  0.0369,  0.0043,  ...,  0.0221,  0.0849, -0.0185],\n                      ...,\n                      [-0.0043, -0.0792, -0.0336,  ..., -0.0239,  0.0359,  0.0236],\n                      [-0.0613,  0.0678, -0.0151,  ..., -0.0061,  0.0224, -0.0141],\n                      [ 0.0371,  0.0336,  0.0161,  ...,  0.0122,  0.0317, -0.0169]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.1.attention.self.value.bias',\n              tensor([ 0.0075,  0.0035,  0.0067,  ...,  0.0019, -0.0548, -0.0015],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.1.attention.output.dense.weight',\n              tensor([[-0.0395, -0.0106,  0.0040,  ..., -0.0737, -0.0299,  0.0178],\n                      [ 0.0034, -0.0533, -0.0031,  ..., -0.0741, -0.0403,  0.0062],\n                      [ 0.0452,  0.0111, -0.0231,  ..., -0.0299,  0.0179,  0.0343],\n                      ...,\n                      [-0.0062, -0.0176,  0.0374,  ...,  0.0214, -0.0252,  0.0478],\n                      [-0.0197,  0.0127,  0.0081,  ..., -0.0505, -0.0406, -0.0509],\n                      [ 0.0049, -0.0239,  0.0419,  ...,  0.0092, -0.0444,  0.0259]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.1.attention.output.dense.bias',\n              tensor([-0.2160,  0.0394, -0.0821,  ..., -0.0751,  0.2320,  0.0739],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.1.attention.output.LayerNorm.weight',\n              tensor([0.9833, 0.9975, 0.9587,  ..., 0.9800, 0.9870, 0.9720], device='cuda:0')),\n             ('model.roberta.encoder.layer.1.attention.output.LayerNorm.bias',\n              tensor([-0.3289,  0.1813, -0.0331,  ..., -0.0728,  0.2486, -0.2290],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.1.intermediate.dense.weight',\n              tensor([[ 0.0284, -0.0327, -0.0037,  ..., -0.0212,  0.1021,  0.0536],\n                      [-0.0009, -0.0684, -0.0286,  ...,  0.0343, -0.0024, -0.0541],\n                      [-0.0036,  0.0844,  0.0756,  ...,  0.0145,  0.0212, -0.0062],\n                      ...,\n                      [-0.0388, -0.0439,  0.1256,  ...,  0.0004,  0.0132, -0.0311],\n                      [ 0.0746, -0.0582, -0.1045,  ..., -0.0316,  0.0240, -0.0831],\n                      [ 0.0013,  0.0750,  0.0950,  ..., -0.0182, -0.0266,  0.0310]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.1.intermediate.dense.bias',\n              tensor([ 0.0971, -0.0546, -0.0540,  ..., -0.0872, -0.0858, -0.0858],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.1.output.dense.weight',\n              tensor([[-0.0225, -0.0184,  0.0632,  ..., -0.0237, -0.0905,  0.0512],\n                      [ 0.0473, -0.0197, -0.0143,  ..., -0.0324, -0.0058, -0.0453],\n                      [ 0.0419,  0.0376,  0.0642,  ..., -0.0279, -0.0519,  0.0960],\n                      ...,\n                      [ 0.0416, -0.0604, -0.0771,  ..., -0.0411,  0.0237,  0.0353],\n                      [ 0.0104,  0.0850,  0.1117,  ...,  0.0646, -0.0423, -0.0222],\n                      [-0.0656,  0.0735,  0.0062,  ..., -0.0340,  0.0691,  0.0395]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.1.output.dense.bias',\n              tensor([ 0.0101,  0.0066,  0.0671,  ...,  0.0125, -0.0487,  0.0137],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.1.output.LayerNorm.weight',\n              tensor([0.9583, 0.9939, 0.9404,  ..., 0.9693, 0.9652, 0.9378], device='cuda:0')),\n             ('model.roberta.encoder.layer.1.output.LayerNorm.bias',\n              tensor([ 0.1896, -0.1928, -0.0502,  ..., -0.0129, -0.2738,  0.1580],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.2.attention.self.query.weight',\n              tensor([[ 0.0033, -0.0498,  0.0446,  ..., -0.0291, -0.0689, -0.0119],\n                      [-0.0818, -0.0899,  0.0372,  ..., -0.0244,  0.0021,  0.0134],\n                      [-0.0223, -0.0073, -0.0397,  ..., -0.0316, -0.0124, -0.0200],\n                      ...,\n                      [ 0.0217, -0.0206, -0.0108,  ..., -0.0346,  0.0830, -0.0703],\n                      [ 0.0311, -0.0946, -0.0506,  ..., -0.0460, -0.0007,  0.0098],\n                      [ 0.0197, -0.0212, -0.0695,  ...,  0.0714, -0.0084,  0.0463]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.2.attention.self.query.bias',\n              tensor([-0.0624,  0.1254,  0.1125,  ..., -0.1509,  0.0647, -0.1614],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.2.attention.self.key.weight',\n              tensor([[ 0.0159,  0.1352,  0.0381,  ...,  0.0354,  0.0182, -0.0580],\n                      [-0.0013,  0.0155, -0.0148,  ..., -0.0324, -0.0385,  0.0434],\n                      [ 0.0644, -0.0107,  0.0025,  ..., -0.0543,  0.0276, -0.0473],\n                      ...,\n                      [-0.0025, -0.0343, -0.0348,  ..., -0.0029, -0.0544, -0.0663],\n                      [-0.0538, -0.0325,  0.0360,  ...,  0.0042, -0.0494,  0.0169],\n                      [ 0.0323,  0.0168, -0.0255,  ...,  0.0668,  0.0120,  0.0538]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.2.attention.self.key.bias',\n              tensor([-9.2359e-04, -2.7109e-03, -1.1251e-03,  ..., -7.1467e-05,\n                      -6.1540e-04, -9.4857e-04], device='cuda:0')),\n             ('model.roberta.encoder.layer.2.attention.self.value.weight',\n              tensor([[-0.0056,  0.0513,  0.0215,  ...,  0.0299,  0.0292, -0.0002],\n                      [-0.0006,  0.0900, -0.0212,  ..., -0.0037, -0.0303, -0.0044],\n                      [ 0.0189,  0.0459,  0.0074,  ...,  0.0357,  0.0268, -0.0205],\n                      ...,\n                      [-0.0127,  0.0280, -0.0116,  ..., -0.0397, -0.0265,  0.0396],\n                      [ 0.0614,  0.0390, -0.0394,  ..., -0.0221,  0.0650,  0.0319],\n                      [-0.0242, -0.0104,  0.0015,  ..., -0.0118,  0.0134,  0.0547]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.2.attention.self.value.bias',\n              tensor([-0.0294,  0.0157,  0.0105,  ..., -0.0036,  0.0099, -0.0049],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.2.attention.output.dense.weight',\n              tensor([[-0.0125, -0.0158,  0.0220,  ..., -0.0053, -0.0121,  0.0248],\n                      [ 0.0043,  0.0156, -0.0154,  ..., -0.0451, -0.0126, -0.0007],\n                      [ 0.0056,  0.0395, -0.0708,  ..., -0.0113, -0.0127, -0.0161],\n                      ...,\n                      [-0.0295, -0.0078,  0.0488,  ..., -0.0039,  0.0100, -0.0123],\n                      [-0.0808,  0.0010,  0.0033,  ...,  0.0433,  0.0174, -0.0086],\n                      [ 0.0550,  0.0501,  0.0009,  ...,  0.0061,  0.0550,  0.0261]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.2.attention.output.dense.bias',\n              tensor([ 0.0615, -0.0592,  0.0323,  ...,  0.0804, -0.0396,  0.0734],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.2.attention.output.LayerNorm.weight',\n              tensor([0.9812, 0.9845, 0.9876,  ..., 0.9641, 0.9553, 0.9702], device='cuda:0')),\n             ('model.roberta.encoder.layer.2.attention.output.LayerNorm.bias',\n              tensor([-0.0939, -0.0031, -0.3419,  ..., -0.0277, -0.0722,  0.2145],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.2.intermediate.dense.weight',\n              tensor([[ 0.0237,  0.0841,  0.0665,  ..., -0.0492, -0.0168, -0.0438],\n                      [-0.0258, -0.0678,  0.0925,  ..., -0.0771,  0.0557, -0.0028],\n                      [-0.0719,  0.0042,  0.0928,  ...,  0.0141,  0.0230,  0.0038],\n                      ...,\n                      [-0.0369, -0.0105,  0.0048,  ...,  0.0063, -0.0009,  0.0205],\n                      [-0.0677,  0.0101, -0.0123,  ...,  0.0003,  0.0355, -0.0960],\n                      [-0.0009,  0.0217,  0.0174,  ..., -0.0168, -0.0520, -0.0997]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.2.intermediate.dense.bias',\n              tensor([-0.0221, -0.0798, -0.0690,  ...,  0.0587, -0.0879, -0.0896],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.2.output.dense.weight',\n              tensor([[-0.0232,  0.0828,  0.0146,  ..., -0.0406, -0.1097, -0.0009],\n                      [-0.0068,  0.0177, -0.0631,  ...,  0.0473, -0.0272, -0.0131],\n                      [ 0.0552, -0.0579,  0.0593,  ...,  0.0144, -0.0481, -0.0760],\n                      ...,\n                      [ 0.0050, -0.0440,  0.0684,  ...,  0.0136,  0.1035,  0.0013],\n                      [ 0.0493, -0.0086, -0.0316,  ...,  0.0107, -0.0945, -0.0087],\n                      [ 0.0295, -0.0499, -0.0236,  ..., -0.0426,  0.0166,  0.0033]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.2.output.dense.bias',\n              tensor([-0.0080, -0.0416,  0.0110,  ...,  0.0300,  0.0130,  0.0442],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.2.output.LayerNorm.weight',\n              tensor([0.9651, 0.9665, 0.9546,  ..., 0.9731, 0.9750, 0.9582], device='cuda:0')),\n             ('model.roberta.encoder.layer.2.output.LayerNorm.bias',\n              tensor([-0.0316, -0.1086,  0.2077,  ..., -0.0505, -0.0276, -0.2035],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.3.attention.self.query.weight',\n              tensor([[-0.0695,  0.0098,  0.0394,  ...,  0.0303,  0.0536, -0.0111],\n                      [-0.0202, -0.0765, -0.0424,  ...,  0.0349, -0.0171,  0.0307],\n                      [-0.0214, -0.0175,  0.0796,  ...,  0.0345, -0.0188,  0.0082],\n                      ...,\n                      [-0.0574, -0.0115, -0.0094,  ...,  0.0386,  0.0861, -0.0186],\n                      [ 0.0670, -0.0347,  0.0395,  ..., -0.0923,  0.0266, -0.0365],\n                      [ 0.0307,  0.0651, -0.0191,  ..., -0.0411, -0.0331,  0.0218]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.3.attention.self.query.bias',\n              tensor([-0.0263, -0.0513,  0.0301,  ...,  0.0142,  0.1170, -0.0821],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.3.attention.self.key.weight',\n              tensor([[-1.3267e-01, -4.0127e-02,  9.9844e-05,  ...,  2.4439e-02,\n                        4.8180e-02, -2.6878e-02],\n                      [ 2.2805e-02,  1.0131e-01,  9.2351e-02,  ..., -2.4665e-02,\n                        8.3128e-02, -4.5175e-02],\n                      [ 3.6429e-03, -3.9370e-02,  9.1092e-02,  ..., -1.4995e-02,\n                        1.5807e-02, -2.1354e-02],\n                      ...,\n                      [-2.7004e-02, -6.0542e-03, -3.6297e-03,  ...,  7.3570e-02,\n                       -6.3620e-02, -3.9564e-02],\n                      [ 1.6819e-03, -2.2934e-02, -7.2189e-03,  ...,  4.2433e-02,\n                       -3.2147e-02,  8.9460e-03],\n                      [ 3.1031e-02,  1.2554e-02,  2.1790e-02,  ...,  1.7362e-02,\n                       -3.0916e-02, -1.9499e-02]], device='cuda:0')),\n             ('model.roberta.encoder.layer.3.attention.self.key.bias',\n              tensor([-3.6352e-05,  1.0985e-03,  1.6986e-04,  ...,  1.3665e-03,\n                      -7.4795e-04,  4.2276e-04], device='cuda:0')),\n             ('model.roberta.encoder.layer.3.attention.self.value.weight',\n              tensor([[ 0.0118, -0.0313, -0.0749,  ...,  0.0304,  0.0965,  0.0215],\n                      [-0.0189,  0.0391,  0.0470,  ..., -0.0291,  0.0086, -0.0058],\n                      [ 0.0381,  0.0382, -0.0706,  ..., -0.0502,  0.0266,  0.0232],\n                      ...,\n                      [ 0.0711, -0.0012,  0.0109,  ...,  0.0201,  0.0298,  0.0450],\n                      [-0.0218,  0.0500, -0.0340,  ..., -0.0045, -0.1062, -0.0106],\n                      [ 0.0280, -0.0088, -0.0431,  ..., -0.0294, -0.0025, -0.0474]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.3.attention.self.value.bias',\n              tensor([-0.0176, -0.0090, -0.0074,  ...,  0.0072,  0.0064, -0.0013],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.3.attention.output.dense.weight',\n              tensor([[ 0.0003,  0.0024,  0.0331,  ...,  0.0003, -0.0252, -0.0300],\n                      [ 0.0100, -0.0102,  0.0398,  ...,  0.0146, -0.0197,  0.0137],\n                      [-0.0794,  0.0136, -0.0075,  ..., -0.0147,  0.0562, -0.0057],\n                      ...,\n                      [ 0.0218, -0.0014, -0.0235,  ...,  0.0219, -0.0335,  0.0139],\n                      [ 0.0123,  0.0414,  0.0029,  ..., -0.0081, -0.0115, -0.0088],\n                      [-0.0273, -0.0211,  0.0177,  ..., -0.0072,  0.0164,  0.0367]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.3.attention.output.dense.bias',\n              tensor([ 0.0424, -0.0194, -0.0270,  ..., -0.0226,  0.0344, -0.0193],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.3.attention.output.LayerNorm.weight',\n              tensor([0.9857, 0.9801, 0.9776,  ..., 0.9861, 0.9771, 0.9668], device='cuda:0')),\n             ('model.roberta.encoder.layer.3.attention.output.LayerNorm.bias',\n              tensor([-0.1808, -0.1194, -0.3020,  ..., -0.0452,  0.1130,  0.0360],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.3.intermediate.dense.weight',\n              tensor([[ 1.1901e-02,  3.9463e-02, -3.4831e-02,  ..., -5.3935e-02,\n                       -2.3536e-02, -3.6059e-02],\n                      [ 3.8951e-02,  3.1691e-02,  1.8003e-02,  ...,  2.5598e-02,\n                       -4.4381e-02, -1.4447e-02],\n                      [ 3.5369e-02, -6.0469e-02, -1.6539e-02,  ...,  6.4640e-02,\n                       -1.8150e-01,  8.3191e-03],\n                      ...,\n                      [-2.9486e-02, -3.3733e-02, -3.6420e-02,  ...,  3.2281e-02,\n                       -2.3603e-02, -5.2585e-02],\n                      [ 3.0303e-02, -5.9428e-02, -3.8965e-02,  ..., -1.0067e-03,\n                        2.0596e-02, -3.0025e-02],\n                      [ 1.1767e-04,  3.2166e-03, -4.8350e-02,  ...,  1.2121e-02,\n                       -6.5457e-02, -4.5164e-02]], device='cuda:0')),\n             ('model.roberta.encoder.layer.3.intermediate.dense.bias',\n              tensor([-0.0900, -0.1120, -0.0585,  ..., -0.1005, -0.0197, -0.0865],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.3.output.dense.weight',\n              tensor([[-0.0220, -0.0329, -0.0213,  ...,  0.0207, -0.0669, -0.0072],\n                      [ 0.0245, -0.0142, -0.0258,  ..., -0.0042, -0.0440, -0.0686],\n                      [-0.0753, -0.0778,  0.0210,  ...,  0.0569, -0.0301, -0.0044],\n                      ...,\n                      [ 0.0080,  0.0298,  0.0789,  ..., -0.0551, -0.0732,  0.0014],\n                      [-0.0986, -0.0827, -0.0589,  ..., -0.0213, -0.0088, -0.1069],\n                      [ 0.1390,  0.0528, -0.0639,  ...,  0.0481, -0.0453,  0.0316]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.3.output.dense.bias',\n              tensor([-0.0875,  0.0334,  0.0635,  ...,  0.0408, -0.0827,  0.2717],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.3.output.LayerNorm.weight',\n              tensor([0.9752, 0.9922, 0.9689,  ..., 0.9718, 0.9660, 0.9499], device='cuda:0')),\n             ('model.roberta.encoder.layer.3.output.LayerNorm.bias',\n              tensor([ 0.0123, -0.0073,  0.1959,  ..., -0.0248, -0.1183, -0.0796],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.4.attention.self.query.weight',\n              tensor([[-0.0056, -0.0140, -0.0598,  ..., -0.0082, -0.0338, -0.0072],\n                      [-0.0048,  0.0016,  0.0440,  ..., -0.0582, -0.0116,  0.0689],\n                      [-0.0852,  0.0052,  0.0146,  ..., -0.0652, -0.0106,  0.0275],\n                      ...,\n                      [ 0.0159,  0.0072, -0.0133,  ...,  0.0199,  0.0044,  0.0030],\n                      [ 0.0183, -0.0281,  0.0378,  ..., -0.0096, -0.0287,  0.0161],\n                      [ 0.0368, -0.1026, -0.0202,  ..., -0.0379, -0.0363, -0.1468]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.4.attention.self.query.bias',\n              tensor([ 0.2082,  0.0040,  0.2258,  ..., -0.2537, -0.1890,  0.2035],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.4.attention.self.key.weight',\n              tensor([[-0.0197, -0.0760, -0.0482,  ..., -0.0074, -0.0455,  0.0080],\n                      [-0.0512, -0.0093,  0.0219,  ...,  0.0042, -0.0234,  0.0127],\n                      [ 0.0147,  0.0289,  0.0413,  ...,  0.0315,  0.0669, -0.0050],\n                      ...,\n                      [ 0.0384,  0.0799,  0.0368,  ...,  0.0531,  0.0113,  0.0908],\n                      [-0.0171, -0.0263, -0.0292,  ..., -0.0157, -0.0435,  0.0635],\n                      [ 0.0038,  0.0267, -0.0058,  ...,  0.0266, -0.0332,  0.0253]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.4.attention.self.key.bias',\n              tensor([ 0.0006, -0.0003,  0.0003,  ..., -0.0005, -0.0002, -0.0002],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.4.attention.self.value.weight',\n              tensor([[-0.0107,  0.0006,  0.0094,  ...,  0.0208,  0.0034,  0.0476],\n                      [ 0.0200,  0.0611, -0.0226,  ...,  0.0653, -0.0166, -0.0730],\n                      [ 0.0369,  0.0161,  0.0247,  ...,  0.0520, -0.0312,  0.0645],\n                      ...,\n                      [ 0.0123,  0.0651,  0.0227,  ...,  0.0472,  0.0003,  0.0249],\n                      [ 0.0291,  0.0077,  0.0150,  ...,  0.0291, -0.0041,  0.0626],\n                      [ 0.0086, -0.1038,  0.0030,  ..., -0.0399,  0.0336,  0.0094]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.4.attention.self.value.bias',\n              tensor([-0.0098, -0.0038,  0.0077,  ..., -0.0081,  0.0037, -0.0007],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.4.attention.output.dense.weight',\n              tensor([[-0.0342, -0.0707, -0.0225,  ..., -0.0070, -0.0075,  0.0660],\n                      [-0.0462, -0.0171, -0.0112,  ...,  0.0439,  0.0301,  0.0081],\n                      [ 0.0066,  0.0093, -0.0089,  ..., -0.0415,  0.0461, -0.0114],\n                      ...,\n                      [ 0.0050,  0.0062,  0.0087,  ...,  0.0121,  0.0028, -0.0171],\n                      [-0.0024,  0.0127, -0.0316,  ...,  0.0538,  0.0027, -0.0243],\n                      [ 0.0166,  0.0462, -0.0133,  ..., -0.0104, -0.0276,  0.0085]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.4.attention.output.dense.bias',\n              tensor([-0.0085, -0.0131, -0.0091,  ...,  0.0176, -0.0645, -0.0030],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.4.attention.output.LayerNorm.weight',\n              tensor([0.9901, 0.9983, 0.9891,  ..., 0.9890, 0.9958, 0.9583], device='cuda:0')),\n             ('model.roberta.encoder.layer.4.attention.output.LayerNorm.bias',\n              tensor([-0.1697,  0.0773, -0.2984,  ...,  0.0616,  0.0169,  0.2060],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.4.intermediate.dense.weight',\n              tensor([[ 0.0032,  0.0552, -0.0345,  ..., -0.0736,  0.0531,  0.0408],\n                      [-0.0160, -0.0172, -0.0089,  ..., -0.0155, -0.0168, -0.0922],\n                      [-0.0340,  0.0162,  0.0479,  ..., -0.0931,  0.0230, -0.0470],\n                      ...,\n                      [-0.0598, -0.0228,  0.0659,  ...,  0.1275,  0.0425, -0.0710],\n                      [-0.0036, -0.0820, -0.0878,  ..., -0.0560, -0.0259,  0.0620],\n                      [-0.0350, -0.0235,  0.0051,  ...,  0.0192,  0.0386, -0.0442]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.4.intermediate.dense.bias',\n              tensor([-0.0891, -0.0784, -0.0704,  ..., -0.0705, -0.0537, -0.0536],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.4.output.dense.weight',\n              tensor([[-0.0186,  0.0593,  0.0048,  ...,  0.0356, -0.0079, -0.0264],\n                      [-0.0461, -0.0270, -0.0517,  ...,  0.0372, -0.0558,  0.0007],\n                      [ 0.0148,  0.0819,  0.1027,  ...,  0.0006,  0.0547, -0.0305],\n                      ...,\n                      [-0.0368, -0.0450, -0.0651,  ...,  0.1407, -0.0655,  0.0133],\n                      [ 0.0356, -0.0226,  0.0316,  ...,  0.1014, -0.0147,  0.0150],\n                      [ 0.0069, -0.0456, -0.0981,  ..., -0.0394,  0.0646, -0.0399]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.4.output.dense.bias',\n              tensor([-0.0841, -0.0166,  0.1396,  ...,  0.0101, -0.1006,  0.1661],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.4.output.LayerNorm.weight',\n              tensor([0.9723, 0.9974, 0.9766,  ..., 0.9779, 0.9940, 0.9677], device='cuda:0')),\n             ('model.roberta.encoder.layer.4.output.LayerNorm.bias',\n              tensor([ 0.0118, -0.1006,  0.2774,  ..., -0.1046, -0.1016, -0.1641],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.5.attention.self.query.weight',\n              tensor([[ 7.0408e-02, -1.1955e-02,  2.5172e-02,  ..., -1.9046e-02,\n                        2.0191e-02,  3.4957e-02],\n                      [ 5.4052e-02, -1.3421e-02,  6.7722e-03,  ...,  5.5795e-03,\n                        4.5856e-02, -1.3925e-01],\n                      [-5.5882e-02, -1.3268e-02, -1.9467e-02,  ...,  2.3074e-02,\n                       -4.5990e-02, -1.4291e-03],\n                      ...,\n                      [-1.7209e-02, -1.9052e-02, -7.2417e-02,  ..., -3.0964e-02,\n                        1.6361e-02,  3.6595e-06],\n                      [ 4.8705e-02, -8.4964e-02,  2.1131e-02,  ...,  3.4429e-02,\n                        2.0969e-02, -3.2870e-02],\n                      [-7.0167e-03,  2.6256e-02,  5.8856e-02,  ...,  1.0599e-02,\n                        6.9330e-02,  5.1739e-02]], device='cuda:0')),\n             ('model.roberta.encoder.layer.5.attention.self.query.bias',\n              tensor([ 0.0335, -0.0294, -0.0256,  ...,  0.0305,  0.1281, -0.1349],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.5.attention.self.key.weight',\n              tensor([[ 0.0020,  0.0645, -0.0094,  ..., -0.0386,  0.0054, -0.0110],\n                      [ 0.0381, -0.0218, -0.0392,  ...,  0.0395,  0.0328, -0.0368],\n                      [ 0.0348, -0.0155, -0.0324,  ..., -0.1145,  0.0457,  0.0317],\n                      ...,\n                      [ 0.0284, -0.0011, -0.0020,  ...,  0.0289,  0.0654,  0.0407],\n                      [ 0.0205,  0.0051, -0.0124,  ...,  0.0188, -0.0142,  0.0084],\n                      [-0.0709,  0.0015,  0.0607,  ..., -0.0542,  0.0175,  0.0029]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.5.attention.self.key.bias',\n              tensor([ 7.9571e-04, -8.5897e-05, -1.4816e-04,  ...,  1.6833e-03,\n                       8.8868e-04,  4.0856e-04], device='cuda:0')),\n             ('model.roberta.encoder.layer.5.attention.self.value.weight',\n              tensor([[-2.1546e-02,  5.2653e-02,  1.2498e-02,  ...,  6.2362e-04,\n                       -7.4157e-03,  5.1913e-02],\n                      [-6.8202e-02, -6.5192e-02, -1.2259e-02,  ...,  2.1783e-02,\n                       -4.1677e-02, -5.9857e-02],\n                      [ 3.7029e-02, -4.1949e-02, -3.0934e-02,  ..., -2.7965e-05,\n                       -1.2835e-02,  3.1742e-02],\n                      ...,\n                      [ 2.8281e-02, -2.0892e-02, -1.4784e-02,  ..., -3.2425e-02,\n                       -1.1419e-02, -3.0589e-02],\n                      [ 5.5626e-02,  2.7615e-02, -9.6339e-02,  ...,  1.3562e-02,\n                       -8.8221e-03,  4.7425e-02],\n                      [-3.4040e-02,  7.0365e-02, -2.2425e-02,  ..., -2.2821e-02,\n                       -2.0055e-02,  2.2568e-02]], device='cuda:0')),\n             ('model.roberta.encoder.layer.5.attention.self.value.bias',\n              tensor([ 0.0026, -0.0042, -0.0018,  ...,  0.0082, -0.0052, -0.0100],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.5.attention.output.dense.weight',\n              tensor([[ 0.0318, -0.0090, -0.0195,  ...,  0.0221,  0.0261, -0.0237],\n                      [-0.0974,  0.0300,  0.0097,  ..., -0.0503, -0.0193,  0.0464],\n                      [ 0.0645, -0.0084,  0.0430,  ..., -0.0186, -0.0415, -0.0315],\n                      ...,\n                      [-0.0108, -0.0517, -0.0144,  ..., -0.0059,  0.0206, -0.0362],\n                      [ 0.0005,  0.0130,  0.0384,  ...,  0.0081, -0.0702, -0.0024],\n                      [ 0.0061,  0.0250,  0.0211,  ...,  0.0360, -0.0082, -0.0020]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.5.attention.output.dense.bias',\n              tensor([ 0.0058,  0.0081, -0.0759,  ..., -0.0266, -0.0634, -0.0135],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.5.attention.output.LayerNorm.weight',\n              tensor([0.9916, 1.0001, 0.9993,  ..., 0.9804, 0.9910, 0.9710], device='cuda:0')),\n             ('model.roberta.encoder.layer.5.attention.output.LayerNorm.bias',\n              tensor([-0.1204,  0.0563, -0.2711,  ...,  0.0020, -0.0935,  0.0929],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.5.intermediate.dense.weight',\n              tensor([[-0.0028, -0.0401, -0.0321,  ..., -0.0078, -0.0041, -0.0242],\n                      [-0.0045, -0.0438, -0.0201,  ..., -0.0156,  0.0683, -0.0448],\n                      [ 0.0638,  0.0496,  0.0053,  ..., -0.0126, -0.0364,  0.0522],\n                      ...,\n                      [-0.0163, -0.0540, -0.0126,  ...,  0.0598,  0.0021, -0.0706],\n                      [-0.0271, -0.1230,  0.0015,  ...,  0.0301,  0.0671, -0.0313],\n                      [-0.0582,  0.0447,  0.0483,  ...,  0.0298, -0.0015,  0.0437]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.5.intermediate.dense.bias',\n              tensor([-0.0720, -0.1128, -0.1090,  ..., -0.0506, -0.1167, -0.1039],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.5.output.dense.weight',\n              tensor([[-0.0295,  0.0588,  0.0029,  ..., -0.0160, -0.0375,  0.0268],\n                      [ 0.0447, -0.0568,  0.0653,  ..., -0.0451, -0.0204, -0.0792],\n                      [ 0.0077,  0.0124, -0.0131,  ..., -0.0317, -0.0110, -0.0691],\n                      ...,\n                      [-0.0680, -0.0630, -0.0343,  ...,  0.0003,  0.1025, -0.0241],\n                      [-0.0124,  0.0186, -0.0780,  ..., -0.0288, -0.0112, -0.0056],\n                      [ 0.0016, -0.0239,  0.0835,  ..., -0.0524,  0.0290, -0.0358]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.5.output.dense.bias',\n              tensor([-0.1070,  0.0104,  0.1369,  ..., -0.0008, -0.0598,  0.1453],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.5.output.LayerNorm.weight',\n              tensor([0.9707, 0.9944, 0.9902,  ..., 0.9751, 0.9914, 0.9562], device='cuda:0')),\n             ('model.roberta.encoder.layer.5.output.LayerNorm.bias',\n              tensor([-0.0167, -0.1141,  0.2699,  ..., -0.1176, -0.0457, -0.1467],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.6.attention.self.query.weight',\n              tensor([[ 0.0178, -0.0265,  0.0568,  ...,  0.0412,  0.0481,  0.0066],\n                      [-0.0069, -0.0335, -0.0071,  ..., -0.0069, -0.0492, -0.0329],\n                      [ 0.0368,  0.1513,  0.0876,  ...,  0.0477,  0.0437,  0.0360],\n                      ...,\n                      [ 0.0084, -0.0030, -0.1044,  ...,  0.0319,  0.0695,  0.0650],\n                      [ 0.0280,  0.0193,  0.0191,  ...,  0.0058,  0.0260,  0.0867],\n                      [-0.0392, -0.0009,  0.0390,  ..., -0.0418,  0.0321, -0.0299]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.6.attention.self.query.bias',\n              tensor([-0.3081,  0.0550, -0.3072,  ..., -0.0916,  0.0277,  0.0066],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.6.attention.self.key.weight',\n              tensor([[ 0.0270, -0.0302,  0.0684,  ..., -0.0115, -0.0259, -0.0021],\n                      [-0.0180,  0.0303,  0.0197,  ...,  0.0996,  0.0774,  0.0156],\n                      [ 0.0534, -0.0017,  0.0160,  ..., -0.0155, -0.0357, -0.0414],\n                      ...,\n                      [-0.0177,  0.0158,  0.0036,  ..., -0.0357,  0.0154, -0.0025],\n                      [-0.0739, -0.0638, -0.0009,  ...,  0.0462,  0.0378, -0.0072],\n                      [ 0.0099,  0.0336, -0.0008,  ...,  0.0577,  0.0321, -0.0107]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.6.attention.self.key.bias',\n              tensor([ 8.0783e-04, -6.4010e-04,  1.3015e-04,  ...,  6.7702e-05,\n                       2.8465e-04,  2.2267e-04], device='cuda:0')),\n             ('model.roberta.encoder.layer.6.attention.self.value.weight',\n              tensor([[ 1.1166e-02,  1.5038e-02, -6.4858e-03,  ..., -3.0267e-02,\n                       -1.3275e-02, -1.0561e-04],\n                      [-2.0662e-03,  3.9441e-02,  1.8223e-02,  ..., -5.6264e-02,\n                        3.8706e-02,  1.3096e-02],\n                      [-1.7152e-02, -5.6935e-02,  5.4740e-02,  ...,  4.6787e-02,\n                        3.0740e-04,  2.1857e-02],\n                      ...,\n                      [ 1.6006e-02,  4.0863e-02,  1.2247e-03,  ...,  4.8919e-02,\n                       -5.9135e-02,  2.0161e-03],\n                      [ 9.0825e-03, -1.5055e-02, -5.7321e-03,  ..., -3.0596e-02,\n                        1.7313e-02, -1.4167e-02],\n                      [ 1.1951e-02,  2.3854e-02,  2.4533e-02,  ..., -3.5601e-02,\n                        1.0612e-01,  2.7050e-02]], device='cuda:0')),\n             ('model.roberta.encoder.layer.6.attention.self.value.bias',\n              tensor([-0.0072, -0.0061, -0.0012,  ..., -0.0173, -0.0084, -0.0031],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.6.attention.output.dense.weight',\n              tensor([[-0.0398,  0.0042, -0.0182,  ...,  0.0396, -0.0317, -0.0052],\n                      [ 0.0194, -0.0350, -0.0399,  ...,  0.0197,  0.0381, -0.0205],\n                      [ 0.0495, -0.0573, -0.0381,  ...,  0.0267,  0.0668, -0.0350],\n                      ...,\n                      [-0.0169,  0.0496,  0.0003,  ..., -0.0423,  0.0053,  0.0203],\n                      [-0.0281,  0.0216,  0.0425,  ...,  0.0251, -0.0196, -0.0400],\n                      [-0.0222,  0.0457,  0.0221,  ...,  0.0599,  0.0053,  0.0198]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.6.attention.output.dense.bias',\n              tensor([ 7.4835e-03, -1.1293e-02,  3.0992e-02,  ..., -7.7391e-05,\n                      -6.7851e-02,  9.7852e-03], device='cuda:0')),\n             ('model.roberta.encoder.layer.6.attention.output.LayerNorm.weight',\n              tensor([0.9843, 0.9998, 0.9994,  ..., 0.9826, 0.9794, 0.9532], device='cuda:0')),\n             ('model.roberta.encoder.layer.6.attention.output.LayerNorm.bias',\n              tensor([-0.1211,  0.0571, -0.2851,  ..., -0.0226, -0.1031,  0.1019],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.6.intermediate.dense.weight',\n              tensor([[-0.0060, -0.0062, -0.0125,  ..., -0.0043,  0.0377,  0.0698],\n                      [ 0.0049,  0.0057, -0.0003,  ..., -0.0284,  0.0230, -0.0415],\n                      [-0.0612, -0.0775, -0.0474,  ...,  0.0750, -0.0154, -0.0805],\n                      ...,\n                      [-0.0476, -0.0436,  0.0068,  ..., -0.0108,  0.0789, -0.0527],\n                      [ 0.0046,  0.0063,  0.0087,  ...,  0.0148,  0.0109,  0.0084],\n                      [-0.0096, -0.0603,  0.0455,  ..., -0.0203,  0.0183, -0.1271]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.6.intermediate.dense.bias',\n              tensor([-0.0803, -0.0929, -0.1049,  ..., -0.0787, -0.0703, -0.1734],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.6.output.dense.weight',\n              tensor([[ 2.6498e-02, -3.5462e-02, -2.0597e-02,  ...,  3.4925e-02,\n                        2.2147e-02, -8.8375e-03],\n                      [ 8.2560e-03, -4.2557e-02, -7.5041e-03,  ..., -8.3933e-03,\n                        3.6447e-02,  9.9017e-03],\n                      [ 2.4543e-02, -1.2822e-02, -9.1667e-02,  ...,  4.4327e-02,\n                       -4.3462e-02,  4.0296e-02],\n                      ...,\n                      [ 9.1859e-03, -6.2894e-02,  9.1473e-03,  ..., -1.6280e-02,\n                        5.5906e-02, -4.6639e-02],\n                      [ 3.0482e-02,  7.5090e-05, -1.1150e-01,  ..., -1.5714e-02,\n                       -2.7834e-02,  6.4731e-02],\n                      [ 3.9153e-02,  6.8173e-03,  1.8265e-02,  ..., -3.1074e-02,\n                        2.6418e-02, -1.9295e-02]], device='cuda:0')),\n             ('model.roberta.encoder.layer.6.output.dense.bias',\n              tensor([-0.0675,  0.0309,  0.0655,  ..., -0.0087, -0.0564,  0.0717],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.6.output.LayerNorm.weight',\n              tensor([0.9819, 0.9901, 0.9997,  ..., 0.9781, 0.9741, 0.9625], device='cuda:0')),\n             ('model.roberta.encoder.layer.6.output.LayerNorm.bias',\n              tensor([-0.0235, -0.1129,  0.1114,  ..., -0.0809, -0.0288, -0.1327],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.7.attention.self.query.weight',\n              tensor([[ 0.0175, -0.0192,  0.0039,  ...,  0.0256, -0.0388, -0.1087],\n                      [ 0.0557, -0.0803, -0.0160,  ...,  0.0165, -0.0431,  0.0228],\n                      [-0.1335,  0.0197,  0.0234,  ...,  0.0102,  0.0810,  0.0014],\n                      ...,\n                      [-0.0311,  0.1944,  0.0416,  ...,  0.0561,  0.0081,  0.1292],\n                      [ 0.0098,  0.0232, -0.0311,  ...,  0.0115,  0.0724,  0.1001],\n                      [ 0.0205,  0.0126,  0.0275,  ...,  0.0933,  0.1159,  0.0467]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.7.attention.self.query.bias',\n              tensor([ 0.0156,  0.0153, -0.0557,  ..., -0.2539,  0.1253, -0.2635],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.7.attention.self.key.weight',\n              tensor([[ 0.0039, -0.0005, -0.0542,  ..., -0.0058, -0.0040,  0.0635],\n                      [ 0.0154,  0.0058,  0.0243,  ...,  0.0235, -0.0449,  0.0060],\n                      [-0.0078, -0.0428, -0.0183,  ...,  0.0224, -0.0369,  0.0288],\n                      ...,\n                      [-0.0257,  0.0021,  0.0270,  ..., -0.0504, -0.0782, -0.0426],\n                      [-0.0687,  0.0406,  0.0184,  ..., -0.0086, -0.0238,  0.0822],\n                      [ 0.0232,  0.0236,  0.0265,  ...,  0.0120, -0.0125, -0.0338]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.7.attention.self.key.bias',\n              tensor([-0.0004, -0.0008, -0.0002,  ...,  0.0003,  0.0006,  0.0002],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.7.attention.self.value.weight',\n              tensor([[ 0.0050, -0.0206,  0.0016,  ...,  0.0200, -0.0025, -0.0399],\n                      [-0.0115, -0.0301, -0.0226,  ..., -0.0164, -0.0344, -0.0484],\n                      [ 0.0257, -0.0206,  0.0247,  ..., -0.0127,  0.0236,  0.0082],\n                      ...,\n                      [-0.0009, -0.0404,  0.0142,  ...,  0.0215,  0.0375,  0.0115],\n                      [-0.0544, -0.0558, -0.0022,  ...,  0.0462,  0.0214,  0.0225],\n                      [ 0.0349, -0.0005, -0.0240,  ...,  0.0336, -0.0357, -0.0145]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.7.attention.self.value.bias',\n              tensor([ 0.0014,  0.0330, -0.0525,  ...,  0.0140,  0.0030,  0.0060],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.7.attention.output.dense.weight',\n              tensor([[-0.0007, -0.0081,  0.0298,  ...,  0.0084,  0.0633, -0.0315],\n                      [-0.0417,  0.0014,  0.0143,  ...,  0.0410, -0.0804,  0.0269],\n                      [-0.0351, -0.0190,  0.0115,  ...,  0.0449,  0.0403,  0.0357],\n                      ...,\n                      [ 0.0478,  0.0040,  0.0518,  ...,  0.0046, -0.0663, -0.0048],\n                      [ 0.0101,  0.0152,  0.0113,  ..., -0.0082,  0.0165,  0.0317],\n                      [-0.0101, -0.0270, -0.0098,  ..., -0.0515, -0.0141,  0.0475]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.7.attention.output.dense.bias',\n              tensor([-0.0173,  0.0167, -0.0376,  ..., -0.0456, -0.0042,  0.0286],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.7.attention.output.LayerNorm.weight',\n              tensor([0.9887, 0.9886, 1.0000,  ..., 0.9794, 0.9583, 0.9758], device='cuda:0')),\n             ('model.roberta.encoder.layer.7.attention.output.LayerNorm.bias',\n              tensor([-0.0920,  0.0113, -0.4248,  ..., -0.0739, -0.1018, -0.0004],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.7.intermediate.dense.weight',\n              tensor([[-0.0629,  0.0339,  0.0172,  ...,  0.0642,  0.0059,  0.0341],\n                      [ 0.0638,  0.0390, -0.0088,  ..., -0.0017, -0.0396, -0.0380],\n                      [-0.0376, -0.0243, -0.0026,  ...,  0.0157,  0.0146,  0.0333],\n                      ...,\n                      [ 0.0074,  0.0325,  0.0034,  ..., -0.0392, -0.0209,  0.0082],\n                      [-0.0332, -0.1083,  0.0162,  ..., -0.0879, -0.0075, -0.0107],\n                      [-0.0480, -0.0514,  0.0027,  ...,  0.0442, -0.0248,  0.0343]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.7.intermediate.dense.bias',\n              tensor([-0.0926, -0.0903, -0.0096,  ...,  0.0081, -0.0850, -0.0318],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.7.output.dense.weight',\n              tensor([[-0.0123,  0.0713, -0.0155,  ...,  0.0144, -0.1024, -0.0405],\n                      [ 0.0316,  0.0810,  0.0063,  ..., -0.0380, -0.0654,  0.0314],\n                      [-0.0144, -0.0092, -0.0086,  ...,  0.0308,  0.0150,  0.0087],\n                      ...,\n                      [-0.0590,  0.0396, -0.0251,  ...,  0.0265, -0.0122,  0.0279],\n                      [-0.0080, -0.0009, -0.0240,  ...,  0.0289,  0.0223,  0.0144],\n                      [ 0.0683, -0.0199,  0.0124,  ..., -0.0291,  0.0195, -0.0424]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.7.output.dense.bias',\n              tensor([-0.1938,  0.0498,  0.1157,  ..., -0.0158, -0.0430,  0.0589],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.7.output.LayerNorm.weight',\n              tensor([0.9889, 0.9911, 0.9977,  ..., 0.9790, 0.9756, 0.9531], device='cuda:0')),\n             ('model.roberta.encoder.layer.7.output.LayerNorm.bias',\n              tensor([-0.0313, -0.0960, -0.0038,  ..., -0.0470, -0.0413, -0.1091],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.8.attention.self.query.weight',\n              tensor([[ 0.0338,  0.0329,  0.0300,  ...,  0.0205,  0.0183,  0.0122],\n                      [-0.0900, -0.0687,  0.0659,  ..., -0.0379,  0.0010,  0.0104],\n                      [-0.0647,  0.1081, -0.0300,  ...,  0.0418,  0.0686, -0.0024],\n                      ...,\n                      [ 0.0599,  0.0424, -0.0033,  ..., -0.0089,  0.0845, -0.0064],\n                      [ 0.0116,  0.0792, -0.0302,  ..., -0.1245,  0.0404,  0.0293],\n                      [-0.0276, -0.0819,  0.0009,  ...,  0.0079, -0.0181, -0.0443]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.8.attention.self.query.bias',\n              tensor([-0.0941,  0.1287,  0.0203,  ..., -0.1395, -0.0559,  0.0577],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.8.attention.self.key.weight',\n              tensor([[-0.0709,  0.0477, -0.0019,  ..., -0.1569,  0.0536, -0.0106],\n                      [-0.0547, -0.0025,  0.1163,  ...,  0.0147,  0.0189, -0.0316],\n                      [ 0.0402, -0.0392, -0.0601,  ..., -0.0313,  0.1138, -0.0295],\n                      ...,\n                      [-0.0088,  0.0547, -0.0404,  ...,  0.0556,  0.0008,  0.0331],\n                      [-0.0785,  0.0272, -0.0478,  ...,  0.0140,  0.0409, -0.0279],\n                      [-0.0102, -0.0875,  0.0326,  ...,  0.0154,  0.0091, -0.0657]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.8.attention.self.key.bias',\n              tensor([ 3.1322e-05,  1.7873e-04, -3.3399e-04,  ...,  6.7567e-05,\n                      -1.5471e-04,  3.4534e-05], device='cuda:0')),\n             ('model.roberta.encoder.layer.8.attention.self.value.weight',\n              tensor([[ 0.0452, -0.0008, -0.0156,  ..., -0.0483,  0.0313,  0.0338],\n                      [-0.0490, -0.0343, -0.0009,  ...,  0.0648,  0.0002,  0.0496],\n                      [ 0.0017, -0.0050,  0.0041,  ..., -0.0022, -0.0091, -0.0654],\n                      ...,\n                      [ 0.0026,  0.0345,  0.0182,  ..., -0.0228, -0.0395, -0.0326],\n                      [ 0.0506, -0.0474,  0.0024,  ..., -0.0063, -0.0169, -0.0147],\n                      [-0.0285, -0.0166,  0.0029,  ...,  0.0424, -0.0168, -0.0419]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.8.attention.self.value.bias',\n              tensor([-0.0071, -0.0145,  0.0098,  ..., -0.0143, -0.0067, -0.0035],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.8.attention.output.dense.weight',\n              tensor([[ 2.7777e-02, -1.9914e-03, -7.5314e-03,  ..., -3.3341e-04,\n                        7.3229e-03, -9.7807e-04],\n                      [ 1.1458e-03, -1.3833e-02,  3.0192e-02,  ..., -6.9556e-05,\n                        3.8902e-02,  6.1805e-02],\n                      [ 4.8357e-02, -3.9734e-02,  2.5039e-02,  ...,  4.3991e-02,\n                       -3.5469e-02,  5.4006e-03],\n                      ...,\n                      [-3.3378e-02,  2.5277e-02, -1.7415e-02,  ...,  3.7146e-02,\n                       -3.4340e-02, -2.9719e-02],\n                      [-4.7832e-02,  5.4526e-03, -2.5408e-02,  ...,  1.6492e-03,\n                        2.4625e-03,  5.4373e-03],\n                      [ 2.8449e-02, -9.7396e-03,  3.3103e-02,  ...,  2.2169e-02,\n                        4.8335e-03,  4.5559e-02]], device='cuda:0')),\n             ('model.roberta.encoder.layer.8.attention.output.dense.bias',\n              tensor([-0.0504,  0.0057,  0.0052,  ...,  0.0096, -0.0960,  0.0561],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.8.attention.output.LayerNorm.weight',\n              tensor([0.9887, 0.9967, 0.9986,  ..., 0.9878, 0.9935, 0.9751], device='cuda:0')),\n             ('model.roberta.encoder.layer.8.attention.output.LayerNorm.bias',\n              tensor([-0.1704,  0.0130, -0.4998,  ..., -0.0107, -0.1096, -0.0132],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.8.intermediate.dense.weight',\n              tensor([[-0.0090, -0.0019,  0.0198,  ..., -0.0657,  0.0124, -0.0757],\n                      [ 0.0295, -0.0741,  0.0385,  ..., -0.1112,  0.0503, -0.0232],\n                      [ 0.0341,  0.0990,  0.0385,  ...,  0.0298,  0.0098,  0.0144],\n                      ...,\n                      [ 0.0459, -0.0163,  0.0366,  ..., -0.0227, -0.0170,  0.0351],\n                      [ 0.0880,  0.0201,  0.0092,  ...,  0.0153,  0.0541, -0.0733],\n                      [ 0.0371,  0.0055,  0.0200,  ..., -0.0257, -0.0658, -0.0160]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.8.intermediate.dense.bias',\n              tensor([-0.0618, -0.0993, -0.0519,  ..., -0.0760, -0.0579, -0.0357],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.8.output.dense.weight',\n              tensor([[ 0.0076,  0.0065,  0.0107,  ..., -0.0309,  0.0682,  0.0143],\n                      [-0.0340,  0.0158,  0.0665,  ...,  0.0175,  0.0109,  0.0272],\n                      [ 0.0385, -0.0156,  0.0180,  ...,  0.0369, -0.0030, -0.0028],\n                      ...,\n                      [ 0.0683, -0.0605,  0.0296,  ...,  0.0504, -0.0133,  0.0211],\n                      [-0.0152, -0.0048,  0.0160,  ..., -0.0636, -0.0103, -0.0048],\n                      [ 0.0222,  0.0234,  0.0252,  ...,  0.0247,  0.0077,  0.0233]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.8.output.dense.bias',\n              tensor([-0.1678,  0.0010,  0.0617,  ..., -0.0398, -0.0500, -0.0565],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.8.output.LayerNorm.weight',\n              tensor([0.9766, 0.9949, 0.9983,  ..., 0.9761, 0.9797, 0.9541], device='cuda:0')),\n             ('model.roberta.encoder.layer.8.output.LayerNorm.bias',\n              tensor([ 0.0213, -0.1100, -0.0189,  ..., -0.0827, -0.0250, -0.0834],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.9.attention.self.query.weight',\n              tensor([[ 0.0099, -0.0850, -0.0348,  ..., -0.0320,  0.0666, -0.0669],\n                      [ 0.0221, -0.0285, -0.0068,  ..., -0.0312,  0.0522,  0.0561],\n                      [ 0.0199,  0.0242,  0.0171,  ..., -0.0409,  0.0129, -0.0287],\n                      ...,\n                      [ 0.0588, -0.0249, -0.0011,  ..., -0.0132, -0.0642, -0.0134],\n                      [ 0.0437,  0.0043, -0.0784,  ...,  0.0373,  0.0282,  0.0219],\n                      [-0.0215, -0.0203, -0.0430,  ..., -0.0455, -0.0362, -0.0461]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.9.attention.self.query.bias',\n              tensor([-0.0135, -0.0328, -0.0607,  ...,  0.0041, -0.0597,  0.0752],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.9.attention.self.key.weight',\n              tensor([[-0.0380,  0.0479,  0.0010,  ..., -0.0148, -0.0309, -0.0375],\n                      [ 0.0373, -0.0061, -0.0034,  ..., -0.0350, -0.0183, -0.0393],\n                      [ 0.0352, -0.0496,  0.0711,  ..., -0.0708, -0.0277,  0.0124],\n                      ...,\n                      [-0.0103,  0.0126,  0.0029,  ...,  0.0093,  0.0546,  0.0084],\n                      [ 0.0430,  0.0340, -0.0124,  ...,  0.0120,  0.0278,  0.0087],\n                      [-0.0386,  0.0088, -0.0310,  ...,  0.0865, -0.0022,  0.0999]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.9.attention.self.key.bias',\n              tensor([-1.1960e-04,  3.3924e-07,  2.7129e-04,  ...,  1.5533e-04,\n                       2.4057e-04,  1.4449e-04], device='cuda:0')),\n             ('model.roberta.encoder.layer.9.attention.self.value.weight',\n              tensor([[-0.0249,  0.0009,  0.0102,  ..., -0.0263, -0.0539,  0.0287],\n                      [-0.0219, -0.0336, -0.0011,  ..., -0.0548, -0.0031,  0.0577],\n                      [ 0.0022,  0.0547,  0.0111,  ...,  0.0223,  0.0269,  0.0105],\n                      ...,\n                      [-0.0557,  0.0351,  0.0136,  ...,  0.0562,  0.0573, -0.0030],\n                      [ 0.0535,  0.0154, -0.0107,  ..., -0.0319, -0.0388, -0.0322],\n                      [-0.0349,  0.0144,  0.0082,  ...,  0.0195,  0.0050, -0.0163]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.9.attention.self.value.bias',\n              tensor([ 1.7203e-03, -7.3136e-03,  7.1018e-04,  ...,  3.1688e-02,\n                      -1.3867e-03,  7.0744e-05], device='cuda:0')),\n             ('model.roberta.encoder.layer.9.attention.output.dense.weight',\n              tensor([[-0.0172, -0.0266,  0.0082,  ...,  0.0201, -0.0109, -0.0212],\n                      [ 0.0043, -0.0433,  0.0050,  ..., -0.0445,  0.0188,  0.0192],\n                      [ 0.0344, -0.0663, -0.0044,  ..., -0.0130,  0.0398,  0.0035],\n                      ...,\n                      [ 0.0027,  0.0119,  0.0315,  ..., -0.0194,  0.0045,  0.0011],\n                      [-0.0182, -0.0066, -0.0092,  ..., -0.0815, -0.0188,  0.0550],\n                      [ 0.0261,  0.0156,  0.0271,  ..., -0.0142, -0.0114, -0.0040]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.9.attention.output.dense.bias',\n              tensor([ 0.0464,  0.0043, -0.1728,  ...,  0.0541, -0.0483,  0.0056],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.9.attention.output.LayerNorm.weight',\n              tensor([0.9855, 0.9911, 0.9995,  ..., 0.9836, 0.9926, 0.9895], device='cuda:0')),\n             ('model.roberta.encoder.layer.9.attention.output.LayerNorm.bias',\n              tensor([-0.1783, -0.0268, -0.4646,  ..., -0.0468, -0.1158, -0.0275],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.9.intermediate.dense.weight',\n              tensor([[ 0.0266,  0.0057, -0.0156,  ...,  0.0243,  0.0238,  0.0025],\n                      [ 0.0060, -0.0034,  0.0139,  ...,  0.0829,  0.0247,  0.0155],\n                      [ 0.0493, -0.0477,  0.0103,  ...,  0.0608, -0.0799,  0.0646],\n                      ...,\n                      [ 0.0063, -0.0366,  0.0122,  ...,  0.0735,  0.0061, -0.0212],\n                      [ 0.0145, -0.0559,  0.0085,  ...,  0.0026, -0.0104, -0.0103],\n                      [-0.0181, -0.0306,  0.0116,  ...,  0.0066,  0.0459,  0.0192]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.9.intermediate.dense.bias',\n              tensor([-0.0532, -0.0788, -0.0226,  ..., -0.0465, -0.0445, -0.1011],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.9.output.dense.weight',\n              tensor([[-0.0025,  0.0216, -0.0162,  ..., -0.0051,  0.0220,  0.0032],\n                      [ 0.0347,  0.0768, -0.0194,  ..., -0.0200, -0.0082, -0.0111],\n                      [ 0.0105,  0.0047,  0.0155,  ...,  0.0133, -0.0044, -0.0019],\n                      ...,\n                      [ 0.0139,  0.0910, -0.0523,  ..., -0.0514,  0.0208,  0.0187],\n                      [ 0.0323,  0.0469, -0.0493,  ...,  0.0442,  0.0327,  0.0002],\n                      [-0.0410, -0.0280, -0.0465,  ..., -0.0105,  0.0209,  0.0041]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.9.output.dense.bias',\n              tensor([-0.1372,  0.0678, -0.0873,  ..., -0.0440, -0.0782, -0.0233],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.9.output.LayerNorm.weight',\n              tensor([0.9747, 0.9865, 0.9983,  ..., 0.9715, 0.9804, 0.9740], device='cuda:0')),\n             ('model.roberta.encoder.layer.9.output.LayerNorm.bias',\n              tensor([ 0.0327, -0.0624, -0.2093,  ..., -0.0584, -0.0123, -0.0452],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.10.attention.self.query.weight',\n              tensor([[ 0.0666,  0.0422,  0.0003,  ..., -0.1503,  0.0457,  0.0184],\n                      [ 0.0650, -0.1047, -0.0120,  ...,  0.0781,  0.0019, -0.0313],\n                      [-0.0313, -0.0138, -0.0383,  ...,  0.0796,  0.0932, -0.0403],\n                      ...,\n                      [ 0.0365,  0.0318,  0.2361,  ...,  0.0909, -0.0647,  0.0623],\n                      [ 0.0203,  0.0189,  0.0632,  ...,  0.1160,  0.0497, -0.0483],\n                      [ 0.0328, -0.0197,  0.0491,  ...,  0.0585,  0.0578, -0.0202]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.10.attention.self.query.bias',\n              tensor([ 0.0239,  0.0331, -0.0164,  ...,  0.0297,  0.0545,  0.0015],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.10.attention.self.key.weight',\n              tensor([[-0.0121,  0.0407,  0.0558,  ..., -0.0191, -0.0260, -0.0511],\n                      [-0.0956, -0.0978,  0.0249,  ..., -0.0680, -0.0020, -0.0057],\n                      [-0.0333,  0.0773, -0.0063,  ..., -0.1074,  0.0021, -0.0553],\n                      ...,\n                      [ 0.0184, -0.0089,  0.2545,  ..., -0.0746, -0.0285,  0.0267],\n                      [ 0.0086, -0.0102,  0.0819,  ..., -0.0087, -0.0010, -0.0160],\n                      [-0.0297, -0.0446,  0.0271,  ...,  0.0653, -0.0072, -0.0236]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.10.attention.self.key.bias',\n              tensor([-0.0006, -0.0008,  0.0005,  ...,  0.0002,  0.0003, -0.0005],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.10.attention.self.value.weight',\n              tensor([[ 0.0098,  0.0449,  0.0092,  ..., -0.0020, -0.0268, -0.0292],\n                      [ 0.0264, -0.0030, -0.0098,  ..., -0.0140, -0.0026, -0.0173],\n                      [ 0.0366,  0.0514,  0.0046,  ..., -0.0031,  0.0301, -0.0193],\n                      ...,\n                      [-0.0179,  0.0041, -0.0014,  ...,  0.0016,  0.0106, -0.0424],\n                      [ 0.0119,  0.0159, -0.0073,  ...,  0.0189, -0.0383,  0.0127],\n                      [-0.0260, -0.0147, -0.0136,  ..., -0.0279,  0.0130, -0.0158]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.10.attention.self.value.bias',\n              tensor([ 0.0224,  0.0109, -0.0342,  ...,  0.0206, -0.0146,  0.0489],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.10.attention.output.dense.weight',\n              tensor([[ 0.0088,  0.0529, -0.0076,  ...,  0.0132,  0.0365,  0.0216],\n                      [-0.0182, -0.0066, -0.0140,  ..., -0.0441,  0.0262, -0.0149],\n                      [ 0.0308,  0.0224, -0.1373,  ..., -0.0697, -0.0635, -0.0106],\n                      ...,\n                      [ 0.0054, -0.0070, -0.0353,  ..., -0.0068, -0.0083, -0.0196],\n                      [ 0.0303, -0.0005,  0.0476,  ..., -0.0175, -0.0006,  0.0161],\n                      [ 0.0519, -0.0015,  0.0059,  ..., -0.0037, -0.0099, -0.0267]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.10.attention.output.dense.bias',\n              tensor([-0.0059,  0.0634,  0.0547,  ...,  0.0359, -0.0604,  0.0749],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.10.attention.output.LayerNorm.weight',\n              tensor([0.9738, 0.9888, 1.0002,  ..., 0.9892, 0.9913, 0.9883], device='cuda:0')),\n             ('model.roberta.encoder.layer.10.attention.output.LayerNorm.bias',\n              tensor([-0.1808, -0.0027, -0.4709,  ..., -0.0679, -0.1518, -0.0403],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.10.intermediate.dense.weight',\n              tensor([[-0.0149, -0.0058,  0.0074,  ..., -0.0165, -0.0106, -0.0762],\n                      [ 0.0040,  0.0187,  0.0343,  ...,  0.0530,  0.0367, -0.0076],\n                      [ 0.0010, -0.0168,  0.0289,  ...,  0.0292,  0.0343,  0.0019],\n                      ...,\n                      [ 0.0199,  0.0248,  0.0224,  ...,  0.0711, -0.0373,  0.0018],\n                      [ 0.0535, -0.0179,  0.0035,  ...,  0.0356,  0.0594, -0.0288],\n                      [-0.0092, -0.0195, -0.0104,  ...,  0.0183,  0.0030, -0.0229]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.10.intermediate.dense.bias',\n              tensor([-0.0587, -0.0506, -0.0152,  ..., -0.0913, -0.0904, -0.0130],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.10.output.dense.weight',\n              tensor([[-1.6004e-02,  3.5679e-03, -2.5855e-03,  ...,  5.6638e-02,\n                        1.6062e-02, -5.7051e-02],\n                      [-3.3362e-02,  1.0684e-02,  9.1441e-04,  ...,  1.3683e-02,\n                       -2.2742e-02, -4.4810e-02],\n                      [ 7.1630e-03, -6.9043e-03,  1.0186e-02,  ...,  3.6656e-04,\n                       -1.1539e-02, -3.1768e-05],\n                      ...,\n                      [ 1.2011e-02, -1.4338e-02, -1.1597e-02,  ...,  2.2233e-02,\n                       -6.5982e-03, -5.2925e-03],\n                      [-2.0382e-02, -9.4639e-02, -4.7330e-02,  ...,  2.6466e-02,\n                        1.0419e-02, -4.7233e-02],\n                      [-2.7080e-02,  1.5105e-05, -4.3638e-02,  ...,  5.6505e-03,\n                        2.3388e-02,  1.0987e-02]], device='cuda:0')),\n             ('model.roberta.encoder.layer.10.output.dense.bias',\n              tensor([-0.2124,  0.0541,  0.0884,  ..., -0.0427,  0.0076, -0.1133],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.10.output.LayerNorm.weight',\n              tensor([0.9775, 0.9867, 1.0005,  ..., 0.9776, 0.9759, 0.9872], device='cuda:0')),\n             ('model.roberta.encoder.layer.10.output.LayerNorm.bias',\n              tensor([ 0.0442, -0.0735, -0.2014,  ..., -0.0414,  0.0385, -0.0472],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.11.attention.self.query.weight',\n              tensor([[ 0.0340, -0.0093,  0.0269,  ...,  0.0180,  0.0840,  0.0375],\n                      [ 0.0713, -0.0442, -0.0032,  ..., -0.0235,  0.0122,  0.0315],\n                      [ 0.0554, -0.0532, -0.2156,  ...,  0.0499,  0.0728, -0.0326],\n                      ...,\n                      [ 0.0948, -0.0218,  0.0834,  ..., -0.0429,  0.0637, -0.0691],\n                      [-0.0688, -0.0196, -0.0629,  ..., -0.0203, -0.0325, -0.0840],\n                      [ 0.0352,  0.0054, -0.0348,  ..., -0.0497, -0.0933, -0.0493]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.11.attention.self.query.bias',\n              tensor([0.0095, 0.0291, 0.0472,  ..., 0.1017, 0.0171, 0.0206], device='cuda:0')),\n             ('model.roberta.encoder.layer.11.attention.self.key.weight',\n              tensor([[-0.0880, -0.0029,  0.0747,  ...,  0.0049, -0.0254, -0.0127],\n                      [-0.0617, -0.0186, -0.0321,  ...,  0.0194, -0.0287,  0.0743],\n                      [-0.1166,  0.0084, -0.2359,  ..., -0.0585, -0.0022,  0.0058],\n                      ...,\n                      [-0.0390, -0.0310,  0.1101,  ..., -0.0805, -0.0360, -0.0243],\n                      [-0.0150,  0.0082, -0.0631,  ..., -0.1076,  0.0090,  0.0088],\n                      [ 0.0111, -0.0785, -0.0334,  ..., -0.0418, -0.0181, -0.0156]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.11.attention.self.key.bias',\n              tensor([ 7.4500e-04, -4.8699e-05, -5.4604e-05,  ...,  7.5768e-05,\n                       8.8422e-05,  1.1074e-04], device='cuda:0')),\n             ('model.roberta.encoder.layer.11.attention.self.value.weight',\n              tensor([[ 0.0102, -0.0305,  0.0021,  ...,  0.0622,  0.0052, -0.0016],\n                      [ 0.0242, -0.0336, -0.0047,  ..., -0.0775, -0.0301,  0.0371],\n                      [-0.0457, -0.0138,  0.0265,  ..., -0.0840, -0.0293,  0.0197],\n                      ...,\n                      [-0.0055,  0.0473, -0.0018,  ...,  0.0011, -0.0473, -0.0011],\n                      [ 0.0430,  0.0271,  0.0009,  ...,  0.0474,  0.0319, -0.0793],\n                      [-0.0125,  0.0003,  0.0026,  ..., -0.0480,  0.0326, -0.0233]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.11.attention.self.value.bias',\n              tensor([-0.0097,  0.0118,  0.0056,  ..., -0.0274,  0.0112, -0.0349],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.11.attention.output.dense.weight',\n              tensor([[-0.0069, -0.0230,  0.0234,  ...,  0.0075,  0.0044, -0.0253],\n                      [ 0.0443,  0.0113,  0.0222,  ...,  0.0066, -0.0023,  0.0317],\n                      [-0.0261,  0.0078, -0.0313,  ...,  0.0064,  0.0195, -0.0211],\n                      ...,\n                      [-0.0111,  0.0019,  0.0655,  ...,  0.0203,  0.0249, -0.0144],\n                      [ 0.0071,  0.0199,  0.0326,  ...,  0.0117,  0.0315, -0.0083],\n                      [ 0.0237, -0.0555, -0.0038,  ..., -0.0173, -0.0238, -0.0273]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.11.attention.output.dense.bias',\n              tensor([-0.0512,  0.1444, -0.1528,  ...,  0.0594, -0.0474,  0.0368],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.11.attention.output.LayerNorm.weight',\n              tensor([0.9819, 0.9916, 0.9996,  ..., 0.9929, 0.9936, 0.9823], device='cuda:0')),\n             ('model.roberta.encoder.layer.11.attention.output.LayerNorm.bias',\n              tensor([-0.2121,  0.0090, -0.3464,  ..., -0.0615, -0.0706, -0.0354],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.11.intermediate.dense.weight',\n              tensor([[ 0.0087, -0.0360, -0.0084,  ...,  0.0520,  0.0297,  0.0429],\n                      [ 0.1040,  0.0584,  0.0057,  ...,  0.0590,  0.0357, -0.0843],\n                      [ 0.0011,  0.0449, -0.0078,  ...,  0.0391,  0.0105, -0.0368],\n                      ...,\n                      [ 0.0597,  0.0128,  0.0079,  ..., -0.0042,  0.0506, -0.1024],\n                      [ 0.0501,  0.0049,  0.0151,  ...,  0.0645, -0.0264,  0.0392],\n                      [ 0.0682,  0.0182,  0.0373,  ...,  0.0135,  0.0011,  0.0129]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.11.intermediate.dense.bias',\n              tensor([ 0.0244, -0.0472, -0.0542,  ..., -0.0525, -0.0638,  0.0372],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.11.output.dense.weight',\n              tensor([[ 0.0067,  0.0330, -0.0033,  ...,  0.0075,  0.0257, -0.0506],\n                      [ 0.0198,  0.0423,  0.0039,  ..., -0.0208,  0.0487,  0.0063],\n                      [-0.0166,  0.0202, -0.0091,  ..., -0.0042, -0.0067, -0.0032],\n                      ...,\n                      [ 0.0518, -0.0554,  0.0646,  ..., -0.0228, -0.0163, -0.0113],\n                      [ 0.0100,  0.0293, -0.0420,  ...,  0.0119, -0.0350,  0.0154],\n                      [ 0.0274, -0.0420,  0.0018,  ..., -0.0300, -0.0603, -0.0135]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.11.output.dense.bias',\n              tensor([-0.1755, -0.0094, -0.0050,  ..., -0.0139, -0.0836, -0.1054],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.11.output.LayerNorm.weight',\n              tensor([0.9868, 0.9908, 1.0002,  ..., 0.9778, 0.9803, 0.9910], device='cuda:0')),\n             ('model.roberta.encoder.layer.11.output.LayerNorm.bias',\n              tensor([ 0.0860, -0.0868,  0.1795,  ..., -0.0373, -0.0192, -0.0718],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.12.attention.self.query.weight',\n              tensor([[-0.0495, -0.0260,  0.0886,  ...,  0.0023,  0.0530,  0.0579],\n                      [-0.0218,  0.0069,  0.0134,  ..., -0.0785, -0.0637,  0.0537],\n                      [-0.0565,  0.0396, -0.1348,  ...,  0.0578,  0.0296,  0.0500],\n                      ...,\n                      [ 0.0242,  0.0198,  0.2908,  ..., -0.0141,  0.0941, -0.0235],\n                      [-0.0874,  0.0120, -0.1081,  ..., -0.0340,  0.0136, -0.0214],\n                      [-0.0018, -0.0063, -0.0306,  ..., -0.0895, -0.0049, -0.0805]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.12.attention.self.query.bias',\n              tensor([ 0.0041, -0.0050,  0.0026,  ..., -0.2392, -0.0200, -0.0034],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.12.attention.self.key.weight',\n              tensor([[-0.0242, -0.1242,  0.0746,  ...,  0.0399, -0.0173,  0.0652],\n                      [ 0.1216,  0.0229, -0.0758,  ..., -0.0031, -0.0260, -0.0171],\n                      [-0.0456,  0.0313, -0.1079,  ..., -0.0759, -0.0470,  0.0032],\n                      ...,\n                      [-0.0049,  0.0192, -0.3710,  ...,  0.0575, -0.0072, -0.0080],\n                      [ 0.0261, -0.0054, -0.1946,  ..., -0.0335,  0.0104,  0.0577],\n                      [ 0.0037, -0.0400, -0.0965,  ...,  0.0396,  0.0105,  0.0100]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.12.attention.self.key.bias',\n              tensor([ 1.4285e-04, -1.0577e-04,  8.0918e-05,  ..., -1.7816e-02,\n                       4.8393e-04, -1.4247e-04], device='cuda:0')),\n             ('model.roberta.encoder.layer.12.attention.self.value.weight',\n              tensor([[-0.0900, -0.0034,  0.0080,  ...,  0.0442, -0.0038, -0.0052],\n                      [ 0.0322,  0.0990, -0.0121,  ..., -0.0096,  0.0298,  0.0828],\n                      [ 0.0135,  0.0062, -0.0009,  ..., -0.0345, -0.0426,  0.0349],\n                      ...,\n                      [-0.0193, -0.0408, -0.0155,  ..., -0.0046, -0.0154,  0.0626],\n                      [-0.0355, -0.0446,  0.0234,  ..., -0.0160, -0.0365,  0.0616],\n                      [ 0.0039, -0.0301,  0.0007,  ..., -0.0341,  0.0348,  0.0227]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.12.attention.self.value.bias',\n              tensor([ 0.0051, -0.0082,  0.0004,  ...,  0.0294,  0.0153,  0.0301],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.12.attention.output.dense.weight',\n              tensor([[-0.0474, -0.0207, -0.0211,  ...,  0.0186, -0.0018,  0.0008],\n                      [-0.0473,  0.0167, -0.0386,  ..., -0.0315,  0.0053,  0.0130],\n                      [-0.0412, -0.0201,  0.0285,  ...,  0.0110,  0.0359,  0.0009],\n                      ...,\n                      [ 0.0175, -0.0139, -0.0407,  ..., -0.0159,  0.0107,  0.0301],\n                      [ 0.0019, -0.0270,  0.0157,  ..., -0.0178,  0.0191, -0.0192],\n                      [ 0.0056,  0.0364,  0.0399,  ..., -0.0104, -0.0281, -0.0249]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.12.attention.output.dense.bias',\n              tensor([ 0.0302, -0.0018,  0.0644,  ..., -0.0135, -0.0421, -0.0076],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.12.attention.output.LayerNorm.weight',\n              tensor([0.9904, 0.9821, 1.0003,  ..., 0.9884, 0.9827, 0.9874], device='cuda:0')),\n             ('model.roberta.encoder.layer.12.attention.output.LayerNorm.bias',\n              tensor([-0.1642,  0.0040, -0.3015,  ..., -0.0276, -0.1448, -0.0360],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.12.intermediate.dense.weight',\n              tensor([[ 0.0737, -0.0337,  0.0183,  ..., -0.0149, -0.0052,  0.0102],\n                      [-0.0183, -0.0286,  0.0198,  ..., -0.0473, -0.0384, -0.0170],\n                      [-0.0389, -0.0460,  0.0774,  ..., -0.0048, -0.0166, -0.0242],\n                      ...,\n                      [ 0.0202, -0.0473,  0.0267,  ..., -0.1220, -0.0192, -0.0036],\n                      [ 0.0138, -0.0500,  0.0156,  ..., -0.0508, -0.0178,  0.0460],\n                      [ 0.0170,  0.0046, -0.0049,  ...,  0.0167, -0.0285, -0.0103]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.12.intermediate.dense.bias',\n              tensor([-0.1098, -0.0418, -0.0107,  ..., -0.1214, -0.0858, -0.1062],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.12.output.dense.weight',\n              tensor([[ 0.0301, -0.0036, -0.0696,  ...,  0.0263,  0.0255, -0.0156],\n                      [-0.0135,  0.0209,  0.0100,  ..., -0.0311,  0.0078, -0.0373],\n                      [ 0.0015, -0.0013, -0.0170,  ..., -0.0011, -0.0313, -0.0030],\n                      ...,\n                      [-0.0104, -0.0120,  0.0166,  ..., -0.0432, -0.0105, -0.0245],\n                      [-0.0177, -0.0127, -0.0223,  ..., -0.0112, -0.0265,  0.0198],\n                      [-0.0539, -0.0369,  0.0315,  ...,  0.0067, -0.0036, -0.0104]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.12.output.dense.bias',\n              tensor([-0.2172,  0.1671, -0.0787,  ..., -0.0088, -0.0804, -0.1230],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.12.output.LayerNorm.weight',\n              tensor([0.9852, 0.9898, 0.9961,  ..., 0.9902, 0.9839, 0.9913], device='cuda:0')),\n             ('model.roberta.encoder.layer.12.output.LayerNorm.bias',\n              tensor([ 0.0094, -0.0771,  0.0876,  ..., -0.0691, -0.0122, -0.0613],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.13.attention.self.query.weight',\n              tensor([[ 0.0441,  0.0499, -0.0502,  ..., -0.0455,  0.0407, -0.0609],\n                      [-0.0255, -0.0079,  0.0504,  ...,  0.0336,  0.0306,  0.0585],\n                      [-0.0034, -0.0253,  0.0024,  ..., -0.0381, -0.0812, -0.0124],\n                      ...,\n                      [ 0.0638, -0.0114,  0.0298,  ...,  0.0063, -0.0180, -0.0146],\n                      [-0.0040, -0.0277, -0.1455,  ...,  0.0423,  0.0245, -0.0237],\n                      [ 0.0505,  0.0562, -0.1591,  ...,  0.0255,  0.0399,  0.0654]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.13.attention.self.query.bias',\n              tensor([-0.0477,  0.0434,  0.0060,  ...,  0.0418,  0.1803,  0.0474],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.13.attention.self.key.weight',\n              tensor([[ 0.0292,  0.0594, -0.0412,  ..., -0.0181,  0.0294, -0.0711],\n                      [ 0.0571, -0.0038,  0.0759,  ..., -0.0354, -0.0026, -0.0318],\n                      [ 0.0191,  0.0640, -0.0400,  ...,  0.1551,  0.0248, -0.0791],\n                      ...,\n                      [-0.0460, -0.0505, -0.0133,  ..., -0.0232,  0.0145, -0.0249],\n                      [ 0.0216,  0.0323, -0.2131,  ..., -0.0188, -0.0205,  0.0038],\n                      [-0.0419,  0.0461, -0.1327,  ...,  0.0209,  0.0089,  0.0304]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.13.attention.self.key.bias',\n              tensor([ 1.1926e-04, -1.2857e-04, -1.3253e-04,  ..., -2.5872e-05,\n                       5.3456e-04, -2.1365e-04], device='cuda:0')),\n             ('model.roberta.encoder.layer.13.attention.self.value.weight',\n              tensor([[ 0.0529,  0.0643, -0.0155,  ...,  0.0379,  0.0035,  0.0380],\n                      [ 0.0169,  0.0545,  0.0063,  ..., -0.0341, -0.0206,  0.0241],\n                      [-0.0054,  0.0528,  0.0021,  ..., -0.0205, -0.0566,  0.0402],\n                      ...,\n                      [ 0.0418, -0.0740,  0.0177,  ...,  0.0009, -0.0164, -0.0250],\n                      [-0.0011,  0.0007, -0.0382,  ...,  0.0083, -0.1100, -0.0087],\n                      [-0.0406, -0.0893,  0.0252,  ...,  0.0002, -0.0063, -0.0177]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.13.attention.self.value.bias',\n              tensor([ 0.0112,  0.0082,  0.0053,  ..., -0.0071, -0.0072, -0.0002],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.13.attention.output.dense.weight',\n              tensor([[ 8.2259e-04, -3.9213e-02,  3.0878e-02,  ..., -5.3020e-02,\n                        1.4437e-03,  1.0531e-01],\n                      [ 1.1718e-02,  4.9691e-03, -6.7972e-03,  ...,  9.2290e-03,\n                       -6.3375e-02,  4.2088e-02],\n                      [-3.5413e-03, -1.1066e-02, -1.4939e-02,  ...,  1.5287e-02,\n                       -1.7551e-02, -1.1832e-03],\n                      ...,\n                      [-3.6332e-02, -1.6146e-03, -4.5336e-02,  ...,  4.0982e-05,\n                       -4.3700e-02,  1.5670e-02],\n                      [-4.0976e-02,  5.8702e-02, -3.1269e-03,  ...,  1.6918e-02,\n                        8.0084e-02, -3.1220e-02],\n                      [-7.2746e-04,  1.8631e-03,  1.2789e-02,  ...,  2.1380e-02,\n                        3.4173e-02,  5.1087e-02]], device='cuda:0')),\n             ('model.roberta.encoder.layer.13.attention.output.dense.bias',\n              tensor([-0.0419,  0.0445, -0.0912,  ...,  0.0382, -0.0342,  0.0341],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.13.attention.output.LayerNorm.weight',\n              tensor([0.9789, 0.9884, 0.9996,  ..., 0.9905, 0.9862, 0.9814], device='cuda:0')),\n             ('model.roberta.encoder.layer.13.attention.output.LayerNorm.bias',\n              tensor([-0.1777,  0.0731, -0.2685,  ..., -0.0583, -0.1190, -0.0617],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.13.intermediate.dense.weight',\n              tensor([[ 0.0220, -0.0225,  0.0202,  ..., -0.0338, -0.0561, -0.0447],\n                      [ 0.0343,  0.0110,  0.0086,  ...,  0.0523,  0.0210, -0.0017],\n                      [ 0.0597, -0.0434,  0.0538,  ...,  0.1138, -0.0106, -0.0634],\n                      ...,\n                      [ 0.0289, -0.0100, -0.0053,  ..., -0.0129,  0.0368,  0.0046],\n                      [ 0.0096,  0.0428,  0.0947,  ...,  0.0449,  0.0427, -0.0332],\n                      [ 0.0335, -0.1285,  0.0404,  ...,  0.0066,  0.0012, -0.0761]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.13.intermediate.dense.bias',\n              tensor([-0.1382, -0.0513, -0.0884,  ..., -0.1027, -0.0338, -0.0372],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.13.output.dense.weight',\n              tensor([[ 6.3122e-02, -3.2831e-02, -4.3943e-02,  ..., -3.5319e-02,\n                        1.0028e-02, -6.7768e-03],\n                      [-6.8516e-03,  3.1164e-04, -5.6083e-02,  ...,  9.6455e-03,\n                        2.1887e-02, -2.6013e-02],\n                      [ 1.4048e-02,  4.7467e-03,  1.5759e-03,  ..., -2.3563e-03,\n                        5.6472e-03, -7.1311e-03],\n                      ...,\n                      [-2.9820e-02,  3.5958e-02,  1.0283e-03,  ..., -3.1174e-02,\n                        1.0165e-02,  3.6451e-02],\n                      [-8.8721e-02, -2.3167e-02,  3.3281e-02,  ..., -1.9592e-02,\n                        6.0037e-02,  1.6732e-02],\n                      [-2.9222e-02, -3.2574e-03, -2.8115e-02,  ..., -2.5089e-02,\n                        3.0165e-05, -4.4561e-02]], device='cuda:0')),\n             ('model.roberta.encoder.layer.13.output.dense.bias',\n              tensor([-0.1062,  0.0572, -0.0717,  ..., -0.0240, -0.0499, -0.0876],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.13.output.LayerNorm.weight',\n              tensor([0.9840, 0.9930, 0.9974,  ..., 0.9887, 0.9890, 0.9949], device='cuda:0')),\n             ('model.roberta.encoder.layer.13.output.LayerNorm.bias',\n              tensor([ 0.0684, -0.1156,  0.2414,  ..., -0.0529, -0.0041, -0.0444],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.14.attention.self.query.weight',\n              tensor([[-0.0444, -0.0590,  0.0101,  ..., -0.1213, -0.0225, -0.1595],\n                      [-0.0681,  0.0283, -0.0947,  ...,  0.0282, -0.0231, -0.0356],\n                      [-0.0064, -0.0336,  0.1100,  ..., -0.0083, -0.0615,  0.0339],\n                      ...,\n                      [ 0.0602,  0.0019,  0.0213,  ..., -0.0392, -0.0680,  0.0412],\n                      [-0.0168,  0.0420, -0.0164,  ..., -0.0926, -0.0328,  0.0601],\n                      [-0.0702,  0.0272,  0.0083,  ...,  0.0581,  0.0299,  0.0420]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.14.attention.self.query.bias',\n              tensor([ 0.1940,  0.1392, -0.1252,  ...,  0.2345, -0.1190,  0.0187],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.14.attention.self.key.weight',\n              tensor([[ 0.0355,  0.0810,  0.0720,  ...,  0.0079,  0.0054, -0.0073],\n                      [-0.0677, -0.0056, -0.1230,  ..., -0.0120,  0.0118,  0.0418],\n                      [-0.0212,  0.0266,  0.0470,  ...,  0.0224, -0.1166,  0.0599],\n                      ...,\n                      [ 0.0796, -0.0610,  0.0655,  ..., -0.1003, -0.0435, -0.0236],\n                      [ 0.0449,  0.0780,  0.0444,  ..., -0.0764,  0.0704,  0.0372],\n                      [-0.0701, -0.0002,  0.0569,  ..., -0.0050, -0.0549, -0.0278]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.14.attention.self.key.bias',\n              tensor([ 1.7807e-04,  3.3211e-05, -9.2942e-05,  ..., -7.9483e-05,\n                      -6.9836e-06,  8.5824e-05], device='cuda:0')),\n             ('model.roberta.encoder.layer.14.attention.self.value.weight',\n              tensor([[-0.0413,  0.0003,  0.0431,  ..., -0.0442, -0.0045, -0.0073],\n                      [-0.0014, -0.0705, -0.0087,  ...,  0.0609,  0.0605, -0.0556],\n                      [ 0.0281, -0.0322, -0.0253,  ..., -0.0631,  0.0086,  0.0182],\n                      ...,\n                      [ 0.0825,  0.0261, -0.0188,  ...,  0.0260, -0.0284, -0.0332],\n                      [ 0.0725,  0.0164, -0.0152,  ..., -0.0019, -0.0222,  0.0317],\n                      [-0.0733, -0.0359, -0.0241,  ..., -0.0009,  0.0478, -0.1338]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.14.attention.self.value.bias',\n              tensor([ 1.2410e-04,  7.1745e-03, -1.1981e-02,  ..., -1.9968e-05,\n                       4.5817e-03,  1.2168e-02], device='cuda:0')),\n             ('model.roberta.encoder.layer.14.attention.output.dense.weight',\n              tensor([[-0.0135, -0.0232,  0.0367,  ..., -0.0414,  0.0004,  0.0853],\n                      [ 0.0171, -0.0393,  0.0474,  ..., -0.0705, -0.0415, -0.0064],\n                      [ 0.0409, -0.0342,  0.0185,  ...,  0.0061,  0.0036, -0.0226],\n                      ...,\n                      [ 0.0664, -0.0324,  0.0680,  ...,  0.0072, -0.0044, -0.0079],\n                      [-0.0052, -0.0650, -0.0219,  ...,  0.0289,  0.0093, -0.0083],\n                      [-0.0133, -0.0305,  0.0257,  ...,  0.0651,  0.0169,  0.0709]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.14.attention.output.dense.bias',\n              tensor([-0.0236,  0.0027,  0.0319,  ...,  0.0649, -0.0407, -0.0054],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.14.attention.output.LayerNorm.weight',\n              tensor([0.9951, 0.9840, 0.9996,  ..., 0.9883, 0.9917, 0.9907], device='cuda:0')),\n             ('model.roberta.encoder.layer.14.attention.output.LayerNorm.bias',\n              tensor([-0.0859, -0.0132, -0.1944,  ..., -0.0627, -0.1378, -0.0534],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.14.intermediate.dense.weight',\n              tensor([[-0.0081, -0.0004,  0.0405,  ..., -0.0005, -0.0193,  0.0366],\n                      [ 0.0143, -0.0518, -0.0101,  ...,  0.0614, -0.0220,  0.0484],\n                      [ 0.0449, -0.0666,  0.0105,  ...,  0.0610,  0.0664, -0.0542],\n                      ...,\n                      [ 0.0758, -0.0364,  0.0059,  ...,  0.0100, -0.0023, -0.0346],\n                      [-0.0355, -0.0559, -0.0127,  ...,  0.0262,  0.0137,  0.0203],\n                      [ 0.0354, -0.0427, -0.0671,  ...,  0.0367, -0.0140,  0.0034]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.14.intermediate.dense.bias',\n              tensor([-0.1091, -0.1090, -0.1109,  ..., -0.0549, -0.0797, -0.0823],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.14.output.dense.weight',\n              tensor([[-0.0207,  0.0268,  0.0087,  ...,  0.0266,  0.0082, -0.0663],\n                      [ 0.0072, -0.0098, -0.0852,  ..., -0.0103,  0.0152, -0.0645],\n                      [-0.0082, -0.0198,  0.0358,  ...,  0.0099,  0.0047,  0.0111],\n                      ...,\n                      [ 0.0604,  0.0103,  0.0066,  ..., -0.0467,  0.0357, -0.0327],\n                      [-0.0513,  0.0127,  0.0243,  ...,  0.0448,  0.0005, -0.0325],\n                      [ 0.0270,  0.0115, -0.0587,  ..., -0.0057, -0.0199, -0.0674]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.14.output.dense.bias',\n              tensor([-0.1088,  0.0501, -0.1955,  ..., -0.0113, -0.1291, -0.0695],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.14.output.LayerNorm.weight',\n              tensor([0.9880, 0.9916, 0.9980,  ..., 0.9915, 0.9908, 0.9932], device='cuda:0')),\n             ('model.roberta.encoder.layer.14.output.LayerNorm.bias',\n              tensor([-0.0237, -0.1088,  0.2501,  ..., -0.0506, -0.0143, -0.0584],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.15.attention.self.query.weight',\n              tensor([[ 0.0620,  0.0553, -0.1351,  ..., -0.0165,  0.0386,  0.0200],\n                      [-0.0222,  0.0174,  0.0851,  ...,  0.0863, -0.0272, -0.0215],\n                      [-0.0036, -0.0200, -0.0302,  ..., -0.0569,  0.0579, -0.0831],\n                      ...,\n                      [ 0.0513,  0.1177, -0.0246,  ...,  0.0262, -0.0182,  0.0158],\n                      [-0.0167,  0.0914,  0.0345,  ..., -0.0038, -0.0615,  0.0410],\n                      [-0.0545, -0.0351,  0.0084,  ..., -0.0124,  0.0937, -0.0522]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.15.attention.self.query.bias',\n              tensor([-0.0129, -0.0711, -0.0244,  ..., -0.1173, -0.0870, -0.0002],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.15.attention.self.key.weight',\n              tensor([[-0.0427,  0.0193, -0.2173,  ..., -0.0851, -0.0133, -0.0088],\n                      [ 0.0618, -0.0173, -0.0575,  ..., -0.0119,  0.0012, -0.0234],\n                      [-0.0301,  0.0868, -0.0755,  ..., -0.0450, -0.0618, -0.0151],\n                      ...,\n                      [ 0.0170,  0.0110, -0.0529,  ..., -0.1032,  0.0352, -0.0192],\n                      [-0.0965, -0.0148, -0.0387,  ..., -0.0028,  0.0136,  0.0328],\n                      [ 0.0526, -0.0428,  0.0250,  ..., -0.0331,  0.0457,  0.0595]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.15.attention.self.key.bias',\n              tensor([ 1.5775e-04,  1.2421e-05,  7.3592e-05,  ...,  4.3854e-04,\n                       1.2776e-04, -2.4400e-04], device='cuda:0')),\n             ('model.roberta.encoder.layer.15.attention.self.value.weight',\n              tensor([[-0.0662,  0.0448,  0.0056,  ...,  0.0030, -0.0232,  0.0641],\n                      [-0.0105, -0.0318, -0.0308,  ..., -0.0387,  0.0395, -0.0645],\n                      [ 0.0013, -0.0082,  0.0038,  ..., -0.1070,  0.1044,  0.0989],\n                      ...,\n                      [ 0.0139, -0.0582,  0.0002,  ...,  0.0364, -0.0074,  0.0770],\n                      [-0.0452, -0.0867,  0.0055,  ...,  0.0605,  0.0170,  0.0119],\n                      [-0.0658,  0.0036, -0.0133,  ..., -0.0631, -0.1059,  0.0022]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.15.attention.self.value.bias',\n              tensor([-0.0036,  0.0056,  0.0019,  ...,  0.0021, -0.0045, -0.0212],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.15.attention.output.dense.weight',\n              tensor([[ 0.0147,  0.0024, -0.0219,  ..., -0.0350, -0.0309,  0.0253],\n                      [ 0.0096,  0.0265, -0.0358,  ..., -0.0030,  0.0473, -0.0236],\n                      [-0.0270, -0.0370,  0.0721,  ..., -0.0145, -0.0176, -0.0114],\n                      ...,\n                      [-0.0020,  0.0540,  0.0159,  ..., -0.0354, -0.0074,  0.0240],\n                      [-0.0590,  0.0970,  0.0523,  ..., -0.0023,  0.0186,  0.0864],\n                      [ 0.0243, -0.0580,  0.0115,  ..., -0.0093, -0.0385, -0.0196]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.15.attention.output.dense.bias',\n              tensor([-0.0170,  0.0172,  0.0542,  ...,  0.0021, -0.0470, -0.0288],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.15.attention.output.LayerNorm.weight',\n              tensor([0.9894, 0.9884, 0.9995,  ..., 0.9924, 0.9927, 0.9871], device='cuda:0')),\n             ('model.roberta.encoder.layer.15.attention.output.LayerNorm.bias',\n              tensor([-0.1104, -0.0160, -0.1701,  ..., -0.0919, -0.0803, -0.1244],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.15.intermediate.dense.weight',\n              tensor([[-0.0222, -0.0172,  0.0238,  ...,  0.0136,  0.0492, -0.0271],\n                      [ 0.0526, -0.0540,  0.0033,  ..., -0.0247, -0.0449, -0.0115],\n                      [-0.0263,  0.0512,  0.0232,  ...,  0.0957,  0.0065,  0.0232],\n                      ...,\n                      [-0.0410, -0.0313,  0.0195,  ...,  0.0804,  0.0376,  0.0248],\n                      [-0.0365,  0.0708,  0.0577,  ..., -0.0112,  0.0223, -0.0111],\n                      [ 0.0658,  0.0051,  0.0070,  ...,  0.0445,  0.0664,  0.0214]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.15.intermediate.dense.bias',\n              tensor([-0.0436, -0.1057, -0.0757,  ..., -0.0333,  0.0094, -0.0839],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.15.output.dense.weight',\n              tensor([[-2.3532e-02,  6.3414e-02, -4.2881e-02,  ...,  2.8208e-04,\n                        1.7490e-02, -8.1136e-03],\n                      [-2.0982e-02,  1.1645e-02, -3.4348e-02,  ..., -5.5805e-02,\n                       -2.5794e-03, -3.3871e-03],\n                      [-1.6928e-02, -8.9150e-03, -2.6160e-02,  ..., -1.6956e-03,\n                        1.8046e-02,  1.5507e-02],\n                      ...,\n                      [ 6.1860e-03,  2.3696e-02,  5.4840e-02,  ..., -9.5128e-03,\n                        4.4669e-03,  6.0312e-05],\n                      [-8.9996e-04, -7.9829e-03,  6.4694e-02,  ..., -4.4142e-02,\n                       -1.7332e-02,  1.1538e-03],\n                      [-4.4661e-02,  1.8352e-02,  2.0899e-02,  ...,  3.2909e-02,\n                       -9.2514e-03,  3.7751e-02]], device='cuda:0')),\n             ('model.roberta.encoder.layer.15.output.dense.bias',\n              tensor([-0.1297,  0.0304, -0.2479,  ..., -0.0314, -0.0867, -0.0997],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.15.output.LayerNorm.weight',\n              tensor([0.9856, 0.9841, 0.9981,  ..., 0.9854, 0.9956, 0.9884], device='cuda:0')),\n             ('model.roberta.encoder.layer.15.output.LayerNorm.bias',\n              tensor([-0.0077, -0.0661,  0.1745,  ..., -0.0148, -0.0310, -0.0156],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.16.attention.self.query.weight',\n              tensor([[-0.0175,  0.0018, -0.0410,  ..., -0.0147,  0.0912,  0.0375],\n                      [-0.0471, -0.0642, -0.0246,  ...,  0.0111, -0.0174, -0.0386],\n                      [ 0.0025, -0.0069,  0.0827,  ..., -0.0698, -0.0620, -0.0077],\n                      ...,\n                      [ 0.0711, -0.0925, -0.0297,  ..., -0.0141,  0.0355,  0.0222],\n                      [-0.0484,  0.0409,  0.0561,  ..., -0.0873,  0.0316, -0.1171],\n                      [ 0.0139,  0.0050,  0.0334,  ..., -0.0359, -0.0872, -0.0327]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.16.attention.self.query.bias',\n              tensor([-0.0085, -0.0698,  0.0136,  ..., -0.0053,  0.0856,  0.0130],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.16.attention.self.key.weight',\n              tensor([[-0.0534, -0.0125, -0.0309,  ...,  0.0745, -0.0115, -0.0557],\n                      [ 0.0526, -0.0137, -0.0291,  ...,  0.0724, -0.0462, -0.0302],\n                      [-0.0144,  0.0679,  0.0811,  ..., -0.0369, -0.0634,  0.0321],\n                      ...,\n                      [ 0.0474,  0.0353,  0.0036,  ..., -0.0678, -0.0156, -0.0032],\n                      [ 0.0022,  0.0795,  0.0172,  ..., -0.0280,  0.0537,  0.0047],\n                      [ 0.0507, -0.0259,  0.0735,  ..., -0.0934, -0.0370,  0.0435]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.16.attention.self.key.bias',\n              tensor([-0.0002, -0.0002,  0.0004,  ..., -0.0002, -0.0003,  0.0002],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.16.attention.self.value.weight',\n              tensor([[ 0.0390,  0.0012,  0.0097,  ...,  0.0377, -0.0599,  0.0535],\n                      [ 0.0232,  0.0576, -0.0325,  ...,  0.0012,  0.0242,  0.0853],\n                      [ 0.0137,  0.0083, -0.0085,  ..., -0.0441, -0.0569,  0.0289],\n                      ...,\n                      [-0.0587, -0.1273,  0.0225,  ..., -0.0872, -0.0082,  0.0566],\n                      [-0.0048, -0.0325, -0.0337,  ..., -0.0224,  0.0052,  0.1281],\n                      [ 0.0404,  0.1249, -0.0011,  ..., -0.0227,  0.0106, -0.0244]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.16.attention.self.value.bias',\n              tensor([-0.0093,  0.0053,  0.0029,  ..., -0.0103, -0.0157, -0.0099],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.16.attention.output.dense.weight',\n              tensor([[ 0.0427, -0.0408, -0.0146,  ...,  0.0007, -0.0216,  0.0084],\n                      [ 0.0040,  0.0220,  0.0085,  ...,  0.0347, -0.0644, -0.0222],\n                      [ 0.0532, -0.0714,  0.0200,  ..., -0.0142, -0.0271,  0.0360],\n                      ...,\n                      [ 0.0223,  0.0098, -0.0599,  ...,  0.0812,  0.0401,  0.0063],\n                      [-0.0464, -0.0362, -0.0253,  ...,  0.0220, -0.0721,  0.0356],\n                      [ 0.0015, -0.0141, -0.0018,  ..., -0.0699, -0.0645, -0.0361]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.16.attention.output.dense.bias',\n              tensor([-0.0016, -0.0218,  0.0822,  ..., -0.0589,  0.0314, -0.0681],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.16.attention.output.LayerNorm.weight',\n              tensor([0.9965, 0.9933, 0.9994,  ..., 0.9938, 0.9964, 0.9983], device='cuda:0')),\n             ('model.roberta.encoder.layer.16.attention.output.LayerNorm.bias',\n              tensor([-0.0774, -0.0410, -0.2086,  ..., -0.0613, -0.0671, -0.0664],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.16.intermediate.dense.weight',\n              tensor([[ 0.0770, -0.0238,  0.0445,  ...,  0.0477,  0.0097, -0.0302],\n                      [ 0.0126,  0.0440,  0.0239,  ...,  0.0647, -0.0739, -0.0027],\n                      [ 0.0139,  0.0166, -0.0143,  ...,  0.1315, -0.0037, -0.0140],\n                      ...,\n                      [-0.0779, -0.0646,  0.0121,  ...,  0.0226,  0.0287, -0.0210],\n                      [-0.0378, -0.0232,  0.0421,  ...,  0.0060, -0.0486, -0.0470],\n                      [-0.0152, -0.0837,  0.0390,  ...,  0.0237, -0.0500,  0.0321]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.16.intermediate.dense.bias',\n              tensor([-0.0773, -0.0599, -0.0987,  ..., -0.0639, -0.0795, -0.0862],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.16.output.dense.weight',\n              tensor([[ 0.0128,  0.0354,  0.0690,  ..., -0.0134, -0.0025,  0.0050],\n                      [-0.0149, -0.0055,  0.0120,  ..., -0.0348, -0.0681, -0.0259],\n                      [ 0.0065, -0.0016, -0.0166,  ...,  0.0012,  0.0010,  0.0081],\n                      ...,\n                      [-0.0163,  0.0386,  0.0067,  ...,  0.0053, -0.0329, -0.0588],\n                      [-0.0549, -0.0423,  0.0448,  ..., -0.0078, -0.0308,  0.0174],\n                      [-0.0048,  0.0231, -0.0112,  ..., -0.0384,  0.0092,  0.0361]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.16.output.dense.bias',\n              tensor([-0.0811,  0.0128, -0.1996,  ..., -0.0392, -0.0646, -0.1039],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.16.output.LayerNorm.weight',\n              tensor([0.9782, 0.9953, 0.9976,  ..., 0.9737, 0.9886, 0.9869], device='cuda:0')),\n             ('model.roberta.encoder.layer.16.output.LayerNorm.bias',\n              tensor([-0.0197, -0.0466,  0.1808,  ..., -0.0238, -0.0308, -0.0298],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.17.attention.self.query.weight',\n              tensor([[-0.0049, -0.0162,  0.0645,  ...,  0.0445, -0.0284,  0.0620],\n                      [ 0.0321,  0.0168, -0.0229,  ...,  0.0885, -0.0296,  0.0497],\n                      [ 0.1355,  0.0696,  0.0284,  ..., -0.0819, -0.0407, -0.0059],\n                      ...,\n                      [-0.0273, -0.0414,  0.0741,  ..., -0.1271,  0.0550, -0.0752],\n                      [ 0.0479, -0.0106,  0.0446,  ..., -0.0001, -0.0469,  0.0300],\n                      [ 0.0156,  0.0883, -0.0288,  ..., -0.0009,  0.0178, -0.0205]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.17.attention.self.query.bias',\n              tensor([-0.1783,  0.0599,  0.0241,  ..., -0.0494,  0.0414,  0.0117],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.17.attention.self.key.weight',\n              tensor([[ 0.0281, -0.0076,  0.0558,  ..., -0.0658,  0.0364,  0.0302],\n                      [ 0.0496,  0.0108,  0.0308,  ...,  0.0915, -0.0342,  0.0214],\n                      [ 0.0099,  0.0307, -0.0349,  ..., -0.0679,  0.0818,  0.0046],\n                      ...,\n                      [ 0.0182,  0.0588,  0.0222,  ...,  0.0184,  0.0386, -0.0753],\n                      [ 0.0319,  0.0487,  0.0493,  ...,  0.0529,  0.0213,  0.0189],\n                      [-0.0887,  0.0012, -0.0297,  ...,  0.0591,  0.0079,  0.0666]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.17.attention.self.key.bias',\n              tensor([ 2.4796e-03, -2.1446e-05,  1.6755e-04,  ..., -2.3289e-05,\n                       3.9231e-04,  2.0397e-04], device='cuda:0')),\n             ('model.roberta.encoder.layer.17.attention.self.value.weight',\n              tensor([[-0.0141,  0.0044, -0.0074,  ...,  0.0101,  0.0027,  0.0050],\n                      [-0.0160,  0.0221, -0.0120,  ...,  0.0238,  0.0135,  0.0256],\n                      [ 0.0067,  0.0101, -0.0097,  ...,  0.0173, -0.0010, -0.0739],\n                      ...,\n                      [-0.0260,  0.0291,  0.0058,  ..., -0.0265, -0.0794, -0.1115],\n                      [ 0.0139, -0.1004, -0.0289,  ...,  0.0021, -0.0309,  0.0948],\n                      [ 0.0571, -0.0677,  0.0106,  ..., -0.1109,  0.0452, -0.0474]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.17.attention.self.value.bias',\n              tensor([ 0.0084, -0.0025, -0.0003,  ...,  0.0018,  0.0085,  0.0048],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.17.attention.output.dense.weight',\n              tensor([[-0.0010, -0.0209, -0.0148,  ..., -0.0192, -0.0314,  0.0523],\n                      [ 0.0275,  0.0074,  0.0111,  ...,  0.0749, -0.0464,  0.0678],\n                      [ 0.0006,  0.0096, -0.0031,  ...,  0.0156, -0.0297, -0.0047],\n                      ...,\n                      [ 0.0324,  0.0081, -0.0401,  ...,  0.0296,  0.0288,  0.0206],\n                      [-0.0299,  0.0241, -0.0086,  ..., -0.0472,  0.0629, -0.0331],\n                      [ 0.0284, -0.0278, -0.0564,  ..., -0.0018, -0.0304,  0.0138]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.17.attention.output.dense.bias',\n              tensor([-0.0230,  0.0376,  0.0296,  ..., -0.0090, -0.0499, -0.0196],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.17.attention.output.LayerNorm.weight',\n              tensor([0.9887, 0.9969, 0.9991,  ..., 0.9835, 0.9889, 0.9871], device='cuda:0')),\n             ('model.roberta.encoder.layer.17.attention.output.LayerNorm.bias',\n              tensor([-0.0939, -0.0615, -0.2153,  ..., -0.0823, -0.0677, -0.0857],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.17.intermediate.dense.weight',\n              tensor([[ 3.4484e-03, -4.4990e-02,  2.2858e-02,  ..., -5.2218e-02,\n                       -3.7068e-02, -2.8741e-02],\n                      [ 8.2675e-02, -3.1041e-02,  2.0322e-02,  ...,  2.0883e-02,\n                       -6.0604e-02, -5.5892e-03],\n                      [-8.0394e-03,  1.4356e-02,  1.2611e-02,  ...,  3.1052e-02,\n                        2.5514e-02,  1.4683e-02],\n                      ...,\n                      [ 7.6467e-02, -5.9261e-02, -3.7329e-02,  ..., -8.7861e-03,\n                        3.4596e-02,  9.0096e-03],\n                      [-4.2492e-03,  9.2833e-02,  1.3871e-02,  ..., -5.2216e-02,\n                       -6.7963e-02,  1.3985e-02],\n                      [-5.5403e-02, -2.7176e-02, -6.7037e-03,  ..., -2.5277e-02,\n                        9.5787e-05,  2.6339e-02]], device='cuda:0')),\n             ('model.roberta.encoder.layer.17.intermediate.dense.bias',\n              tensor([-0.0440, -0.0902, -0.0246,  ...,  0.0201, -0.1121, -0.0439],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.17.output.dense.weight',\n              tensor([[ 0.0279,  0.0659, -0.0176,  ...,  0.0044, -0.0244, -0.0545],\n                      [ 0.0076, -0.0069,  0.0211,  ...,  0.0042,  0.0018, -0.0075],\n                      [-0.0175, -0.0257, -0.0025,  ...,  0.0117,  0.0300, -0.0120],\n                      ...,\n                      [ 0.0091,  0.0969, -0.0138,  ...,  0.0209, -0.0331,  0.0010],\n                      [ 0.0145,  0.0120, -0.0259,  ...,  0.0162,  0.0374, -0.0018],\n                      [-0.0294, -0.0369, -0.0160,  ...,  0.0091, -0.0186,  0.0129]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.17.output.dense.bias',\n              tensor([-0.0691,  0.0149, -0.1524,  ..., -0.0154, -0.0881, -0.0521],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.17.output.LayerNorm.weight',\n              tensor([0.9861, 0.9914, 0.9974,  ..., 0.9828, 0.9874, 0.9856], device='cuda:0')),\n             ('model.roberta.encoder.layer.17.output.LayerNorm.bias',\n              tensor([-0.0027, -0.0269,  0.1257,  ..., -0.0128, -0.0378, -0.0076],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.18.attention.self.query.weight',\n              tensor([[ 0.0353, -0.0004,  0.1385,  ...,  0.0245,  0.0669, -0.0240],\n                      [ 0.0341,  0.0182,  0.0098,  ..., -0.1405, -0.0460, -0.0408],\n                      [-0.0531,  0.0547,  0.1906,  ...,  0.0414, -0.0243, -0.0282],\n                      ...,\n                      [-0.0510, -0.0033, -0.0279,  ...,  0.0189, -0.0147, -0.0319],\n                      [-0.0562,  0.0433, -0.0128,  ...,  0.0135,  0.0536,  0.1121],\n                      [-0.0177,  0.0431,  0.1433,  ..., -0.0296,  0.0111, -0.0531]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.18.attention.self.query.bias',\n              tensor([-0.2747, -0.0098, -0.1439,  ...,  0.0097, -0.0065,  0.0211],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.18.attention.self.key.weight',\n              tensor([[-0.0599, -0.0168,  0.0540,  ...,  0.0128, -0.0092,  0.0458],\n                      [-0.0244,  0.0853,  0.1170,  ..., -0.0222, -0.0091, -0.0546],\n                      [ 0.0661,  0.0500,  0.2612,  ..., -0.0236, -0.0003,  0.0081],\n                      ...,\n                      [-0.0559, -0.0354, -0.0155,  ..., -0.0321,  0.0639,  0.0241],\n                      [ 0.0629, -0.0276, -0.0211,  ..., -0.0136, -0.0422, -0.0035],\n                      [-0.0156, -0.0174,  0.1457,  ...,  0.0380, -0.0079,  0.0755]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.18.attention.self.key.bias',\n              tensor([-7.8590e-04,  8.9694e-05, -1.9103e-04,  ..., -8.8530e-05,\n                       5.9896e-06,  1.6873e-04], device='cuda:0')),\n             ('model.roberta.encoder.layer.18.attention.self.value.weight',\n              tensor([[ 0.0264,  0.0199,  0.0013,  ...,  0.0164, -0.0005,  0.0095],\n                      [-0.0366,  0.0617,  0.0156,  ...,  0.0076,  0.0049,  0.0251],\n                      [ 0.0088, -0.0397, -0.0331,  ...,  0.1059, -0.0265,  0.0046],\n                      ...,\n                      [ 0.0088,  0.0001,  0.0186,  ...,  0.0234, -0.0193, -0.0380],\n                      [ 0.0399, -0.0027, -0.0421,  ...,  0.0077,  0.0350,  0.0068],\n                      [ 0.0247,  0.0699, -0.0151,  ..., -0.0484,  0.0165,  0.0043]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.18.attention.self.value.bias',\n              tensor([-0.0083,  0.0056, -0.0177,  ..., -0.0070,  0.0025, -0.0119],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.18.attention.output.dense.weight',\n              tensor([[-0.0340, -0.0230, -0.0633,  ..., -0.0265, -0.0099, -0.0426],\n                      [-0.0230,  0.0703, -0.0546,  ...,  0.0041, -0.0265,  0.0194],\n                      [-0.0444,  0.0143, -0.0091,  ...,  0.0112, -0.0231, -0.0004],\n                      ...,\n                      [ 0.0318,  0.0072,  0.0685,  ..., -0.0069,  0.0231,  0.0170],\n                      [-0.0052,  0.0213,  0.0201,  ..., -0.0200, -0.0006,  0.0151],\n                      [-0.0062,  0.0085, -0.0074,  ..., -0.0015,  0.0284,  0.0222]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.18.attention.output.dense.bias',\n              tensor([-0.0147,  0.0793,  0.0276,  ...,  0.0720,  0.0370,  0.0002],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.18.attention.output.LayerNorm.weight',\n              tensor([0.9865, 0.9848, 0.9997,  ..., 0.9882, 0.9810, 0.9892], device='cuda:0')),\n             ('model.roberta.encoder.layer.18.attention.output.LayerNorm.bias',\n              tensor([-0.0942, -0.0224, -0.2511,  ..., -0.0716, -0.0620, -0.0683],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.18.intermediate.dense.weight',\n              tensor([[ 0.0507, -0.0251,  0.0314,  ...,  0.0193,  0.0051, -0.0284],\n                      [ 0.0604, -0.0431,  0.0099,  ..., -0.0146,  0.0005, -0.0468],\n                      [ 0.0279, -0.0322, -0.0306,  ..., -0.0347,  0.0080, -0.0323],\n                      ...,\n                      [ 0.0179,  0.0714,  0.0405,  ...,  0.0297,  0.0333,  0.0076],\n                      [-0.0314, -0.0487,  0.0064,  ...,  0.0417, -0.0215,  0.0104],\n                      [ 0.0406,  0.0614, -0.0630,  ...,  0.0046,  0.0114, -0.0065]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.18.intermediate.dense.bias',\n              tensor([-0.0710, -0.1024, -0.0051,  ..., -0.0475, -0.0980, -0.0815],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.18.output.dense.weight',\n              tensor([[-0.0409, -0.0015,  0.0159,  ...,  0.0199, -0.0292, -0.0324],\n                      [-0.0378, -0.0358, -0.0188,  ...,  0.0432, -0.0290, -0.0070],\n                      [ 0.0172, -0.0051, -0.0058,  ..., -0.0102, -0.0072, -0.0160],\n                      ...,\n                      [ 0.0174,  0.0185,  0.0399,  ..., -0.0027, -0.0007, -0.0080],\n                      [-0.0094, -0.0156, -0.0004,  ..., -0.0041,  0.0523,  0.0129],\n                      [-0.0282, -0.0139,  0.0433,  ...,  0.0034, -0.0575, -0.0226]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.18.output.dense.bias',\n              tensor([-0.0766,  0.0010, -0.1590,  ...,  0.0427, -0.0421, -0.0424],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.18.output.LayerNorm.weight',\n              tensor([0.9824, 0.9985, 0.9992,  ..., 0.9773, 0.9874, 0.9813], device='cuda:0')),\n             ('model.roberta.encoder.layer.18.output.LayerNorm.bias',\n              tensor([-0.0066, -0.0489,  0.1245,  ..., -0.0178, -0.0276, -0.0291],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.19.attention.self.query.weight',\n              tensor([[-0.0511, -0.0315, -0.0157,  ..., -0.0712,  0.0624, -0.1442],\n                      [ 0.0296,  0.0152, -0.1318,  ...,  0.0340,  0.0402,  0.0785],\n                      [-0.0296, -0.0219,  0.0446,  ..., -0.0566, -0.0794,  0.0194],\n                      ...,\n                      [-0.0032, -0.0093, -0.1970,  ...,  0.0866, -0.0068, -0.0354],\n                      [-0.1286,  0.0576,  0.0553,  ..., -0.0116,  0.0535, -0.0843],\n                      [-0.0278,  0.0596,  0.0598,  ...,  0.0956,  0.0696, -0.0195]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.19.attention.self.query.bias',\n              tensor([-0.0147, -0.0618, -0.0130,  ..., -0.0218,  0.0370, -0.0214],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.19.attention.self.key.weight',\n              tensor([[-0.0185,  0.0046,  0.0005,  ...,  0.0443, -0.0958, -0.0126],\n                      [ 0.0635,  0.0673, -0.0948,  ...,  0.0563, -0.0419,  0.0609],\n                      [-0.0579, -0.0144,  0.0370,  ...,  0.0211,  0.1090, -0.0441],\n                      ...,\n                      [-0.0823, -0.0240, -0.2647,  ...,  0.0135,  0.0678,  0.0276],\n                      [-0.0117,  0.0078,  0.1672,  ...,  0.0250,  0.0839,  0.0176],\n                      [ 0.0192, -0.0757,  0.1230,  ..., -0.0478, -0.0178,  0.0718]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.19.attention.self.key.bias',\n              tensor([-4.1335e-05, -1.2419e-04,  7.1332e-05,  ..., -6.5099e-05,\n                       1.4592e-03, -8.5857e-05], device='cuda:0')),\n             ('model.roberta.encoder.layer.19.attention.self.value.weight',\n              tensor([[ 0.0251,  0.0132,  0.0292,  ...,  0.0277,  0.0210,  0.0262],\n                      [-0.0079,  0.0104, -0.0019,  ...,  0.0323,  0.0258, -0.0188],\n                      [-0.0343, -0.0131,  0.0152,  ...,  0.0350, -0.0225, -0.0307],\n                      ...,\n                      [ 0.0165,  0.0155,  0.0136,  ..., -0.0209, -0.0128, -0.0176],\n                      [ 0.0215,  0.0275, -0.0354,  ...,  0.0303, -0.0193, -0.0447],\n                      [-0.0190,  0.0603, -0.0245,  ...,  0.0391,  0.0269, -0.0043]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.19.attention.self.value.bias',\n              tensor([ 0.0036,  0.0702,  0.0002,  ..., -0.0048, -0.0003, -0.0171],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.19.attention.output.dense.weight',\n              tensor([[ 0.0022,  0.0276, -0.0159,  ...,  0.0281,  0.0126, -0.0042],\n                      [ 0.0066, -0.0312, -0.0452,  ..., -0.0063,  0.0375,  0.0048],\n                      [ 0.0008, -0.0483, -0.0239,  ...,  0.0439, -0.0229,  0.0157],\n                      ...,\n                      [-0.0624, -0.0610, -0.0317,  ...,  0.0031,  0.0002,  0.0121],\n                      [-0.0166,  0.0458,  0.0080,  ...,  0.0107,  0.0171, -0.0066],\n                      [-0.0190, -0.0372,  0.0025,  ..., -0.0138, -0.0241, -0.0471]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.19.attention.output.dense.bias',\n              tensor([ 0.0044,  0.0603,  0.0792,  ..., -0.0015,  0.0252,  0.0064],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.19.attention.output.LayerNorm.weight',\n              tensor([0.9781, 0.9894, 0.9997,  ..., 0.9698, 0.9873, 0.9945], device='cuda:0')),\n             ('model.roberta.encoder.layer.19.attention.output.LayerNorm.bias',\n              tensor([-0.0798, -0.0371, -0.2135,  ..., -0.0626, -0.0841, -0.0627],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.19.intermediate.dense.weight',\n              tensor([[ 0.0412, -0.0413,  0.0144,  ...,  0.0851,  0.0756,  0.1545],\n                      [ 0.0449, -0.0103,  0.0535,  ...,  0.0177,  0.0291,  0.0292],\n                      [ 0.0223,  0.0098,  0.0382,  ..., -0.0193, -0.0047,  0.0120],\n                      ...,\n                      [ 0.0764, -0.0257,  0.0187,  ..., -0.0011, -0.0540,  0.0375],\n                      [ 0.0322, -0.0092, -0.0391,  ..., -0.0176,  0.0330, -0.0855],\n                      [ 0.0145,  0.0443,  0.0376,  ..., -0.0380, -0.0096,  0.0337]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.19.intermediate.dense.bias',\n              tensor([-0.1023,  0.0131, -0.0423,  ..., -0.1162,  0.0043, -0.0182],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.19.output.dense.weight',\n              tensor([[ 0.0120, -0.0166, -0.0203,  ..., -0.0404, -0.0099,  0.0084],\n                      [-0.0579, -0.0393,  0.0084,  ...,  0.0536, -0.0182,  0.0222],\n                      [ 0.0115, -0.0164,  0.0003,  ..., -0.0174,  0.0111, -0.0014],\n                      ...,\n                      [ 0.0101, -0.0087,  0.0304,  ..., -0.0481,  0.0091,  0.0243],\n                      [ 0.0307, -0.0115, -0.0308,  ..., -0.0166, -0.0091, -0.0112],\n                      [ 0.0084,  0.0144, -0.0005,  ...,  0.0403,  0.0405,  0.0317]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.19.output.dense.bias',\n              tensor([-0.0388,  0.0269, -0.0494,  ...,  0.1112, -0.0574,  0.0028],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.19.output.LayerNorm.weight',\n              tensor([0.9624, 0.9943, 0.9967,  ..., 0.9737, 0.9910, 0.9841], device='cuda:0')),\n             ('model.roberta.encoder.layer.19.output.LayerNorm.bias',\n              tensor([-0.0103, -0.0332,  0.0781,  ..., -0.0200, -0.0166, -0.0065],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.20.attention.self.query.weight',\n              tensor([[-1.6271e-02,  1.9143e-02, -1.0576e-02,  ...,  2.0112e-04,\n                       -5.6581e-02,  2.0954e-02],\n                      [-7.2067e-02, -7.2452e-02,  2.7696e-02,  ..., -3.0385e-02,\n                       -3.9535e-02, -7.4808e-03],\n                      [-1.9556e-02, -2.9674e-02,  4.4989e-05,  ...,  3.2380e-02,\n                        4.5328e-02, -5.0610e-03],\n                      ...,\n                      [ 9.6098e-02, -1.0856e-02, -2.9242e-02,  ...,  6.9966e-02,\n                       -2.1975e-02,  1.7606e-02],\n                      [ 1.1973e-04, -1.9764e-03,  1.9795e-01,  ..., -2.5715e-02,\n                        2.4742e-02, -4.1047e-05],\n                      [-2.5534e-02,  1.4161e-02, -1.2331e-01,  ..., -4.5080e-02,\n                       -3.6763e-02, -5.2346e-02]], device='cuda:0')),\n             ('model.roberta.encoder.layer.20.attention.self.query.bias',\n              tensor([-0.0275,  0.0257,  0.0146,  ...,  0.0917, -0.0324,  0.0538],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.20.attention.self.key.weight',\n              tensor([[ 0.0149, -0.0666, -0.0261,  ..., -0.1064, -0.0148, -0.0852],\n                      [-0.0117, -0.0034,  0.0363,  ..., -0.0152, -0.0011, -0.0065],\n                      [ 0.0275,  0.0472, -0.0863,  ...,  0.1043,  0.0263,  0.0383],\n                      ...,\n                      [-0.0402, -0.0531, -0.1082,  ..., -0.0272, -0.0152,  0.0273],\n                      [-0.0299,  0.0547,  0.2250,  ...,  0.0264, -0.0641,  0.0279],\n                      [-0.0214,  0.0220, -0.0455,  ..., -0.0950, -0.0175,  0.0162]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.20.attention.self.key.bias',\n              tensor([ 3.1029e-04, -3.0548e-05,  2.7751e-04,  ...,  1.8441e-04,\n                      -3.0549e-07,  5.3028e-05], device='cuda:0')),\n             ('model.roberta.encoder.layer.20.attention.self.value.weight',\n              tensor([[-0.0486, -0.0087,  0.0272,  ..., -0.0243, -0.0012,  0.0620],\n                      [-0.0191,  0.0034, -0.0153,  ..., -0.0202,  0.1040,  0.0215],\n                      [ 0.0050,  0.0333, -0.0131,  ..., -0.0711, -0.0779,  0.0170],\n                      ...,\n                      [-0.0644,  0.0298,  0.0202,  ...,  0.0677,  0.0079,  0.0184],\n                      [ 0.0902,  0.0106,  0.0627,  ..., -0.0059,  0.0152, -0.0514],\n                      [ 0.0198,  0.0411, -0.0132,  ..., -0.0242, -0.0088, -0.0764]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.20.attention.self.value.bias',\n              tensor([-0.0008,  0.0050,  0.0192,  ...,  0.0020,  0.0026, -0.0055],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.20.attention.output.dense.weight',\n              tensor([[ 0.0067, -0.0645, -0.0499,  ...,  0.0570, -0.0525, -0.0175],\n                      [ 0.0131,  0.0439,  0.0170,  ..., -0.0125, -0.0159, -0.0574],\n                      [ 0.0231,  0.0107,  0.0101,  ..., -0.0183, -0.0542, -0.0045],\n                      ...,\n                      [ 0.0408, -0.0274, -0.0434,  ..., -0.0726, -0.0228,  0.0415],\n                      [ 0.0209, -0.0010,  0.0101,  ..., -0.0213, -0.0118,  0.0302],\n                      [ 0.0096,  0.0062, -0.0164,  ...,  0.0231,  0.0138,  0.0372]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.20.attention.output.dense.bias',\n              tensor([-0.0103,  0.0778,  0.0608,  ...,  0.0716,  0.0865,  0.0220],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.20.attention.output.LayerNorm.weight',\n              tensor([0.9727, 0.9950, 0.9996,  ..., 0.9767, 0.9856, 0.9935], device='cuda:0')),\n             ('model.roberta.encoder.layer.20.attention.output.LayerNorm.bias',\n              tensor([-0.0292, -0.0337, -0.1557,  ..., -0.0442, -0.0894, -0.0462],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.20.intermediate.dense.weight',\n              tensor([[ 0.0068, -0.0112,  0.0573,  ...,  0.0058,  0.0102,  0.0124],\n                      [ 0.0199, -0.0073,  0.0044,  ...,  0.0091, -0.0659,  0.0677],\n                      [-0.0157, -0.0145, -0.0542,  ..., -0.0237, -0.0321, -0.0306],\n                      ...,\n                      [-0.0010, -0.0289,  0.0063,  ...,  0.0373,  0.0914,  0.0217],\n                      [ 0.0590,  0.0493,  0.0601,  ..., -0.0525, -0.0098, -0.0302],\n                      [-0.0168, -0.0416,  0.0501,  ..., -0.0091, -0.0012, -0.0092]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.20.intermediate.dense.bias',\n              tensor([-0.0225, -0.0377, -0.0483,  ..., -0.0271, -0.0183, -0.0251],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.20.output.dense.weight',\n              tensor([[-0.0180,  0.0672,  0.0110,  ...,  0.0567, -0.0084, -0.0165],\n                      [ 0.0092,  0.0307,  0.0498,  ...,  0.0042,  0.0147, -0.0401],\n                      [-0.0143, -0.0020, -0.0022,  ..., -0.0079, -0.0095, -0.0140],\n                      ...,\n                      [ 0.0150,  0.0064,  0.0444,  ..., -0.0174,  0.0153, -0.0033],\n                      [-0.0186, -0.0454,  0.0205,  ...,  0.0301,  0.0598,  0.0235],\n                      [-0.0349,  0.0025,  0.0396,  ...,  0.0220,  0.0378, -0.0045]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.20.output.dense.bias',\n              tensor([-0.0563,  0.0038, -0.0087,  ...,  0.0621, -0.0949,  0.0493],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.20.output.LayerNorm.weight',\n              tensor([0.9662, 0.9905, 0.9979,  ..., 0.9779, 0.9842, 0.9786], device='cuda:0')),\n             ('model.roberta.encoder.layer.20.output.LayerNorm.bias',\n              tensor([-0.0307, -0.0303,  0.0805,  ..., -0.0297, -0.0238, -0.0128],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.21.attention.self.query.weight',\n              tensor([[ 0.0304, -0.0804, -0.2148,  ..., -0.0168, -0.0089, -0.0422],\n                      [-0.0259, -0.0822,  0.0522,  ..., -0.0220,  0.0210,  0.0014],\n                      [ 0.0539,  0.0409,  0.0589,  ...,  0.0319, -0.0387, -0.0786],\n                      ...,\n                      [ 0.0029,  0.0124, -0.1495,  ...,  0.0728,  0.0619,  0.0583],\n                      [-0.0594,  0.0075,  0.0379,  ...,  0.0042,  0.0110,  0.0056],\n                      [-0.0484, -0.0409,  0.0476,  ...,  0.0153, -0.0195,  0.0652]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.21.attention.self.query.bias',\n              tensor([ 0.1669, -0.0286, -0.0164,  ..., -0.0690,  0.0403, -0.0465],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.21.attention.self.key.weight',\n              tensor([[ 0.0089,  0.0464, -0.1587,  ...,  0.0691,  0.0312,  0.0135],\n                      [ 0.0436,  0.0459,  0.0725,  ...,  0.0067,  0.1303, -0.0421],\n                      [ 0.0009,  0.0010, -0.0029,  ..., -0.0805,  0.0138, -0.0138],\n                      ...,\n                      [-0.0561, -0.0440, -0.1247,  ...,  0.0294, -0.0561,  0.0327],\n                      [ 0.0899, -0.0136,  0.0696,  ..., -0.0949,  0.0302, -0.0129],\n                      [-0.0331,  0.0128,  0.0777,  ..., -0.0408,  0.0147, -0.0047]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.21.attention.self.key.bias',\n              tensor([-5.0631e-02,  3.6023e-05,  5.8524e-04,  ...,  3.8974e-03,\n                      -3.6252e-04,  1.0153e-02], device='cuda:0')),\n             ('model.roberta.encoder.layer.21.attention.self.value.weight',\n              tensor([[ 0.0306, -0.0314, -0.0527,  ..., -0.0360,  0.0173, -0.0276],\n                      [ 0.0423, -0.0478,  0.0014,  ...,  0.0226, -0.0507,  0.0681],\n                      [ 0.0185, -0.0423, -0.0093,  ...,  0.0032, -0.0390,  0.0264],\n                      ...,\n                      [ 0.0173, -0.0737, -0.0088,  ..., -0.0221, -0.0764, -0.0287],\n                      [ 0.0291, -0.0287, -0.0445,  ...,  0.0666, -0.0161,  0.0260],\n                      [-0.0196, -0.0023,  0.0969,  ..., -0.0353, -0.0200,  0.0231]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.21.attention.self.value.bias',\n              tensor([ 0.0070, -0.0025, -0.0015,  ..., -0.0104, -0.0249, -0.0133],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.21.attention.output.dense.weight',\n              tensor([[-0.0063,  0.0423, -0.0310,  ..., -0.0193,  0.0048,  0.0370],\n                      [ 0.0371,  0.0133, -0.0362,  ...,  0.0058,  0.0303, -0.0193],\n                      [ 0.0099, -0.0398, -0.0047,  ..., -0.0117,  0.0135, -0.0389],\n                      ...,\n                      [ 0.0214,  0.0433, -0.0054,  ...,  0.0221, -0.0234,  0.0487],\n                      [-0.0498,  0.0709, -0.0619,  ...,  0.0037, -0.0385, -0.0167],\n                      [ 0.0160,  0.0147, -0.0500,  ...,  0.0484, -0.0238, -0.0257]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.21.attention.output.dense.bias',\n              tensor([-0.0269,  0.0902,  0.0440,  ...,  0.0357,  0.1777,  0.0310],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.21.attention.output.LayerNorm.weight',\n              tensor([0.9702, 0.9928, 0.9994,  ..., 0.9774, 0.9891, 0.9924], device='cuda:0')),\n             ('model.roberta.encoder.layer.21.attention.output.LayerNorm.bias',\n              tensor([-0.0457, -0.0586, -0.1337,  ..., -0.0496, -0.1018, -0.0346],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.21.intermediate.dense.weight',\n              tensor([[-0.0165, -0.0320,  0.0364,  ...,  0.0555,  0.0219, -0.0251],\n                      [-0.0013, -0.0004, -0.0773,  ...,  0.0259,  0.0008,  0.0590],\n                      [ 0.0048,  0.0405, -0.0444,  ..., -0.0034,  0.0088, -0.0004],\n                      ...,\n                      [-0.0083, -0.0198,  0.0267,  ..., -0.0237, -0.0739, -0.0230],\n                      [ 0.0127, -0.0685,  0.0171,  ..., -0.0277,  0.0075, -0.0232],\n                      [ 0.0048,  0.0046,  0.0210,  ..., -0.0117,  0.0287,  0.0373]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.21.intermediate.dense.bias',\n              tensor([-0.0242, -0.0425,  0.0199,  ..., -0.0307, -0.0021, -0.0159],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.21.output.dense.weight',\n              tensor([[-0.0099, -0.0001, -0.0054,  ...,  0.0356, -0.0197, -0.0344],\n                      [ 0.0009,  0.0115, -0.0349,  ...,  0.0151, -0.0356,  0.0513],\n                      [-0.0187,  0.0012,  0.0043,  ...,  0.0108,  0.0051,  0.0146],\n                      ...,\n                      [-0.0297,  0.0127,  0.0252,  ..., -0.0184, -0.0133, -0.0066],\n                      [-0.0332,  0.0027, -0.0383,  ..., -0.0363, -0.1030,  0.0099],\n                      [-0.0081, -0.0059,  0.0009,  ..., -0.0035, -0.0307,  0.0029]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.21.output.dense.bias',\n              tensor([-0.0102,  0.0063, -0.0729,  ...,  0.0920, -0.1210,  0.0157],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.21.output.LayerNorm.weight',\n              tensor([0.9688, 0.9905, 0.9958,  ..., 0.9814, 0.9826, 0.9946], device='cuda:0')),\n             ('model.roberta.encoder.layer.21.output.LayerNorm.bias',\n              tensor([-0.0436, -0.0136,  0.0399,  ..., -0.0389, -0.0572, -0.0163],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.22.attention.self.query.weight',\n              tensor([[ 0.0025,  0.0385,  0.0835,  ...,  0.0165,  0.0149,  0.0572],\n                      [ 0.0177, -0.0425,  0.0034,  ..., -0.0986,  0.0517,  0.0115],\n                      [ 0.0993, -0.0260,  0.1828,  ...,  0.0091,  0.0554, -0.0094],\n                      ...,\n                      [-0.0059, -0.0132, -0.0788,  ..., -0.0688, -0.0046,  0.0724],\n                      [-0.0423,  0.0734, -0.0109,  ...,  0.0607, -0.0400, -0.0320],\n                      [ 0.0381,  0.0627,  0.0986,  ...,  0.0257, -0.0212, -0.0810]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.22.attention.self.query.bias',\n              tensor([-0.0048,  0.0089,  0.1529,  ...,  0.0003,  0.0996, -0.0557],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.22.attention.self.key.weight',\n              tensor([[-0.0643, -0.0511,  0.0713,  ...,  0.0021, -0.0573,  0.0469],\n                      [ 0.0327, -0.0266,  0.0237,  ...,  0.0349, -0.0108,  0.0135],\n                      [ 0.0044,  0.0337,  0.0934,  ...,  0.0184, -0.0139,  0.0121],\n                      ...,\n                      [ 0.0740,  0.0007, -0.0983,  ..., -0.0612, -0.0553, -0.0941],\n                      [ 0.0232,  0.0691, -0.0527,  ...,  0.0349,  0.0347, -0.0392],\n                      [-0.0533,  0.0158, -0.0291,  ..., -0.0125,  0.0203, -0.0115]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.22.attention.self.key.bias',\n              tensor([ 9.0004e-06, -1.4666e-04,  4.6923e-04,  ..., -2.0520e-04,\n                      -5.9013e-04, -6.9184e-04], device='cuda:0')),\n             ('model.roberta.encoder.layer.22.attention.self.value.weight',\n              tensor([[ 0.0272,  0.0336,  0.0423,  ...,  0.0490, -0.0199,  0.0520],\n                      [-0.0453, -0.0113, -0.0178,  ...,  0.0524, -0.0730,  0.0567],\n                      [-0.0204,  0.0339,  0.0041,  ..., -0.0065, -0.0217,  0.0225],\n                      ...,\n                      [-0.0397, -0.0015, -0.0145,  ..., -0.0319,  0.0279, -0.0453],\n                      [ 0.0174,  0.0151,  0.0358,  ...,  0.0077,  0.0057, -0.0365],\n                      [-0.0218, -0.0199, -0.0393,  ..., -0.0297,  0.0117,  0.0236]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.22.attention.self.value.bias',\n              tensor([ 0.0080, -0.0130,  0.0142,  ..., -0.0117, -0.0051, -0.0069],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.22.attention.output.dense.weight',\n              tensor([[ 0.0431,  0.0455, -0.0021,  ..., -0.0369, -0.0330, -0.0136],\n                      [ 0.0087,  0.0255,  0.0297,  ..., -0.0476,  0.0081,  0.0279],\n                      [ 0.0159,  0.0061, -0.0274,  ..., -0.0223,  0.0507, -0.0011],\n                      ...,\n                      [ 0.0336,  0.0282, -0.0242,  ...,  0.0093, -0.0206, -0.0127],\n                      [ 0.0379, -0.0192,  0.0183,  ..., -0.0132, -0.0107, -0.0380],\n                      [ 0.0299,  0.0415,  0.0423,  ..., -0.0369,  0.0065, -0.0045]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.22.attention.output.dense.bias',\n              tensor([ 0.0770,  0.0951,  0.0674,  ...,  0.0089,  0.1465, -0.0303],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.22.attention.output.LayerNorm.weight',\n              tensor([0.9525, 0.9882, 0.9966,  ..., 0.9709, 0.9909, 0.9857], device='cuda:0')),\n             ('model.roberta.encoder.layer.22.attention.output.LayerNorm.bias',\n              tensor([-0.0471, -0.0135, -0.1200,  ..., -0.0381, -0.1031,  0.0338],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.22.intermediate.dense.weight',\n              tensor([[-0.0018, -0.0670, -0.0488,  ..., -0.0090,  0.0066,  0.0804],\n                      [-0.0204, -0.0194,  0.0644,  ...,  0.0062,  0.0418, -0.0028],\n                      [-0.0303, -0.0726,  0.0768,  ...,  0.0128, -0.0103, -0.0454],\n                      ...,\n                      [-0.0322,  0.0280,  0.0450,  ...,  0.0471,  0.0118,  0.0350],\n                      [ 0.0619,  0.0227,  0.0405,  ...,  0.0222, -0.0072,  0.0059],\n                      [ 0.0067, -0.0751, -0.0558,  ..., -0.0048,  0.0195, -0.0509]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.22.intermediate.dense.bias',\n              tensor([ 0.0187, -0.0059, -0.0118,  ..., -0.0450,  0.0163, -0.0280],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.22.output.dense.weight',\n              tensor([[-0.0242,  0.0391, -0.0234,  ...,  0.0065, -0.0054, -0.0847],\n                      [ 0.0097, -0.0249,  0.0237,  ..., -0.0090, -0.0309,  0.0135],\n                      [-0.0173, -0.0122, -0.0035,  ...,  0.0025, -0.0043,  0.0106],\n                      ...,\n                      [-0.0149, -0.0305,  0.0104,  ..., -0.0225, -0.0063,  0.0554],\n                      [ 0.0193, -0.0384,  0.0407,  ..., -0.0086, -0.0256, -0.0234],\n                      [-0.0058,  0.0705,  0.0299,  ..., -0.0218,  0.0088, -0.0204]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.22.output.dense.bias',\n              tensor([ 0.0240,  0.0456, -0.0758,  ...,  0.0241, -0.1322,  0.0455],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.22.output.LayerNorm.weight',\n              tensor([0.9533, 0.9953, 0.9967,  ..., 0.9784, 0.9871, 0.9805], device='cuda:0')),\n             ('model.roberta.encoder.layer.22.output.LayerNorm.bias',\n              tensor([-0.0231,  0.0372,  0.0506,  ..., -0.0322, -0.0120,  0.0171],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.23.attention.self.query.weight',\n              tensor([[-3.4316e-02,  2.2371e-02,  9.3761e-02,  ..., -8.9008e-02,\n                        8.8243e-02,  6.6447e-02],\n                      [-4.5287e-02,  2.1982e-02,  1.0297e-01,  ..., -5.9007e-03,\n                       -1.3614e-02, -2.9292e-03],\n                      [ 3.7220e-02,  4.1914e-02,  2.9020e-02,  ..., -7.3659e-03,\n                       -8.9769e-02, -3.3625e-02],\n                      ...,\n                      [ 1.0099e-01, -5.4709e-02,  1.2606e-01,  ..., -2.7200e-03,\n                       -1.0008e-02,  6.5871e-02],\n                      [-5.0544e-02, -1.1251e-04, -2.4884e-03,  ..., -5.0854e-02,\n                       -1.6917e-02, -1.5433e-02],\n                      [-1.2512e-02, -4.8222e-02, -6.0923e-02,  ...,  4.5640e-02,\n                       -6.6142e-02,  1.4999e-02]], device='cuda:0')),\n             ('model.roberta.encoder.layer.23.attention.self.query.bias',\n              tensor([ 0.0774,  0.2731,  0.0308,  ..., -0.2018,  0.1126,  0.0784],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.23.attention.self.key.weight',\n              tensor([[ 5.8104e-02, -2.9373e-02,  2.1678e-01,  ..., -1.6889e-02,\n                        2.9737e-02,  2.2775e-02],\n                      [ 7.0143e-02, -6.3189e-02,  6.6482e-02,  ...,  1.2761e-02,\n                       -4.8450e-02,  5.9808e-03],\n                      [-1.1517e-01, -2.5052e-02, -3.6506e-02,  ..., -1.2824e-01,\n                        4.1083e-02, -2.9713e-02],\n                      ...,\n                      [ 5.3279e-02,  7.5495e-03,  6.9315e-02,  ..., -4.4950e-02,\n                        2.2654e-03,  5.3211e-02],\n                      [ 2.1515e-02,  1.9970e-04, -6.4238e-02,  ..., -2.6147e-02,\n                        3.5755e-02,  3.5929e-02],\n                      [-1.1397e-02, -1.5452e-02, -1.6882e-02,  ...,  1.0485e-01,\n                        3.4490e-03,  6.3967e-02]], device='cuda:0')),\n             ('model.roberta.encoder.layer.23.attention.self.key.bias',\n              tensor([-1.3775e-07, -5.4873e-06,  3.9239e-05,  ...,  6.2499e-02,\n                       1.4294e-05, -1.9649e-04], device='cuda:0')),\n             ('model.roberta.encoder.layer.23.attention.self.value.weight',\n              tensor([[-0.0673, -0.0672, -0.0137,  ..., -0.0365,  0.0022,  0.0067],\n                      [ 0.0225,  0.0093,  0.0390,  ..., -0.0280,  0.0043, -0.0473],\n                      [-0.0402,  0.0680, -0.0095,  ...,  0.0068, -0.0485, -0.0105],\n                      ...,\n                      [ 0.0495,  0.0283, -0.0322,  ..., -0.0131, -0.0240,  0.0172],\n                      [ 0.0006, -0.0357,  0.0270,  ..., -0.0131, -0.0004,  0.0149],\n                      [ 0.0104, -0.0204,  0.0237,  ...,  0.0019,  0.0170,  0.0156]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.23.attention.self.value.bias',\n              tensor([-0.0143,  0.0174,  0.0114,  ...,  0.0083,  0.0075,  0.0135],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.23.attention.output.dense.weight',\n              tensor([[-0.0210, -0.0085, -0.0015,  ...,  0.0375, -0.0310,  0.0338],\n                      [-0.0367, -0.0069,  0.0243,  ...,  0.0550, -0.0107, -0.0481],\n                      [-0.0303,  0.0089,  0.0104,  ..., -0.0108,  0.0152, -0.0321],\n                      ...,\n                      [-0.0030, -0.0466, -0.0114,  ..., -0.0211, -0.0034, -0.0089],\n                      [ 0.0167,  0.0410,  0.0137,  ...,  0.0133,  0.0056,  0.0063],\n                      [ 0.0407, -0.0149, -0.0230,  ..., -0.0096,  0.0125,  0.0126]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.23.attention.output.dense.bias',\n              tensor([ 0.0626,  0.0711, -0.0043,  ...,  0.0326,  0.0215,  0.1620],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.23.attention.output.LayerNorm.weight',\n              tensor([0.9588, 0.9819, 0.9927,  ..., 0.9774, 0.9884, 0.9624], device='cuda:0')),\n             ('model.roberta.encoder.layer.23.attention.output.LayerNorm.bias',\n              tensor([-0.1359,  0.0302, -0.1578,  ..., -0.0652, -0.1421, -0.0915],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.23.intermediate.dense.weight',\n              tensor([[-0.0108, -0.0876, -0.0119,  ..., -0.0052,  0.0122, -0.0272],\n                      [-0.0195, -0.0225, -0.0117,  ..., -0.0101,  0.0180, -0.0643],\n                      [ 0.0019, -0.0904, -0.0039,  ...,  0.0330,  0.0315, -0.0231],\n                      ...,\n                      [-0.0398,  0.0247,  0.0124,  ..., -0.0006, -0.0402, -0.0099],\n                      [ 0.0445,  0.0010, -0.0057,  ..., -0.0111,  0.0612,  0.0373],\n                      [ 0.0356, -0.0132, -0.0312,  ...,  0.0534,  0.0255,  0.0441]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.23.intermediate.dense.bias',\n              tensor([ 0.0074, -0.0674, -0.1261,  ..., -0.0416, -0.0954, -0.0115],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.23.output.dense.weight',\n              tensor([[ 0.0228, -0.0419, -0.0165,  ...,  0.0193,  0.0242, -0.0097],\n                      [ 0.0086,  0.0676, -0.0244,  ..., -0.0134, -0.0426,  0.0297],\n                      [-0.0042, -0.0123,  0.0074,  ...,  0.0142, -0.0256, -0.0131],\n                      ...,\n                      [ 0.0162, -0.0337,  0.0852,  ...,  0.0322, -0.0130,  0.0427],\n                      [ 0.0077, -0.0190,  0.0471,  ...,  0.0080,  0.0278, -0.0073],\n                      [-0.0394,  0.0084,  0.0051,  ...,  0.0367,  0.0073, -0.0119]],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.23.output.dense.bias',\n              tensor([-0.0758, -0.0247, -0.0965,  ...,  0.0764, -0.1576,  0.0786],\n                     device='cuda:0')),\n             ('model.roberta.encoder.layer.23.output.LayerNorm.weight',\n              tensor([0.9762, 0.9963, 0.9997,  ..., 0.9787, 0.9959, 0.9870], device='cuda:0')),\n             ('model.roberta.encoder.layer.23.output.LayerNorm.bias',\n              tensor([-0.0253, -0.0420, -0.0142,  ..., -0.0474,  0.0078, -0.0066],\n                     device='cuda:0')),\n             ('model.classifier.dense.weight',\n              tensor([[-4.5305e-03,  1.6454e-02, -1.0140e-02,  ..., -5.4069e-02,\n                        1.2966e-02, -7.7423e-03],\n                      [ 7.6518e-03,  3.7774e-03, -1.0732e-02,  ...,  3.7664e-02,\n                        3.2625e-02,  3.4741e-05],\n                      [-2.0810e-03,  1.5648e-02,  1.3219e-02,  ...,  1.7111e-02,\n                        5.5781e-03,  6.6491e-04],\n                      ...,\n                      [ 6.2936e-03,  1.5275e-02,  1.2228e-02,  ..., -2.2015e-02,\n                        2.3957e-02, -2.7791e-02],\n                      [-3.8006e-03, -5.0472e-02,  3.2533e-02,  ...,  3.3561e-02,\n                       -1.3561e-02, -1.4812e-03],\n                      [ 1.1051e-02,  2.5411e-02,  9.2299e-03,  ...,  5.0173e-03,\n                        3.9983e-02, -4.0184e-03]], device='cuda:0')),\n             ('model.classifier.dense.bias',\n              tensor([-6.1668e-04,  1.6826e-05, -7.2603e-04,  ...,  3.9344e-04,\n                       4.9191e-04, -8.0615e-04], device='cuda:0')),\n             ('model.classifier.out_proj.weight',\n              tensor([[ 0.0204, -0.0161,  0.0052,  ..., -0.0114,  0.0251, -0.0040],\n                      [ 0.0164,  0.0106, -0.0061,  ..., -0.0006,  0.0076, -0.0137],\n                      [-0.0091, -0.0101, -0.0163,  ...,  0.0144,  0.0238, -0.0150]],\n                     device='cuda:0')),\n             ('model.classifier.out_proj.bias',\n              tensor([-0.0004, -0.0003,  0.0005], device='cuda:0'))])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model_smart.to(device)\n",
    "\n",
    "PATH = \"SMART_Roberta_large_FinancialTweets/exp8-0.5/\"+str(6)\n",
    "state_dict = torch.load(PATH)\n",
    "torch.cuda.empty_cache()\n",
    "state_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T15:34:57.255575200Z",
     "start_time": "2024-03-15T15:34:28.747327500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-03-15T15:34:57.250583300Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 247/1193 [43:44<2:47:58, 10.65s/it]"
     ]
    }
   ],
   "source": [
    "# 查询模型中key值\n",
    "# model = model_smart # Replace YourModelClass with your actual model class\n",
    "# print(\"Model state_dict keys:\")\n",
    "# for key in model.state_dict().keys():\n",
    "#     print(key)\n",
    "\n",
    "\n",
    "model_smart.load_state_dict(state_dict, strict=False)\n",
    "model_smart.zero_grad()\n",
    "\n",
    "# Initialize GradScaler for mixed precision training\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "\n",
    "    model_smart.train()\n",
    "    epoch_loss = []\n",
    "\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        batch = get_inputs_dict(batch)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Original Code\n",
    "        # logits, loss = model_smart(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "        \n",
    "        # Use mixed precision training\n",
    "        with autocast():\n",
    "            logits, loss = model_smart(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # Free up GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        scheduler.step()\n",
    "        model_smart.zero_grad()\n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "#    SAVE\n",
    "    PATH = \"SMART_Roberta_large_FinancialTweets/exp8-0.5/\"+str(epoch)\n",
    "    torch.save(model_smart.state_dict(), PATH)\n",
    "\n",
    "    print('epoch',epoch,'Training avg loss',np.mean(epoch_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "model_smart.to(device)\n",
    "\n",
    "for epoch in range(15):\n",
    "    \n",
    "    #evaluate model with test_df at the end of the epoch.\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    n_batches = len(test_dataloader)\n",
    "    preds = np.empty((len(test_dataset), num_labels))\n",
    "    out_label_ids = np.empty((len(test_dataset)))\n",
    "    PATH = \"SMART_Roberta_large_FinancialTweets/exp1/\"+str(epoch)\n",
    "    model_smart.load_state_dict(torch.load(PATH))\n",
    "    model_smart.eval()\n",
    "    \n",
    "    for i,test_batch in enumerate(test_dataloader):\n",
    "#         with torch.no_grad():\n",
    "        test_batch = get_inputs_dict(test_batch)\n",
    "        input_ids = test_batch['input_ids'].to(device)\n",
    "        attention_mask = test_batch['attention_mask'].to(device)\n",
    "        labels = test_batch['labels'].to(device)\n",
    "        logits, tmp_eval_loss = model_smart(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    #             tmp_eval_loss, logits = outputs[:2]\n",
    "        eval_loss += tmp_eval_loss.item()\n",
    "            \n",
    "        nb_eval_steps += 1\n",
    "        start_index = test_batch_size * i\n",
    "        end_index = start_index + test_batch_size if i != (n_batches - 1) else len(test_dataset)\n",
    "#         print(logits)\n",
    "        preds[start_index:end_index] = logits.detach().cpu().numpy()\n",
    "        out_label_ids[start_index:end_index] = test_batch[\"labels\"].detach().cpu().numpy()\n",
    "        \n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    model_outputs = preds\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    result, con_m = compute_metrics(preds, model_outputs, out_label_ids)\n",
    "    \n",
    "    #print('epoch',epoch,'Training avg loss',np.mean(epoch_loss))\n",
    "    print('epoch',epoch,'Testing  avg loss',eval_loss)\n",
    "    print(result)\n",
    "    print(con_m)\n",
    "    print('---------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# df_sample =  pd.read_csv(\"../data/tweets/stockerbot-export-test-2.csv\")\n",
    "\n",
    "df_sample =  pd.read_csv(\"../data/tweets.csv\")\n",
    "df_sample\n",
    "\n",
    "new_labels=np.zeros(704)\n",
    "\n",
    "# for l in df_sample['label'].tolist():\n",
    "#     if l == 2:\n",
    "#         new_labels.append(1)\n",
    "#     elif l==1:\n",
    "#         new_labels.append(2)\n",
    "#     else:\n",
    "#         new_labels.append(0)\n",
    "# print(new_labels)\n",
    "sample_examples = (df_sample['text'].astype(str).tolist(), new_labels)\n",
    "# sample_examples = (df_sample['clean_text'].astype(str).tolist(), new_labels)\n",
    "sample_dataset = MyClassificationDataset(sample_examples,tokenizer)\n",
    "\n",
    "sample_dataloader = DataLoader(sample_dataset,shuffle=False,batch_size=test_batch_size)\n",
    "\n",
    "print(sample_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "model_smart.to(device)\n",
    "pred_final = []\n",
    "for epoch in range(5,7):\n",
    "    \n",
    "    #evaluate model with test_df at the end of the epoch.\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    n_batches = len(sample_dataloader)\n",
    "    preds = np.empty((len(sample_dataset), num_labels))\n",
    "    out_label_ids = np.empty((len(sample_dataset)))\n",
    "    \n",
    "    PATH = \"SMART_Roberta_large_FinancialTweets/exp8-0.5/\"+str(epoch)\n",
    "    model_smart.load_state_dict(torch.load(PATH))\n",
    "    model_smart.eval()\n",
    "    \n",
    "    for i,test_batch in enumerate(sample_dataloader):\n",
    "#         with torch.no_grad():\n",
    "        test_batch = get_inputs_dict(test_batch)\n",
    "        input_ids = test_batch['input_ids'].to(device)\n",
    "        attention_mask = test_batch['attention_mask'].to(device)\n",
    "        labels = test_batch['labels'].to(device)\n",
    "        logits, tmp_eval_loss = model_smart(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    #             tmp_eval_loss, logits = outputs[:2]\n",
    "        eval_loss += tmp_eval_loss.item()\n",
    "            \n",
    "        nb_eval_steps += 1\n",
    "        start_index = test_batch_size * i\n",
    "        end_index = start_index + test_batch_size if i != (n_batches - 1) else len(sample_dataset)\n",
    "#         print(logits)\n",
    "        preds[start_index:end_index] = logits.detach().cpu().numpy()\n",
    "        out_label_ids[start_index:end_index] = test_batch[\"labels\"].detach().cpu().numpy()\n",
    "        \n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    model_outputs = preds\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    result, con_m = compute_metrics(preds, model_outputs, out_label_ids)\n",
    "    if epoch == 6:\n",
    "        pred_final = preds \n",
    "    \n",
    "    #print('epoch',epoch,'Training avg loss',np.mean(epoch_loss))\n",
    "#     print('epoch',epoch,'Testing  avg loss',eval_loss)\n",
    "#     print(result)\n",
    "#     print(con_m)\n",
    "#     print('---------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "pred_final_df=pd.DataFrame(pred_final)\n",
    "pred_final_df.columns = ['pred_sen']\n",
    "df_sample = df_sample.join(pred_final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "df_sample.to_csv(\"../data/tweets.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "\n",
    "model.zero_grad()\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss = []\n",
    "    \n",
    "    for batch in tqdm(train_dataloader):\n",
    "        batch = get_inputs_dict(batch)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        model.zero_grad()\n",
    "        epoch_loss.append(loss.item())\n",
    "        \n",
    "#    SAVE\n",
    "    PATH = \"SMART_Roberta_large_FinancialTweets/exp_roberta/\"+str(epoch)\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    \n",
    "        \n",
    "#     evaluate model with test_df at the end of the epoch.\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    n_batches = len(test_dataloader)\n",
    "    preds = np.empty((len(test_dataset), num_labels))\n",
    "    out_label_ids = np.empty((len(test_dataset)))\n",
    "    model.eval()\n",
    "    \n",
    "    for i,test_batch in enumerate(test_dataloader):\n",
    "        with torch.no_grad():\n",
    "            test_batch = get_inputs_dict(test_batch)\n",
    "            input_ids = test_batch['input_ids'].to(device)\n",
    "            attention_mask = test_batch['attention_mask'].to(device)\n",
    "            labels = test_batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "            eval_loss += tmp_eval_loss.item()\n",
    "            \n",
    "        nb_eval_steps += 1\n",
    "        start_index = test_batch_size * i\n",
    "        end_index = start_index + test_batch_size if i != (n_batches - 1) else len(test_dataset)\n",
    "        preds[start_index:end_index] = logits.detach().cpu().numpy()\n",
    "        out_label_ids[start_index:end_index] = test_batch[\"labels\"].detach().cpu().numpy()\n",
    "        \n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    model_outputs = preds\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    result, con_m = compute_metrics(preds, model_outputs, out_label_ids)\n",
    "    \n",
    "    print('epoch',epoch,'Training avg loss',np.mean(epoch_loss))\n",
    "    print('epoch',epoch,'Testing  avg loss',eval_loss)\n",
    "    print(result) \n",
    "    print(con_m)\n",
    "    print('---------------------------------------------------\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
